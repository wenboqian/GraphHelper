const text = `
HloModule SyncTensorsGraph.16388, input_output_alias={ {0}: (41, {}, must-alias), {1}: (37, {}, must-alias), {2}: (39, {}, must-alias), {3}: (35, {}, must-alias), {4}: (42, {}, must-alias), {5}: (53, {}, must-alias), {6}: (52, {}, must-alias), {7}: (51, {}, must-alias), {8}: (50, {}, must-alias), {9}: (46, {}, must-alias), {10}: (45, {}, must-alias), {11}: (44, {}, must-alias), {12}: (43, {}, must-alias), {13}: (34, {}, must-alias), {14}: (54, {}, must-alias), {15}: (58, {}, must-alias), {16}: (57, {}, must-alias), {17}: (56, {}, must-alias), {18}: (55, {}, must-alias), {19}: (33, {}, must-alias), {20}: (59, {}, must-alias), {21}: (67, {}, must-alias), {22}: (66, {}, must-alias), {23}: (65, {}, must-alias), {24}: (64, {}, must-alias), {25}: (63, {}, must-alias), {26}: (62, {}, must-alias), {27}: (61, {}, must-alias), {28}: (60, {}, must-alias), {29}: (32, {}, must-alias), {30}: (68, {}, must-alias), {31}: (72, {}, must-alias), {32}: (71, {}, must-alias), {33}: (70, {}, must-alias), {34}: (69, {}, must-alias), {35}: (31, {}, must-alias), {36}: (73, {}, must-alias), {37}: (81, {}, must-alias), {38}: (80, {}, must-alias), {39}: (79, {}, must-alias), {40}: (78, {}, must-alias), {41}: (77, {}, must-alias), {42}: (76, {}, must-alias), {43}: (75, {}, must-alias), {44}: (74, {}, must-alias), {45}: (30, {}, must-alias), {46}: (82, {}, must-alias), {47}: (86, {}, must-alias), {48}: (85, {}, must-alias), {49}: (84, {}, must-alias), {50}: (83, {}, must-alias), {51}: (29, {}, must-alias), {52}: (87, {}, must-alias), {53}: (95, {}, must-alias), {54}: (94, {}, must-alias), {55}: (93, {}, must-alias), {56}: (92, {}, must-alias), {57}: (91, {}, must-alias), {58}: (90, {}, must-alias), {59}: (89, {}, must-alias), {60}: (88, {}, must-alias), {61}: (28, {}, must-alias), {62}: (96, {}, must-alias), {63}: (100, {}, must-alias), {64}: (99, {}, must-alias), {65}: (98, {}, must-alias), {66}: (97, {}, must-alias), {67}: (27, {}, must-alias), {68}: (101, {}, must-alias), {69}: (109, {}, must-alias), {70}: (108, {}, must-alias), {71}: (107, {}, must-alias), {72}: (106, {}, must-alias), {73}: (105, {}, must-alias), {74}: (104, {}, must-alias), {75}: (103, {}, must-alias), {76}: (102, {}, must-alias), {77}: (26, {}, must-alias), {78}: (110, {}, must-alias), {79}: (114, {}, must-alias), {80}: (113, {}, must-alias), {81}: (112, {}, must-alias), {82}: (111, {}, must-alias), {83}: (25, {}, must-alias), {84}: (115, {}, must-alias), {85}: (123, {}, must-alias), {86}: (122, {}, must-alias), {87}: (121, {}, must-alias), {88}: (120, {}, must-alias), {89}: (119, {}, must-alias), {90}: (118, {}, must-alias), {91}: (117, {}, must-alias), {92}: (116, {}, must-alias), {93}: (24, {}, must-alias), {94}: (124, {}, must-alias), {95}: (128, {}, must-alias), {96}: (127, {}, must-alias), {97}: (126, {}, must-alias), {98}: (125, {}, must-alias), {99}: (23, {}, must-alias), {100}: (129, {}, must-alias), {101}: (137, {}, must-alias), {102}: (136, {}, must-alias), {103}: (135, {}, must-alias), {104}: (134, {}, must-alias), {105}: (133, {}, must-alias), {106}: (132, {}, must-alias), {107}: (131, {}, must-alias), {108}: (130, {}, must-alias), {109}: (22, {}, must-alias), {110}: (138, {}, must-alias), {111}: (142, {}, must-alias), {112}: (141, {}, must-alias), {113}: (140, {}, must-alias), {114}: (139, {}, must-alias), {115}: (21, {}, must-alias), {116}: (143, {}, must-alias), {117}: (151, {}, must-alias), {118}: (150, {}, must-alias), {119}: (149, {}, must-alias), {120}: (148, {}, must-alias), {121}: (147, {}, must-alias), {122}: (146, {}, must-alias), {123}: (145, {}, must-alias), {124}: (144, {}, must-alias), {125}: (20, {}, must-alias), {126}: (152, {}, must-alias), {127}: (156, {}, must-alias), {128}: (155, {}, must-alias), {129}: (154, {}, must-alias), {130}: (153, {}, must-alias), {131}: (19, {}, must-alias), {132}: (157, {}, must-alias), {133}: (165, {}, must-alias), {134}: (164, {}, must-alias), {135}: (163, {}, must-alias), {136}: (162, {}, must-alias), {137}: (161, {}, must-alias), {138}: (160, {}, must-alias), {139}: (159, {}, must-alias), {140}: (158, {}, must-alias), {141}: (18, {}, must-alias), {142}: (166, {}, must-alias), {143}: (170, {}, must-alias), {144}: (169, {}, must-alias), {145}: (168, {}, must-alias), {146}: (167, {}, must-alias), {147}: (17, {}, must-alias), {148}: (171, {}, must-alias), {149}: (179, {}, must-alias), {150}: (178, {}, must-alias), {151}: (177, {}, must-alias), {152}: (176, {}, must-alias), {153}: (175, {}, must-alias), {154}: (174, {}, must-alias), {155}: (173, {}, must-alias), {156}: (172, {}, must-alias), {157}: (16, {}, must-alias), {158}: (180, {}, must-alias), {159}: (184, {}, must-alias), {160}: (183, {}, must-alias), {161}: (182, {}, must-alias), {162}: (181, {}, must-alias), {163}: (15, {}, must-alias), {164}: (185, {}, must-alias), {165}: (193, {}, must-alias), {166}: (192, {}, must-alias), {167}: (191, {}, must-alias), {168}: (190, {}, must-alias), {169}: (189, {}, must-alias), {170}: (188, {}, must-alias), {171}: (187, {}, must-alias), {172}: (186, {}, must-alias), {173}: (14, {}, must-alias), {174}: (194, {}, must-alias), {175}: (198, {}, must-alias), {176}: (197, {}, must-alias), {177}: (196, {}, must-alias), {178}: (195, {}, must-alias), {179}: (13, {}, must-alias), {180}: (199, {}, must-alias), {181}: (207, {}, must-alias), {182}: (206, {}, must-alias), {183}: (205, {}, must-alias), {184}: (204, {}, must-alias), {185}: (203, {}, must-alias), {186}: (202, {}, must-alias), {187}: (201, {}, must-alias), {188}: (200, {}, must-alias), {189}: (12, {}, must-alias), {190}: (208, {}, must-alias), {191}: (212, {}, must-alias), {192}: (211, {}, must-alias), {193}: (210, {}, must-alias), {194}: (209, {}, must-alias), {195}: (11, {}, must-alias), {196}: (213, {}, must-alias), {197}: (10, {}, must-alias), {198}: (9, {}, must-alias), {199}: (6, {}, must-alias), {200}: (5, {}, must-alias), {201}: (619, {}, must-alias), {202}: (218, {}, must-alias), {203}: (216, {}, must-alias), {204}: (220, {}, must-alias), {205}: (219, {}, must-alias), {206}: (222, {}, must-alias), {207}: (221, {}, must-alias), {208}: (228, {}, must-alias), {209}: (227, {}, must-alias), {210}: (232, {}, must-alias), {211}: (231, {}, must-alias), {212}: (236, {}, must-alias), {213}: (235, {}, must-alias), {214}: (240, {}, must-alias), {215}: (239, {}, must-alias), {216}: (248, {}, must-alias), {217}: (247, {}, must-alias), {218}: (252, {}, must-alias), {219}: (251, {}, must-alias), {220}: (260, {}, must-alias), {221}: (259, {}, must-alias), {222}: (264, {}, must-alias), {223}: (263, {}, must-alias), {224}: (268, {}, must-alias), {225}: (267, {}, must-alias), {226}: (272, {}, must-alias), {227}: (271, {}, must-alias), {228}: (280, {}, must-alias), {229}: (279, {}, must-alias), {230}: (284, {}, must-alias), {231}: (283, {}, must-alias), {232}: (292, {}, must-alias), {233}: (291, {}, must-alias), {234}: (296, {}, must-alias), {235}: (295, {}, must-alias), {236}: (300, {}, must-alias), {237}: (299, {}, must-alias), {238}: (304, {}, must-alias), {239}: (303, {}, must-alias), {240}: (312, {}, must-alias), {241}: (311, {}, must-alias), {242}: (316, {}, must-alias), {243}: (315, {}, must-alias), {244}: (324, {}, must-alias), {245}: (323, {}, must-alias), {246}: (328, {}, must-alias), {247}: (327, {}, must-alias), {248}: (332, {}, must-alias), {249}: (331, {}, must-alias), {250}: (336, {}, must-alias), {251}: (335, {}, must-alias), {252}: (344, {}, must-alias), {253}: (343, {}, must-alias), {254}: (348, {}, must-alias), {255}: (347, {}, must-alias), {256}: (356, {}, must-alias), {257}: (355, {}, must-alias), {258}: (360, {}, must-alias), {259}: (359, {}, must-alias), {260}: (364, {}, must-alias), {261}: (363, {}, must-alias), {262}: (368, {}, must-alias), {263}: (367, {}, must-alias), {264}: (376, {}, must-alias), {265}: (375, {}, must-alias), {266}: (380, {}, must-alias), {267}: (379, {}, must-alias), {268}: (388, {}, must-alias), {269}: (387, {}, must-alias), {270}: (392, {}, must-alias), {271}: (391, {}, must-alias), {272}: (396, {}, must-alias), {273}: (395, {}, must-alias), {274}: (400, {}, must-alias), {275}: (399, {}, must-alias), {276}: (408, {}, must-alias), {277}: (407, {}, must-alias), {278}: (412, {}, must-alias), {279}: (411, {}, must-alias), {280}: (420, {}, must-alias), {281}: (419, {}, must-alias), {282}: (424, {}, must-alias), {283}: (423, {}, must-alias), {284}: (428, {}, must-alias), {285}: (427, {}, must-alias), {286}: (432, {}, must-alias), {287}: (431, {}, must-alias), {288}: (440, {}, must-alias), {289}: (439, {}, must-alias), {290}: (444, {}, must-alias), {291}: (443, {}, must-alias), {292}: (452, {}, must-alias), {293}: (451, {}, must-alias), {294}: (456, {}, must-alias), {295}: (455, {}, must-alias), {296}: (460, {}, must-alias), {297}: (459, {}, must-alias), {298}: (464, {}, must-alias), {299}: (463, {}, must-alias), {300}: (472, {}, must-alias), {301}: (471, {}, must-alias), {302}: (476, {}, must-alias), {303}: (475, {}, must-alias), {304}: (484, {}, must-alias), {305}: (483, {}, must-alias), {306}: (488, {}, must-alias), {307}: (487, {}, must-alias), {308}: (492, {}, must-alias), {309}: (491, {}, must-alias), {310}: (496, {}, must-alias), {311}: (495, {}, must-alias), {312}: (504, {}, must-alias), {313}: (503, {}, must-alias), {314}: (508, {}, must-alias), {315}: (507, {}, must-alias), {316}: (516, {}, must-alias), {317}: (515, {}, must-alias), {318}: (520, {}, must-alias), {319}: (519, {}, must-alias), {320}: (524, {}, must-alias), {321}: (523, {}, must-alias), {322}: (528, {}, must-alias), {323}: (527, {}, must-alias), {324}: (536, {}, must-alias), {325}: (535, {}, must-alias), {326}: (540, {}, must-alias), {327}: (539, {}, must-alias), {328}: (548, {}, must-alias), {329}: (547, {}, must-alias), {330}: (552, {}, must-alias), {331}: (551, {}, must-alias), {332}: (556, {}, must-alias), {333}: (555, {}, must-alias), {334}: (560, {}, must-alias), {335}: (559, {}, must-alias), {336}: (568, {}, must-alias), {337}: (567, {}, must-alias), {338}: (572, {}, must-alias), {339}: (571, {}, must-alias), {340}: (580, {}, must-alias), {341}: (579, {}, must-alias), {342}: (584, {}, must-alias), {343}: (583, {}, must-alias), {344}: (588, {}, must-alias), {345}: (587, {}, must-alias), {346}: (592, {}, must-alias), {347}: (591, {}, must-alias), {348}: (600, {}, must-alias), {349}: (599, {}, must-alias), {350}: (604, {}, must-alias), {351}: (603, {}, must-alias), {352}: (612, {}, must-alias), {353}: (611, {}, must-alias), {354}: (616, {}, must-alias), {355}: (615, {}, must-alias), {356}: (224, {}, must-alias), {357}: (223, {}, must-alias), {358}: (226, {}, must-alias), {359}: (225, {}, must-alias), {360}: (230, {}, must-alias), {361}: (229, {}, must-alias), {362}: (234, {}, must-alias), {363}: (233, {}, must-alias), {364}: (238, {}, must-alias), {365}: (237, {}, must-alias), {366}: (242, {}, must-alias), {367}: (241, {}, must-alias), {368}: (244, {}, must-alias), {369}: (243, {}, must-alias), {370}: (246, {}, must-alias), {371}: (245, {}, must-alias), {372}: (250, {}, must-alias), {373}: (249, {}, must-alias), {374}: (254, {}, must-alias), {375}: (253, {}, must-alias), {376}: (256, {}, must-alias), {377}: (255, {}, must-alias), {378}: (258, {}, must-alias), {379}: (257, {}, must-alias), {380}: (262, {}, must-alias), {381}: (261, {}, must-alias), {382}: (266, {}, must-alias), {383}: (265, {}, must-alias), {384}: (270, {}, must-alias), {385}: (269, {}, must-alias), {386}: (274, {}, must-alias), {387}: (273, {}, must-alias), {388}: (276, {}, must-alias), {389}: (275, {}, must-alias), {390}: (278, {}, must-alias), {391}: (277, {}, must-alias), {392}: (282, {}, must-alias), {393}: (281, {}, must-alias), {394}: (286, {}, must-alias), {395}: (285, {}, must-alias), {396}: (288, {}, must-alias), {397}: (287, {}, must-alias), {398}: (290, {}, must-alias), {399}: (289, {}, must-alias), {400}: (294, {}, must-alias), {401}: (293, {}, must-alias), {402}: (298, {}, must-alias), {403}: (297, {}, must-alias), {404}: (302, {}, must-alias), {405}: (301, {}, must-alias), {406}: (306, {}, must-alias), {407}: (305, {}, must-alias), {408}: (308, {}, must-alias), {409}: (307, {}, must-alias), {410}: (310, {}, must-alias), {411}: (309, {}, must-alias), {412}: (314, {}, must-alias), {413}: (313, {}, must-alias), {414}: (318, {}, must-alias), {415}: (317, {}, must-alias), {416}: (320, {}, must-alias), {417}: (319, {}, must-alias), {418}: (322, {}, must-alias), {419}: (321, {}, must-alias), {420}: (326, {}, must-alias), {421}: (325, {}, must-alias), {422}: (330, {}, must-alias), {423}: (329, {}, must-alias), {424}: (334, {}, must-alias), {425}: (333, {}, must-alias), {426}: (338, {}, must-alias), {427}: (337, {}, must-alias), {428}: (340, {}, must-alias), {429}: (339, {}, must-alias), {430}: (342, {}, must-alias), {431}: (341, {}, must-alias), {432}: (346, {}, must-alias), {433}: (345, {}, must-alias), {434}: (350, {}, must-alias), {435}: (349, {}, must-alias), {436}: (352, {}, must-alias), {437}: (351, {}, must-alias), {438}: (354, {}, must-alias), {439}: (353, {}, must-alias), {440}: (358, {}, must-alias), {441}: (357, {}, must-alias), {442}: (362, {}, must-alias), {443}: (361, {}, must-alias), {444}: (366, {}, must-alias), {445}: (365, {}, must-alias), {446}: (370, {}, must-alias), {447}: (369, {}, must-alias), {448}: (372, {}, must-alias), {449}: (371, {}, must-alias), {450}: (374, {}, must-alias), {451}: (373, {}, must-alias), {452}: (378, {}, must-alias), {453}: (377, {}, must-alias), {454}: (382, {}, must-alias), {455}: (381, {}, must-alias), {456}: (384, {}, must-alias), {457}: (383, {}, must-alias), {458}: (386, {}, must-alias), {459}: (385, {}, must-alias), {460}: (390, {}, must-alias), {461}: (389, {}, must-alias), {462}: (394, {}, must-alias), {463}: (393, {}, must-alias), {464}: (398, {}, must-alias), {465}: (397, {}, must-alias), {466}: (402, {}, must-alias), {467}: (401, {}, must-alias), {468}: (404, {}, must-alias), {469}: (403, {}, must-alias), {470}: (406, {}, must-alias), {471}: (405, {}, must-alias), {472}: (410, {}, must-alias), {473}: (409, {}, must-alias), {474}: (414, {}, must-alias), {475}: (413, {}, must-alias), {476}: (416, {}, must-alias), {477}: (415, {}, must-alias), {478}: (418, {}, must-alias), {479}: (417, {}, must-alias), {480}: (422, {}, must-alias), {481}: (421, {}, must-alias), {482}: (426, {}, must-alias), {483}: (425, {}, must-alias), {484}: (430, {}, must-alias), {485}: (429, {}, must-alias), {486}: (434, {}, must-alias), {487}: (433, {}, must-alias), {488}: (436, {}, must-alias), {489}: (435, {}, must-alias), {490}: (438, {}, must-alias), {491}: (437, {}, must-alias), {492}: (442, {}, must-alias), {493}: (441, {}, must-alias), {494}: (446, {}, must-alias), {495}: (445, {}, must-alias), {496}: (448, {}, must-alias), {497}: (447, {}, must-alias), {498}: (450, {}, must-alias), {499}: (449, {}, must-alias), {500}: (454, {}, must-alias), {501}: (453, {}, must-alias), {502}: (458, {}, must-alias), {503}: (457, {}, must-alias), {504}: (462, {}, must-alias), {505}: (461, {}, must-alias), {506}: (466, {}, must-alias), {507}: (465, {}, must-alias), {508}: (468, {}, must-alias), {509}: (467, {}, must-alias), {510}: (470, {}, must-alias), {511}: (469, {}, must-alias), {512}: (474, {}, must-alias), {513}: (473, {}, must-alias), {514}: (478, {}, must-alias), {515}: (477, {}, must-alias), {516}: (480, {}, must-alias), {517}: (479, {}, must-alias), {518}: (482, {}, must-alias), {519}: (481, {}, must-alias), {520}: (486, {}, must-alias), {521}: (485, {}, must-alias), {522}: (490, {}, must-alias), {523}: (489, {}, must-alias), {524}: (494, {}, must-alias), {525}: (493, {}, must-alias), {526}: (498, {}, must-alias), {527}: (497, {}, must-alias), {528}: (500, {}, must-alias), {529}: (499, {}, must-alias), {530}: (502, {}, must-alias), {531}: (501, {}, must-alias), {532}: (506, {}, must-alias), {533}: (505, {}, must-alias), {534}: (510, {}, must-alias), {535}: (509, {}, must-alias), {536}: (512, {}, must-alias), {537}: (511, {}, must-alias), {538}: (514, {}, must-alias), {539}: (513, {}, must-alias), {540}: (518, {}, must-alias), {541}: (517, {}, must-alias), {542}: (522, {}, must-alias), {543}: (521, {}, must-alias), {544}: (526, {}, must-alias), {545}: (525, {}, must-alias), {546}: (530, {}, must-alias), {547}: (529, {}, must-alias), {548}: (532, {}, must-alias), {549}: (531, {}, must-alias), {550}: (534, {}, must-alias), {551}: (533, {}, must-alias), {552}: (538, {}, must-alias), {553}: (537, {}, must-alias), {554}: (542, {}, must-alias), {555}: (541, {}, must-alias), {556}: (544, {}, must-alias), {557}: (543, {}, must-alias), {558}: (546, {}, must-alias), {559}: (545, {}, must-alias), {560}: (550, {}, must-alias), {561}: (549, {}, must-alias), {562}: (554, {}, must-alias), {563}: (553, {}, must-alias), {564}: (558, {}, must-alias), {565}: (557, {}, must-alias), {566}: (562, {}, must-alias), {567}: (561, {}, must-alias), {568}: (564, {}, must-alias), {569}: (563, {}, must-alias), {570}: (566, {}, must-alias), {571}: (565, {}, must-alias), {572}: (570, {}, must-alias), {573}: (569, {}, must-alias), {574}: (574, {}, must-alias), {575}: (573, {}, must-alias), {576}: (576, {}, must-alias), {577}: (575, {}, must-alias), {578}: (578, {}, must-alias), {579}: (577, {}, must-alias), {580}: (582, {}, must-alias), {581}: (581, {}, must-alias), {582}: (586, {}, must-alias), {583}: (585, {}, must-alias), {584}: (590, {}, must-alias), {585}: (589, {}, must-alias), {586}: (594, {}, must-alias), {587}: (593, {}, must-alias), {588}: (596, {}, must-alias), {589}: (595, {}, must-alias), {590}: (598, {}, must-alias), {591}: (597, {}, must-alias), {592}: (602, {}, must-alias), {593}: (601, {}, must-alias), {594}: (606, {}, must-alias), {595}: (605, {}, must-alias), {596}: (608, {}, must-alias), {597}: (607, {}, must-alias), {598}: (610, {}, must-alias), {599}: (609, {}, must-alias), {600}: (614, {}, must-alias), {601}: (613, {}, must-alias), {602}: (618, {}, must-alias), {603}: (617, {}, must-alias) }

%MaxComputation.1158 (x.1159: bf16[], y.1160: bf16[]) -> bf16[] {
  %x.1159 = bf16[] parameter(0)
  %y.1160 = bf16[] parameter(1)
  ROOT %maximum.1161 = bf16[] maximum(bf16[] %x.1159, bf16[] %y.1160)
}

%AddComputation.1167 (x.1168: bf16[], y.1169: bf16[]) -> bf16[] {
  %x.1168 = bf16[] parameter(0)
  %y.1169 = bf16[] parameter(1)
  ROOT %add.1170 = bf16[] add(bf16[] %x.1168, bf16[] %y.1169)
}

%MaxComputation.1453 (x.1454: bf16[], y.1455: bf16[]) -> bf16[] {
  %x.1454 = bf16[] parameter(0)
  %y.1455 = bf16[] parameter(1)
  ROOT %maximum.1456 = bf16[] maximum(bf16[] %x.1454, bf16[] %y.1455)
}

%AddComputation.1462 (x.1463: bf16[], y.1464: bf16[]) -> bf16[] {
  %x.1463 = bf16[] parameter(0)
  %y.1464 = bf16[] parameter(1)
  ROOT %add.1465 = bf16[] add(bf16[] %x.1463, bf16[] %y.1464)
}

%MaxComputation.1748 (x.1749: bf16[], y.1750: bf16[]) -> bf16[] {
  %x.1749 = bf16[] parameter(0)
  %y.1750 = bf16[] parameter(1)
  ROOT %maximum.1751 = bf16[] maximum(bf16[] %x.1749, bf16[] %y.1750)
}

%AddComputation.1757 (x.1758: bf16[], y.1759: bf16[]) -> bf16[] {
  %x.1758 = bf16[] parameter(0)
  %y.1759 = bf16[] parameter(1)
  ROOT %add.1760 = bf16[] add(bf16[] %x.1758, bf16[] %y.1759)
}

%MaxComputation.2043 (x.2044: bf16[], y.2045: bf16[]) -> bf16[] {
  %x.2044 = bf16[] parameter(0)
  %y.2045 = bf16[] parameter(1)
  ROOT %maximum.2046 = bf16[] maximum(bf16[] %x.2044, bf16[] %y.2045)
}

%AddComputation.2052 (x.2053: bf16[], y.2054: bf16[]) -> bf16[] {
  %x.2053 = bf16[] parameter(0)
  %y.2054 = bf16[] parameter(1)
  ROOT %add.2055 = bf16[] add(bf16[] %x.2053, bf16[] %y.2054)
}

%MaxComputation.2338 (x.2339: bf16[], y.2340: bf16[]) -> bf16[] {
  %x.2339 = bf16[] parameter(0)
  %y.2340 = bf16[] parameter(1)
  ROOT %maximum.2341 = bf16[] maximum(bf16[] %x.2339, bf16[] %y.2340)
}

%AddComputation.2347 (x.2348: bf16[], y.2349: bf16[]) -> bf16[] {
  %x.2348 = bf16[] parameter(0)
  %y.2349 = bf16[] parameter(1)
  ROOT %add.2350 = bf16[] add(bf16[] %x.2348, bf16[] %y.2349)
}

%MaxComputation.2633 (x.2634: bf16[], y.2635: bf16[]) -> bf16[] {
  %x.2634 = bf16[] parameter(0)
  %y.2635 = bf16[] parameter(1)
  ROOT %maximum.2636 = bf16[] maximum(bf16[] %x.2634, bf16[] %y.2635)
}

%AddComputation.2642 (x.2643: bf16[], y.2644: bf16[]) -> bf16[] {
  %x.2643 = bf16[] parameter(0)
  %y.2644 = bf16[] parameter(1)
  ROOT %add.2645 = bf16[] add(bf16[] %x.2643, bf16[] %y.2644)
}

%MaxComputation.2928 (x.2929: bf16[], y.2930: bf16[]) -> bf16[] {
  %x.2929 = bf16[] parameter(0)
  %y.2930 = bf16[] parameter(1)
  ROOT %maximum.2931 = bf16[] maximum(bf16[] %x.2929, bf16[] %y.2930)
}

%AddComputation.2937 (x.2938: bf16[], y.2939: bf16[]) -> bf16[] {
  %x.2938 = bf16[] parameter(0)
  %y.2939 = bf16[] parameter(1)
  ROOT %add.2940 = bf16[] add(bf16[] %x.2938, bf16[] %y.2939)
}

%MaxComputation.3223 (x.3224: bf16[], y.3225: bf16[]) -> bf16[] {
  %x.3224 = bf16[] parameter(0)
  %y.3225 = bf16[] parameter(1)
  ROOT %maximum.3226 = bf16[] maximum(bf16[] %x.3224, bf16[] %y.3225)
}

%AddComputation.3232 (x.3233: bf16[], y.3234: bf16[]) -> bf16[] {
  %x.3233 = bf16[] parameter(0)
  %y.3234 = bf16[] parameter(1)
  ROOT %add.3235 = bf16[] add(bf16[] %x.3233, bf16[] %y.3234)
}

%MaxComputation.3518 (x.3519: bf16[], y.3520: bf16[]) -> bf16[] {
  %x.3519 = bf16[] parameter(0)
  %y.3520 = bf16[] parameter(1)
  ROOT %maximum.3521 = bf16[] maximum(bf16[] %x.3519, bf16[] %y.3520)
}

%AddComputation.3527 (x.3528: bf16[], y.3529: bf16[]) -> bf16[] {
  %x.3528 = bf16[] parameter(0)
  %y.3529 = bf16[] parameter(1)
  ROOT %add.3530 = bf16[] add(bf16[] %x.3528, bf16[] %y.3529)
}

%MaxComputation.3813 (x.3814: bf16[], y.3815: bf16[]) -> bf16[] {
  %x.3814 = bf16[] parameter(0)
  %y.3815 = bf16[] parameter(1)
  ROOT %maximum.3816 = bf16[] maximum(bf16[] %x.3814, bf16[] %y.3815)
}

%AddComputation.3822 (x.3823: bf16[], y.3824: bf16[]) -> bf16[] {
  %x.3823 = bf16[] parameter(0)
  %y.3824 = bf16[] parameter(1)
  ROOT %add.3825 = bf16[] add(bf16[] %x.3823, bf16[] %y.3824)
}

%MaxComputation.4108 (x.4109: bf16[], y.4110: bf16[]) -> bf16[] {
  %x.4109 = bf16[] parameter(0)
  %y.4110 = bf16[] parameter(1)
  ROOT %maximum.4111 = bf16[] maximum(bf16[] %x.4109, bf16[] %y.4110)
}

%AddComputation.4117 (x.4118: bf16[], y.4119: bf16[]) -> bf16[] {
  %x.4118 = bf16[] parameter(0)
  %y.4119 = bf16[] parameter(1)
  ROOT %add.4120 = bf16[] add(bf16[] %x.4118, bf16[] %y.4119)
}

%MaxComputation.4403 (x.4404: bf16[], y.4405: bf16[]) -> bf16[] {
  %x.4404 = bf16[] parameter(0)
  %y.4405 = bf16[] parameter(1)
  ROOT %maximum.4406 = bf16[] maximum(bf16[] %x.4404, bf16[] %y.4405)
}

%AddComputation.4412 (x.4413: bf16[], y.4414: bf16[]) -> bf16[] {
  %x.4413 = bf16[] parameter(0)
  %y.4414 = bf16[] parameter(1)
  ROOT %add.4415 = bf16[] add(bf16[] %x.4413, bf16[] %y.4414)
}

%AddComputation.4663 (x.4664: bf16[], y.4665: bf16[]) -> bf16[] {
  %x.4664 = bf16[] parameter(0)
  %y.4665 = bf16[] parameter(1)
  ROOT %add.4666 = bf16[] add(bf16[] %x.4664, bf16[] %y.4665)
}

%AddComputation.4673 (x.4674: bf16[], y.4675: bf16[]) -> bf16[] {
  %x.4674 = bf16[] parameter(0)
  %y.4675 = bf16[] parameter(1)
  ROOT %add.4676 = bf16[] add(bf16[] %x.4674, bf16[] %y.4675)
}

%AddComputation.4685 (x.4686: bf16[], y.4687: bf16[]) -> bf16[] {
  %x.4686 = bf16[] parameter(0)
  %y.4687 = bf16[] parameter(1)
  ROOT %add.4688 = bf16[] add(bf16[] %x.4686, bf16[] %y.4687)
}

%AddComputation.4716 (x.4717: bf16[], y.4718: bf16[]) -> bf16[] {
  %x.4717 = bf16[] parameter(0)
  %y.4718 = bf16[] parameter(1)
  ROOT %add.4719 = bf16[] add(bf16[] %x.4717, bf16[] %y.4718)
}

%AddComputation.4726 (x.4727: bf16[], y.4728: bf16[]) -> bf16[] {
  %x.4727 = bf16[] parameter(0)
  %y.4728 = bf16[] parameter(1)
  ROOT %add.4729 = bf16[] add(bf16[] %x.4727, bf16[] %y.4728)
}

%AddComputation.4741 (x.4742: bf16[], y.4743: bf16[]) -> bf16[] {
  %x.4742 = bf16[] parameter(0)
  %y.4743 = bf16[] parameter(1)
  ROOT %add.4744 = bf16[] add(bf16[] %x.4742, bf16[] %y.4743)
}

%AddComputation.4767 (x.4768: bf16[], y.4769: bf16[]) -> bf16[] {
  %x.4768 = bf16[] parameter(0)
  %y.4769 = bf16[] parameter(1)
  ROOT %add.4770 = bf16[] add(bf16[] %x.4768, bf16[] %y.4769)
}

%AddComputation.4777 (x.4778: bf16[], y.4779: bf16[]) -> bf16[] {
  %x.4778 = bf16[] parameter(0)
  %y.4779 = bf16[] parameter(1)
  ROOT %add.4780 = bf16[] add(bf16[] %x.4778, bf16[] %y.4779)
}

%AddComputation.4792 (x.4793: bf16[], y.4794: bf16[]) -> bf16[] {
  %x.4793 = bf16[] parameter(0)
  %y.4794 = bf16[] parameter(1)
  ROOT %add.4795 = bf16[] add(bf16[] %x.4793, bf16[] %y.4794)
}

%AddComputation.4802 (x.4803: bf16[], y.4804: bf16[]) -> bf16[] {
  %x.4803 = bf16[] parameter(0)
  %y.4804 = bf16[] parameter(1)
  ROOT %add.4805 = bf16[] add(bf16[] %x.4803, bf16[] %y.4804)
}

%AddComputation.4837 (x.4838: bf16[], y.4839: bf16[]) -> bf16[] {
  %x.4838 = bf16[] parameter(0)
  %y.4839 = bf16[] parameter(1)
  ROOT %add.4840 = bf16[] add(bf16[] %x.4838, bf16[] %y.4839)
}

%AddComputation.4847 (x.4848: bf16[], y.4849: bf16[]) -> bf16[] {
  %x.4848 = bf16[] parameter(0)
  %y.4849 = bf16[] parameter(1)
  ROOT %add.4850 = bf16[] add(bf16[] %x.4848, bf16[] %y.4849)
}

%AddComputation.4861 (x.4862: bf16[], y.4863: bf16[]) -> bf16[] {
  %x.4862 = bf16[] parameter(0)
  %y.4863 = bf16[] parameter(1)
  ROOT %add.4864 = bf16[] add(bf16[] %x.4862, bf16[] %y.4863)
}

%AddComputation.4883 (x.4884: bf16[], y.4885: bf16[]) -> bf16[] {
  %x.4884 = bf16[] parameter(0)
  %y.4885 = bf16[] parameter(1)
  ROOT %add.4886 = bf16[] add(bf16[] %x.4884, bf16[] %y.4885)
}

%AddComputation.4893 (x.4894: bf16[], y.4895: bf16[]) -> bf16[] {
  %x.4894 = bf16[] parameter(0)
  %y.4895 = bf16[] parameter(1)
  ROOT %add.4896 = bf16[] add(bf16[] %x.4894, bf16[] %y.4895)
}

%AddComputation.4907 (x.4908: bf16[], y.4909: bf16[]) -> bf16[] {
  %x.4908 = bf16[] parameter(0)
  %y.4909 = bf16[] parameter(1)
  ROOT %add.4910 = bf16[] add(bf16[] %x.4908, bf16[] %y.4909)
}

%AddComputation.4926 (x.4927: bf16[], y.4928: bf16[]) -> bf16[] {
  %x.4927 = bf16[] parameter(0)
  %y.4928 = bf16[] parameter(1)
  ROOT %add.4929 = bf16[] add(bf16[] %x.4927, bf16[] %y.4928)
}

%AddComputation.4936 (x.4937: bf16[], y.4938: bf16[]) -> bf16[] {
  %x.4937 = bf16[] parameter(0)
  %y.4938 = bf16[] parameter(1)
  ROOT %add.4939 = bf16[] add(bf16[] %x.4937, bf16[] %y.4938)
}

%AddComputation.4951 (x.4952: bf16[], y.4953: bf16[]) -> bf16[] {
  %x.4952 = bf16[] parameter(0)
  %y.4953 = bf16[] parameter(1)
  ROOT %add.4954 = bf16[] add(bf16[] %x.4952, bf16[] %y.4953)
}

%AddComputation.4961 (x.4962: bf16[], y.4963: bf16[]) -> bf16[] {
  %x.4962 = bf16[] parameter(0)
  %y.4963 = bf16[] parameter(1)
  ROOT %add.4964 = bf16[] add(bf16[] %x.4962, bf16[] %y.4963)
}

%AddComputation.4996 (x.4997: bf16[], y.4998: bf16[]) -> bf16[] {
  %x.4997 = bf16[] parameter(0)
  %y.4998 = bf16[] parameter(1)
  ROOT %add.4999 = bf16[] add(bf16[] %x.4997, bf16[] %y.4998)
}

%AddComputation.5006 (x.5007: bf16[], y.5008: bf16[]) -> bf16[] {
  %x.5007 = bf16[] parameter(0)
  %y.5008 = bf16[] parameter(1)
  ROOT %add.5009 = bf16[] add(bf16[] %x.5007, bf16[] %y.5008)
}

%AddComputation.5023 (x.5024: bf16[], y.5025: bf16[]) -> bf16[] {
  %x.5024 = bf16[] parameter(0)
  %y.5025 = bf16[] parameter(1)
  ROOT %add.5026 = bf16[] add(bf16[] %x.5024, bf16[] %y.5025)
}

%AddComputation.5071 (x.5072: bf16[], y.5073: bf16[]) -> bf16[] {
  %x.5072 = bf16[] parameter(0)
  %y.5073 = bf16[] parameter(1)
  ROOT %add.5074 = bf16[] add(bf16[] %x.5072, bf16[] %y.5073)
}

%AddComputation.5084 (x.5085: bf16[], y.5086: bf16[]) -> bf16[] {
  %x.5085 = bf16[] parameter(0)
  %y.5086 = bf16[] parameter(1)
  ROOT %add.5087 = bf16[] add(bf16[] %x.5085, bf16[] %y.5086)
}

%AddComputation.5132 (x.5133: bf16[], y.5134: bf16[]) -> bf16[] {
  %x.5133 = bf16[] parameter(0)
  %y.5134 = bf16[] parameter(1)
  ROOT %add.5135 = bf16[] add(bf16[] %x.5133, bf16[] %y.5134)
}

%AddComputation.5173 (x.5174: bf16[], y.5175: bf16[]) -> bf16[] {
  %x.5174 = bf16[] parameter(0)
  %y.5175 = bf16[] parameter(1)
  ROOT %add.5176 = bf16[] add(bf16[] %x.5174, bf16[] %y.5175)
}

%AddComputation.5214 (x.5215: bf16[], y.5216: bf16[]) -> bf16[] {
  %x.5215 = bf16[] parameter(0)
  %y.5216 = bf16[] parameter(1)
  ROOT %add.5217 = bf16[] add(bf16[] %x.5215, bf16[] %y.5216)
}

%AddComputation.5224 (x.5225: bf16[], y.5226: bf16[]) -> bf16[] {
  %x.5225 = bf16[] parameter(0)
  %y.5226 = bf16[] parameter(1)
  ROOT %add.5227 = bf16[] add(bf16[] %x.5225, bf16[] %y.5226)
}

%AddComputation.5239 (x.5240: bf16[], y.5241: bf16[]) -> bf16[] {
  %x.5240 = bf16[] parameter(0)
  %y.5241 = bf16[] parameter(1)
  ROOT %add.5242 = bf16[] add(bf16[] %x.5240, bf16[] %y.5241)
}

%AddComputation.5249 (x.5250: bf16[], y.5251: bf16[]) -> bf16[] {
  %x.5250 = bf16[] parameter(0)
  %y.5251 = bf16[] parameter(1)
  ROOT %add.5252 = bf16[] add(bf16[] %x.5250, bf16[] %y.5251)
}

%AddComputation.5284 (x.5285: bf16[], y.5286: bf16[]) -> bf16[] {
  %x.5285 = bf16[] parameter(0)
  %y.5286 = bf16[] parameter(1)
  ROOT %add.5287 = bf16[] add(bf16[] %x.5285, bf16[] %y.5286)
}

%AddComputation.5294 (x.5295: bf16[], y.5296: bf16[]) -> bf16[] {
  %x.5295 = bf16[] parameter(0)
  %y.5296 = bf16[] parameter(1)
  ROOT %add.5297 = bf16[] add(bf16[] %x.5295, bf16[] %y.5296)
}

%AddComputation.5308 (x.5309: bf16[], y.5310: bf16[]) -> bf16[] {
  %x.5309 = bf16[] parameter(0)
  %y.5310 = bf16[] parameter(1)
  ROOT %add.5311 = bf16[] add(bf16[] %x.5309, bf16[] %y.5310)
}

%AddComputation.5330 (x.5331: bf16[], y.5332: bf16[]) -> bf16[] {
  %x.5331 = bf16[] parameter(0)
  %y.5332 = bf16[] parameter(1)
  ROOT %add.5333 = bf16[] add(bf16[] %x.5331, bf16[] %y.5332)
}

%AddComputation.5340 (x.5341: bf16[], y.5342: bf16[]) -> bf16[] {
  %x.5341 = bf16[] parameter(0)
  %y.5342 = bf16[] parameter(1)
  ROOT %add.5343 = bf16[] add(bf16[] %x.5341, bf16[] %y.5342)
}

%AddComputation.5354 (x.5355: bf16[], y.5356: bf16[]) -> bf16[] {
  %x.5355 = bf16[] parameter(0)
  %y.5356 = bf16[] parameter(1)
  ROOT %add.5357 = bf16[] add(bf16[] %x.5355, bf16[] %y.5356)
}

%AddComputation.5373 (x.5374: bf16[], y.5375: bf16[]) -> bf16[] {
  %x.5374 = bf16[] parameter(0)
  %y.5375 = bf16[] parameter(1)
  ROOT %add.5376 = bf16[] add(bf16[] %x.5374, bf16[] %y.5375)
}

%AddComputation.5383 (x.5384: bf16[], y.5385: bf16[]) -> bf16[] {
  %x.5384 = bf16[] parameter(0)
  %y.5385 = bf16[] parameter(1)
  ROOT %add.5386 = bf16[] add(bf16[] %x.5384, bf16[] %y.5385)
}

%AddComputation.5398 (x.5399: bf16[], y.5400: bf16[]) -> bf16[] {
  %x.5399 = bf16[] parameter(0)
  %y.5400 = bf16[] parameter(1)
  ROOT %add.5401 = bf16[] add(bf16[] %x.5399, bf16[] %y.5400)
}

%AddComputation.5408 (x.5409: bf16[], y.5410: bf16[]) -> bf16[] {
  %x.5409 = bf16[] parameter(0)
  %y.5410 = bf16[] parameter(1)
  ROOT %add.5411 = bf16[] add(bf16[] %x.5409, bf16[] %y.5410)
}

%AddComputation.5443 (x.5444: bf16[], y.5445: bf16[]) -> bf16[] {
  %x.5444 = bf16[] parameter(0)
  %y.5445 = bf16[] parameter(1)
  ROOT %add.5446 = bf16[] add(bf16[] %x.5444, bf16[] %y.5445)
}

%AddComputation.5453 (x.5454: bf16[], y.5455: bf16[]) -> bf16[] {
  %x.5454 = bf16[] parameter(0)
  %y.5455 = bf16[] parameter(1)
  ROOT %add.5456 = bf16[] add(bf16[] %x.5454, bf16[] %y.5455)
}

%AddComputation.5470 (x.5471: bf16[], y.5472: bf16[]) -> bf16[] {
  %x.5471 = bf16[] parameter(0)
  %y.5472 = bf16[] parameter(1)
  ROOT %add.5473 = bf16[] add(bf16[] %x.5471, bf16[] %y.5472)
}

%AddComputation.5518 (x.5519: bf16[], y.5520: bf16[]) -> bf16[] {
  %x.5519 = bf16[] parameter(0)
  %y.5520 = bf16[] parameter(1)
  ROOT %add.5521 = bf16[] add(bf16[] %x.5519, bf16[] %y.5520)
}

%AddComputation.5531 (x.5532: bf16[], y.5533: bf16[]) -> bf16[] {
  %x.5532 = bf16[] parameter(0)
  %y.5533 = bf16[] parameter(1)
  ROOT %add.5534 = bf16[] add(bf16[] %x.5532, bf16[] %y.5533)
}

%AddComputation.5579 (x.5580: bf16[], y.5581: bf16[]) -> bf16[] {
  %x.5580 = bf16[] parameter(0)
  %y.5581 = bf16[] parameter(1)
  ROOT %add.5582 = bf16[] add(bf16[] %x.5580, bf16[] %y.5581)
}

%AddComputation.5620 (x.5621: bf16[], y.5622: bf16[]) -> bf16[] {
  %x.5621 = bf16[] parameter(0)
  %y.5622 = bf16[] parameter(1)
  ROOT %add.5623 = bf16[] add(bf16[] %x.5621, bf16[] %y.5622)
}

%AddComputation.5661 (x.5662: bf16[], y.5663: bf16[]) -> bf16[] {
  %x.5662 = bf16[] parameter(0)
  %y.5663 = bf16[] parameter(1)
  ROOT %add.5664 = bf16[] add(bf16[] %x.5662, bf16[] %y.5663)
}

%AddComputation.5671 (x.5672: bf16[], y.5673: bf16[]) -> bf16[] {
  %x.5672 = bf16[] parameter(0)
  %y.5673 = bf16[] parameter(1)
  ROOT %add.5674 = bf16[] add(bf16[] %x.5672, bf16[] %y.5673)
}

%AddComputation.5686 (x.5687: bf16[], y.5688: bf16[]) -> bf16[] {
  %x.5687 = bf16[] parameter(0)
  %y.5688 = bf16[] parameter(1)
  ROOT %add.5689 = bf16[] add(bf16[] %x.5687, bf16[] %y.5688)
}

%AddComputation.5696 (x.5697: bf16[], y.5698: bf16[]) -> bf16[] {
  %x.5697 = bf16[] parameter(0)
  %y.5698 = bf16[] parameter(1)
  ROOT %add.5699 = bf16[] add(bf16[] %x.5697, bf16[] %y.5698)
}

%AddComputation.5731 (x.5732: bf16[], y.5733: bf16[]) -> bf16[] {
  %x.5732 = bf16[] parameter(0)
  %y.5733 = bf16[] parameter(1)
  ROOT %add.5734 = bf16[] add(bf16[] %x.5732, bf16[] %y.5733)
}

%AddComputation.5741 (x.5742: bf16[], y.5743: bf16[]) -> bf16[] {
  %x.5742 = bf16[] parameter(0)
  %y.5743 = bf16[] parameter(1)
  ROOT %add.5744 = bf16[] add(bf16[] %x.5742, bf16[] %y.5743)
}

%AddComputation.5755 (x.5756: bf16[], y.5757: bf16[]) -> bf16[] {
  %x.5756 = bf16[] parameter(0)
  %y.5757 = bf16[] parameter(1)
  ROOT %add.5758 = bf16[] add(bf16[] %x.5756, bf16[] %y.5757)
}

%AddComputation.5777 (x.5778: bf16[], y.5779: bf16[]) -> bf16[] {
  %x.5778 = bf16[] parameter(0)
  %y.5779 = bf16[] parameter(1)
  ROOT %add.5780 = bf16[] add(bf16[] %x.5778, bf16[] %y.5779)
}

%AddComputation.5787 (x.5788: bf16[], y.5789: bf16[]) -> bf16[] {
  %x.5788 = bf16[] parameter(0)
  %y.5789 = bf16[] parameter(1)
  ROOT %add.5790 = bf16[] add(bf16[] %x.5788, bf16[] %y.5789)
}

%AddComputation.5801 (x.5802: bf16[], y.5803: bf16[]) -> bf16[] {
  %x.5802 = bf16[] parameter(0)
  %y.5803 = bf16[] parameter(1)
  ROOT %add.5804 = bf16[] add(bf16[] %x.5802, bf16[] %y.5803)
}

%AddComputation.5820 (x.5821: bf16[], y.5822: bf16[]) -> bf16[] {
  %x.5821 = bf16[] parameter(0)
  %y.5822 = bf16[] parameter(1)
  ROOT %add.5823 = bf16[] add(bf16[] %x.5821, bf16[] %y.5822)
}

%AddComputation.5830 (x.5831: bf16[], y.5832: bf16[]) -> bf16[] {
  %x.5831 = bf16[] parameter(0)
  %y.5832 = bf16[] parameter(1)
  ROOT %add.5833 = bf16[] add(bf16[] %x.5831, bf16[] %y.5832)
}

%AddComputation.5845 (x.5846: bf16[], y.5847: bf16[]) -> bf16[] {
  %x.5846 = bf16[] parameter(0)
  %y.5847 = bf16[] parameter(1)
  ROOT %add.5848 = bf16[] add(bf16[] %x.5846, bf16[] %y.5847)
}

%AddComputation.5855 (x.5856: bf16[], y.5857: bf16[]) -> bf16[] {
  %x.5856 = bf16[] parameter(0)
  %y.5857 = bf16[] parameter(1)
  ROOT %add.5858 = bf16[] add(bf16[] %x.5856, bf16[] %y.5857)
}

%AddComputation.5890 (x.5891: bf16[], y.5892: bf16[]) -> bf16[] {
  %x.5891 = bf16[] parameter(0)
  %y.5892 = bf16[] parameter(1)
  ROOT %add.5893 = bf16[] add(bf16[] %x.5891, bf16[] %y.5892)
}

%AddComputation.5900 (x.5901: bf16[], y.5902: bf16[]) -> bf16[] {
  %x.5901 = bf16[] parameter(0)
  %y.5902 = bf16[] parameter(1)
  ROOT %add.5903 = bf16[] add(bf16[] %x.5901, bf16[] %y.5902)
}

%AddComputation.5917 (x.5918: bf16[], y.5919: bf16[]) -> bf16[] {
  %x.5918 = bf16[] parameter(0)
  %y.5919 = bf16[] parameter(1)
  ROOT %add.5920 = bf16[] add(bf16[] %x.5918, bf16[] %y.5919)
}

%AddComputation.5965 (x.5966: bf16[], y.5967: bf16[]) -> bf16[] {
  %x.5966 = bf16[] parameter(0)
  %y.5967 = bf16[] parameter(1)
  ROOT %add.5968 = bf16[] add(bf16[] %x.5966, bf16[] %y.5967)
}

%AddComputation.5978 (x.5979: bf16[], y.5980: bf16[]) -> bf16[] {
  %x.5979 = bf16[] parameter(0)
  %y.5980 = bf16[] parameter(1)
  ROOT %add.5981 = bf16[] add(bf16[] %x.5979, bf16[] %y.5980)
}

%AddComputation.6026 (x.6027: bf16[], y.6028: bf16[]) -> bf16[] {
  %x.6027 = bf16[] parameter(0)
  %y.6028 = bf16[] parameter(1)
  ROOT %add.6029 = bf16[] add(bf16[] %x.6027, bf16[] %y.6028)
}

%AddComputation.6067 (x.6068: bf16[], y.6069: bf16[]) -> bf16[] {
  %x.6068 = bf16[] parameter(0)
  %y.6069 = bf16[] parameter(1)
  ROOT %add.6070 = bf16[] add(bf16[] %x.6068, bf16[] %y.6069)
}

%AddComputation.6108 (x.6109: bf16[], y.6110: bf16[]) -> bf16[] {
  %x.6109 = bf16[] parameter(0)
  %y.6110 = bf16[] parameter(1)
  ROOT %add.6111 = bf16[] add(bf16[] %x.6109, bf16[] %y.6110)
}

%AddComputation.6118 (x.6119: bf16[], y.6120: bf16[]) -> bf16[] {
  %x.6119 = bf16[] parameter(0)
  %y.6120 = bf16[] parameter(1)
  ROOT %add.6121 = bf16[] add(bf16[] %x.6119, bf16[] %y.6120)
}

%AddComputation.6133 (x.6134: bf16[], y.6135: bf16[]) -> bf16[] {
  %x.6134 = bf16[] parameter(0)
  %y.6135 = bf16[] parameter(1)
  ROOT %add.6136 = bf16[] add(bf16[] %x.6134, bf16[] %y.6135)
}

%AddComputation.6143 (x.6144: bf16[], y.6145: bf16[]) -> bf16[] {
  %x.6144 = bf16[] parameter(0)
  %y.6145 = bf16[] parameter(1)
  ROOT %add.6146 = bf16[] add(bf16[] %x.6144, bf16[] %y.6145)
}

%AddComputation.6178 (x.6179: bf16[], y.6180: bf16[]) -> bf16[] {
  %x.6179 = bf16[] parameter(0)
  %y.6180 = bf16[] parameter(1)
  ROOT %add.6181 = bf16[] add(bf16[] %x.6179, bf16[] %y.6180)
}

%AddComputation.6188 (x.6189: bf16[], y.6190: bf16[]) -> bf16[] {
  %x.6189 = bf16[] parameter(0)
  %y.6190 = bf16[] parameter(1)
  ROOT %add.6191 = bf16[] add(bf16[] %x.6189, bf16[] %y.6190)
}

%AddComputation.6202 (x.6203: bf16[], y.6204: bf16[]) -> bf16[] {
  %x.6203 = bf16[] parameter(0)
  %y.6204 = bf16[] parameter(1)
  ROOT %add.6205 = bf16[] add(bf16[] %x.6203, bf16[] %y.6204)
}

%AddComputation.6224 (x.6225: bf16[], y.6226: bf16[]) -> bf16[] {
  %x.6225 = bf16[] parameter(0)
  %y.6226 = bf16[] parameter(1)
  ROOT %add.6227 = bf16[] add(bf16[] %x.6225, bf16[] %y.6226)
}

%AddComputation.6234 (x.6235: bf16[], y.6236: bf16[]) -> bf16[] {
  %x.6235 = bf16[] parameter(0)
  %y.6236 = bf16[] parameter(1)
  ROOT %add.6237 = bf16[] add(bf16[] %x.6235, bf16[] %y.6236)
}

%AddComputation.6248 (x.6249: bf16[], y.6250: bf16[]) -> bf16[] {
  %x.6249 = bf16[] parameter(0)
  %y.6250 = bf16[] parameter(1)
  ROOT %add.6251 = bf16[] add(bf16[] %x.6249, bf16[] %y.6250)
}

%AddComputation.6267 (x.6268: bf16[], y.6269: bf16[]) -> bf16[] {
  %x.6268 = bf16[] parameter(0)
  %y.6269 = bf16[] parameter(1)
  ROOT %add.6270 = bf16[] add(bf16[] %x.6268, bf16[] %y.6269)
}

%AddComputation.6277 (x.6278: bf16[], y.6279: bf16[]) -> bf16[] {
  %x.6278 = bf16[] parameter(0)
  %y.6279 = bf16[] parameter(1)
  ROOT %add.6280 = bf16[] add(bf16[] %x.6278, bf16[] %y.6279)
}

%AddComputation.6292 (x.6293: bf16[], y.6294: bf16[]) -> bf16[] {
  %x.6293 = bf16[] parameter(0)
  %y.6294 = bf16[] parameter(1)
  ROOT %add.6295 = bf16[] add(bf16[] %x.6293, bf16[] %y.6294)
}

%AddComputation.6302 (x.6303: bf16[], y.6304: bf16[]) -> bf16[] {
  %x.6303 = bf16[] parameter(0)
  %y.6304 = bf16[] parameter(1)
  ROOT %add.6305 = bf16[] add(bf16[] %x.6303, bf16[] %y.6304)
}

%AddComputation.6337 (x.6338: bf16[], y.6339: bf16[]) -> bf16[] {
  %x.6338 = bf16[] parameter(0)
  %y.6339 = bf16[] parameter(1)
  ROOT %add.6340 = bf16[] add(bf16[] %x.6338, bf16[] %y.6339)
}

%AddComputation.6347 (x.6348: bf16[], y.6349: bf16[]) -> bf16[] {
  %x.6348 = bf16[] parameter(0)
  %y.6349 = bf16[] parameter(1)
  ROOT %add.6350 = bf16[] add(bf16[] %x.6348, bf16[] %y.6349)
}

%AddComputation.6364 (x.6365: bf16[], y.6366: bf16[]) -> bf16[] {
  %x.6365 = bf16[] parameter(0)
  %y.6366 = bf16[] parameter(1)
  ROOT %add.6367 = bf16[] add(bf16[] %x.6365, bf16[] %y.6366)
}

%AddComputation.6412 (x.6413: bf16[], y.6414: bf16[]) -> bf16[] {
  %x.6413 = bf16[] parameter(0)
  %y.6414 = bf16[] parameter(1)
  ROOT %add.6415 = bf16[] add(bf16[] %x.6413, bf16[] %y.6414)
}

%AddComputation.6425 (x.6426: bf16[], y.6427: bf16[]) -> bf16[] {
  %x.6426 = bf16[] parameter(0)
  %y.6427 = bf16[] parameter(1)
  ROOT %add.6428 = bf16[] add(bf16[] %x.6426, bf16[] %y.6427)
}

%AddComputation.6473 (x.6474: bf16[], y.6475: bf16[]) -> bf16[] {
  %x.6474 = bf16[] parameter(0)
  %y.6475 = bf16[] parameter(1)
  ROOT %add.6476 = bf16[] add(bf16[] %x.6474, bf16[] %y.6475)
}

%AddComputation.6514 (x.6515: bf16[], y.6516: bf16[]) -> bf16[] {
  %x.6515 = bf16[] parameter(0)
  %y.6516 = bf16[] parameter(1)
  ROOT %add.6517 = bf16[] add(bf16[] %x.6515, bf16[] %y.6516)
}

%AddComputation.6555 (x.6556: bf16[], y.6557: bf16[]) -> bf16[] {
  %x.6556 = bf16[] parameter(0)
  %y.6557 = bf16[] parameter(1)
  ROOT %add.6558 = bf16[] add(bf16[] %x.6556, bf16[] %y.6557)
}

%AddComputation.6565 (x.6566: bf16[], y.6567: bf16[]) -> bf16[] {
  %x.6566 = bf16[] parameter(0)
  %y.6567 = bf16[] parameter(1)
  ROOT %add.6568 = bf16[] add(bf16[] %x.6566, bf16[] %y.6567)
}

%AddComputation.6580 (x.6581: bf16[], y.6582: bf16[]) -> bf16[] {
  %x.6581 = bf16[] parameter(0)
  %y.6582 = bf16[] parameter(1)
  ROOT %add.6583 = bf16[] add(bf16[] %x.6581, bf16[] %y.6582)
}

%AddComputation.6590 (x.6591: bf16[], y.6592: bf16[]) -> bf16[] {
  %x.6591 = bf16[] parameter(0)
  %y.6592 = bf16[] parameter(1)
  ROOT %add.6593 = bf16[] add(bf16[] %x.6591, bf16[] %y.6592)
}

%AddComputation.6625 (x.6626: bf16[], y.6627: bf16[]) -> bf16[] {
  %x.6626 = bf16[] parameter(0)
  %y.6627 = bf16[] parameter(1)
  ROOT %add.6628 = bf16[] add(bf16[] %x.6626, bf16[] %y.6627)
}

%AddComputation.6635 (x.6636: bf16[], y.6637: bf16[]) -> bf16[] {
  %x.6636 = bf16[] parameter(0)
  %y.6637 = bf16[] parameter(1)
  ROOT %add.6638 = bf16[] add(bf16[] %x.6636, bf16[] %y.6637)
}

%AddComputation.6649 (x.6650: bf16[], y.6651: bf16[]) -> bf16[] {
  %x.6650 = bf16[] parameter(0)
  %y.6651 = bf16[] parameter(1)
  ROOT %add.6652 = bf16[] add(bf16[] %x.6650, bf16[] %y.6651)
}

%AddComputation.6671 (x.6672: bf16[], y.6673: bf16[]) -> bf16[] {
  %x.6672 = bf16[] parameter(0)
  %y.6673 = bf16[] parameter(1)
  ROOT %add.6674 = bf16[] add(bf16[] %x.6672, bf16[] %y.6673)
}

%AddComputation.6681 (x.6682: bf16[], y.6683: bf16[]) -> bf16[] {
  %x.6682 = bf16[] parameter(0)
  %y.6683 = bf16[] parameter(1)
  ROOT %add.6684 = bf16[] add(bf16[] %x.6682, bf16[] %y.6683)
}

%AddComputation.6695 (x.6696: bf16[], y.6697: bf16[]) -> bf16[] {
  %x.6696 = bf16[] parameter(0)
  %y.6697 = bf16[] parameter(1)
  ROOT %add.6698 = bf16[] add(bf16[] %x.6696, bf16[] %y.6697)
}

%AddComputation.6714 (x.6715: bf16[], y.6716: bf16[]) -> bf16[] {
  %x.6715 = bf16[] parameter(0)
  %y.6716 = bf16[] parameter(1)
  ROOT %add.6717 = bf16[] add(bf16[] %x.6715, bf16[] %y.6716)
}

%AddComputation.6724 (x.6725: bf16[], y.6726: bf16[]) -> bf16[] {
  %x.6725 = bf16[] parameter(0)
  %y.6726 = bf16[] parameter(1)
  ROOT %add.6727 = bf16[] add(bf16[] %x.6725, bf16[] %y.6726)
}

%AddComputation.6739 (x.6740: bf16[], y.6741: bf16[]) -> bf16[] {
  %x.6740 = bf16[] parameter(0)
  %y.6741 = bf16[] parameter(1)
  ROOT %add.6742 = bf16[] add(bf16[] %x.6740, bf16[] %y.6741)
}

%AddComputation.6749 (x.6750: bf16[], y.6751: bf16[]) -> bf16[] {
  %x.6750 = bf16[] parameter(0)
  %y.6751 = bf16[] parameter(1)
  ROOT %add.6752 = bf16[] add(bf16[] %x.6750, bf16[] %y.6751)
}

%AddComputation.6784 (x.6785: bf16[], y.6786: bf16[]) -> bf16[] {
  %x.6785 = bf16[] parameter(0)
  %y.6786 = bf16[] parameter(1)
  ROOT %add.6787 = bf16[] add(bf16[] %x.6785, bf16[] %y.6786)
}

%AddComputation.6794 (x.6795: bf16[], y.6796: bf16[]) -> bf16[] {
  %x.6795 = bf16[] parameter(0)
  %y.6796 = bf16[] parameter(1)
  ROOT %add.6797 = bf16[] add(bf16[] %x.6795, bf16[] %y.6796)
}

%AddComputation.6811 (x.6812: bf16[], y.6813: bf16[]) -> bf16[] {
  %x.6812 = bf16[] parameter(0)
  %y.6813 = bf16[] parameter(1)
  ROOT %add.6814 = bf16[] add(bf16[] %x.6812, bf16[] %y.6813)
}

%AddComputation.6859 (x.6860: bf16[], y.6861: bf16[]) -> bf16[] {
  %x.6860 = bf16[] parameter(0)
  %y.6861 = bf16[] parameter(1)
  ROOT %add.6862 = bf16[] add(bf16[] %x.6860, bf16[] %y.6861)
}

%AddComputation.6872 (x.6873: bf16[], y.6874: bf16[]) -> bf16[] {
  %x.6873 = bf16[] parameter(0)
  %y.6874 = bf16[] parameter(1)
  ROOT %add.6875 = bf16[] add(bf16[] %x.6873, bf16[] %y.6874)
}

%AddComputation.6920 (x.6921: bf16[], y.6922: bf16[]) -> bf16[] {
  %x.6921 = bf16[] parameter(0)
  %y.6922 = bf16[] parameter(1)
  ROOT %add.6923 = bf16[] add(bf16[] %x.6921, bf16[] %y.6922)
}

%AddComputation.6961 (x.6962: bf16[], y.6963: bf16[]) -> bf16[] {
  %x.6962 = bf16[] parameter(0)
  %y.6963 = bf16[] parameter(1)
  ROOT %add.6964 = bf16[] add(bf16[] %x.6962, bf16[] %y.6963)
}

%AddComputation.7002 (x.7003: bf16[], y.7004: bf16[]) -> bf16[] {
  %x.7003 = bf16[] parameter(0)
  %y.7004 = bf16[] parameter(1)
  ROOT %add.7005 = bf16[] add(bf16[] %x.7003, bf16[] %y.7004)
}

%AddComputation.7012 (x.7013: bf16[], y.7014: bf16[]) -> bf16[] {
  %x.7013 = bf16[] parameter(0)
  %y.7014 = bf16[] parameter(1)
  ROOT %add.7015 = bf16[] add(bf16[] %x.7013, bf16[] %y.7014)
}

%AddComputation.7027 (x.7028: bf16[], y.7029: bf16[]) -> bf16[] {
  %x.7028 = bf16[] parameter(0)
  %y.7029 = bf16[] parameter(1)
  ROOT %add.7030 = bf16[] add(bf16[] %x.7028, bf16[] %y.7029)
}

%AddComputation.7037 (x.7038: bf16[], y.7039: bf16[]) -> bf16[] {
  %x.7038 = bf16[] parameter(0)
  %y.7039 = bf16[] parameter(1)
  ROOT %add.7040 = bf16[] add(bf16[] %x.7038, bf16[] %y.7039)
}

%AddComputation.7072 (x.7073: bf16[], y.7074: bf16[]) -> bf16[] {
  %x.7073 = bf16[] parameter(0)
  %y.7074 = bf16[] parameter(1)
  ROOT %add.7075 = bf16[] add(bf16[] %x.7073, bf16[] %y.7074)
}

%AddComputation.7082 (x.7083: bf16[], y.7084: bf16[]) -> bf16[] {
  %x.7083 = bf16[] parameter(0)
  %y.7084 = bf16[] parameter(1)
  ROOT %add.7085 = bf16[] add(bf16[] %x.7083, bf16[] %y.7084)
}

%AddComputation.7096 (x.7097: bf16[], y.7098: bf16[]) -> bf16[] {
  %x.7097 = bf16[] parameter(0)
  %y.7098 = bf16[] parameter(1)
  ROOT %add.7099 = bf16[] add(bf16[] %x.7097, bf16[] %y.7098)
}

%AddComputation.7118 (x.7119: bf16[], y.7120: bf16[]) -> bf16[] {
  %x.7119 = bf16[] parameter(0)
  %y.7120 = bf16[] parameter(1)
  ROOT %add.7121 = bf16[] add(bf16[] %x.7119, bf16[] %y.7120)
}

%AddComputation.7128 (x.7129: bf16[], y.7130: bf16[]) -> bf16[] {
  %x.7129 = bf16[] parameter(0)
  %y.7130 = bf16[] parameter(1)
  ROOT %add.7131 = bf16[] add(bf16[] %x.7129, bf16[] %y.7130)
}

%AddComputation.7142 (x.7143: bf16[], y.7144: bf16[]) -> bf16[] {
  %x.7143 = bf16[] parameter(0)
  %y.7144 = bf16[] parameter(1)
  ROOT %add.7145 = bf16[] add(bf16[] %x.7143, bf16[] %y.7144)
}

%AddComputation.7161 (x.7162: bf16[], y.7163: bf16[]) -> bf16[] {
  %x.7162 = bf16[] parameter(0)
  %y.7163 = bf16[] parameter(1)
  ROOT %add.7164 = bf16[] add(bf16[] %x.7162, bf16[] %y.7163)
}

%AddComputation.7171 (x.7172: bf16[], y.7173: bf16[]) -> bf16[] {
  %x.7172 = bf16[] parameter(0)
  %y.7173 = bf16[] parameter(1)
  ROOT %add.7174 = bf16[] add(bf16[] %x.7172, bf16[] %y.7173)
}

%AddComputation.7186 (x.7187: bf16[], y.7188: bf16[]) -> bf16[] {
  %x.7187 = bf16[] parameter(0)
  %y.7188 = bf16[] parameter(1)
  ROOT %add.7189 = bf16[] add(bf16[] %x.7187, bf16[] %y.7188)
}

%AddComputation.7196 (x.7197: bf16[], y.7198: bf16[]) -> bf16[] {
  %x.7197 = bf16[] parameter(0)
  %y.7198 = bf16[] parameter(1)
  ROOT %add.7199 = bf16[] add(bf16[] %x.7197, bf16[] %y.7198)
}

%AddComputation.7231 (x.7232: bf16[], y.7233: bf16[]) -> bf16[] {
  %x.7232 = bf16[] parameter(0)
  %y.7233 = bf16[] parameter(1)
  ROOT %add.7234 = bf16[] add(bf16[] %x.7232, bf16[] %y.7233)
}

%AddComputation.7241 (x.7242: bf16[], y.7243: bf16[]) -> bf16[] {
  %x.7242 = bf16[] parameter(0)
  %y.7243 = bf16[] parameter(1)
  ROOT %add.7244 = bf16[] add(bf16[] %x.7242, bf16[] %y.7243)
}

%AddComputation.7258 (x.7259: bf16[], y.7260: bf16[]) -> bf16[] {
  %x.7259 = bf16[] parameter(0)
  %y.7260 = bf16[] parameter(1)
  ROOT %add.7261 = bf16[] add(bf16[] %x.7259, bf16[] %y.7260)
}

%AddComputation.7306 (x.7307: bf16[], y.7308: bf16[]) -> bf16[] {
  %x.7307 = bf16[] parameter(0)
  %y.7308 = bf16[] parameter(1)
  ROOT %add.7309 = bf16[] add(bf16[] %x.7307, bf16[] %y.7308)
}

%AddComputation.7319 (x.7320: bf16[], y.7321: bf16[]) -> bf16[] {
  %x.7320 = bf16[] parameter(0)
  %y.7321 = bf16[] parameter(1)
  ROOT %add.7322 = bf16[] add(bf16[] %x.7320, bf16[] %y.7321)
}

%AddComputation.7367 (x.7368: bf16[], y.7369: bf16[]) -> bf16[] {
  %x.7368 = bf16[] parameter(0)
  %y.7369 = bf16[] parameter(1)
  ROOT %add.7370 = bf16[] add(bf16[] %x.7368, bf16[] %y.7369)
}

%AddComputation.7408 (x.7409: bf16[], y.7410: bf16[]) -> bf16[] {
  %x.7409 = bf16[] parameter(0)
  %y.7410 = bf16[] parameter(1)
  ROOT %add.7411 = bf16[] add(bf16[] %x.7409, bf16[] %y.7410)
}

%AddComputation.7449 (x.7450: bf16[], y.7451: bf16[]) -> bf16[] {
  %x.7450 = bf16[] parameter(0)
  %y.7451 = bf16[] parameter(1)
  ROOT %add.7452 = bf16[] add(bf16[] %x.7450, bf16[] %y.7451)
}

%AddComputation.7459 (x.7460: bf16[], y.7461: bf16[]) -> bf16[] {
  %x.7460 = bf16[] parameter(0)
  %y.7461 = bf16[] parameter(1)
  ROOT %add.7462 = bf16[] add(bf16[] %x.7460, bf16[] %y.7461)
}

%AddComputation.7474 (x.7475: bf16[], y.7476: bf16[]) -> bf16[] {
  %x.7475 = bf16[] parameter(0)
  %y.7476 = bf16[] parameter(1)
  ROOT %add.7477 = bf16[] add(bf16[] %x.7475, bf16[] %y.7476)
}

%AddComputation.7484 (x.7485: bf16[], y.7486: bf16[]) -> bf16[] {
  %x.7485 = bf16[] parameter(0)
  %y.7486 = bf16[] parameter(1)
  ROOT %add.7487 = bf16[] add(bf16[] %x.7485, bf16[] %y.7486)
}

%AddComputation.7519 (x.7520: bf16[], y.7521: bf16[]) -> bf16[] {
  %x.7520 = bf16[] parameter(0)
  %y.7521 = bf16[] parameter(1)
  ROOT %add.7522 = bf16[] add(bf16[] %x.7520, bf16[] %y.7521)
}

%AddComputation.7529 (x.7530: bf16[], y.7531: bf16[]) -> bf16[] {
  %x.7530 = bf16[] parameter(0)
  %y.7531 = bf16[] parameter(1)
  ROOT %add.7532 = bf16[] add(bf16[] %x.7530, bf16[] %y.7531)
}

%AddComputation.7543 (x.7544: bf16[], y.7545: bf16[]) -> bf16[] {
  %x.7544 = bf16[] parameter(0)
  %y.7545 = bf16[] parameter(1)
  ROOT %add.7546 = bf16[] add(bf16[] %x.7544, bf16[] %y.7545)
}

%AddComputation.7565 (x.7566: bf16[], y.7567: bf16[]) -> bf16[] {
  %x.7566 = bf16[] parameter(0)
  %y.7567 = bf16[] parameter(1)
  ROOT %add.7568 = bf16[] add(bf16[] %x.7566, bf16[] %y.7567)
}

%AddComputation.7575 (x.7576: bf16[], y.7577: bf16[]) -> bf16[] {
  %x.7576 = bf16[] parameter(0)
  %y.7577 = bf16[] parameter(1)
  ROOT %add.7578 = bf16[] add(bf16[] %x.7576, bf16[] %y.7577)
}

%AddComputation.7589 (x.7590: bf16[], y.7591: bf16[]) -> bf16[] {
  %x.7590 = bf16[] parameter(0)
  %y.7591 = bf16[] parameter(1)
  ROOT %add.7592 = bf16[] add(bf16[] %x.7590, bf16[] %y.7591)
}

%AddComputation.7608 (x.7609: bf16[], y.7610: bf16[]) -> bf16[] {
  %x.7609 = bf16[] parameter(0)
  %y.7610 = bf16[] parameter(1)
  ROOT %add.7611 = bf16[] add(bf16[] %x.7609, bf16[] %y.7610)
}

%AddComputation.7618 (x.7619: bf16[], y.7620: bf16[]) -> bf16[] {
  %x.7619 = bf16[] parameter(0)
  %y.7620 = bf16[] parameter(1)
  ROOT %add.7621 = bf16[] add(bf16[] %x.7619, bf16[] %y.7620)
}

%AddComputation.7633 (x.7634: bf16[], y.7635: bf16[]) -> bf16[] {
  %x.7634 = bf16[] parameter(0)
  %y.7635 = bf16[] parameter(1)
  ROOT %add.7636 = bf16[] add(bf16[] %x.7634, bf16[] %y.7635)
}

%AddComputation.7643 (x.7644: bf16[], y.7645: bf16[]) -> bf16[] {
  %x.7644 = bf16[] parameter(0)
  %y.7645 = bf16[] parameter(1)
  ROOT %add.7646 = bf16[] add(bf16[] %x.7644, bf16[] %y.7645)
}

%AddComputation.7678 (x.7679: bf16[], y.7680: bf16[]) -> bf16[] {
  %x.7679 = bf16[] parameter(0)
  %y.7680 = bf16[] parameter(1)
  ROOT %add.7681 = bf16[] add(bf16[] %x.7679, bf16[] %y.7680)
}

%AddComputation.7688 (x.7689: bf16[], y.7690: bf16[]) -> bf16[] {
  %x.7689 = bf16[] parameter(0)
  %y.7690 = bf16[] parameter(1)
  ROOT %add.7691 = bf16[] add(bf16[] %x.7689, bf16[] %y.7690)
}

%AddComputation.7705 (x.7706: bf16[], y.7707: bf16[]) -> bf16[] {
  %x.7706 = bf16[] parameter(0)
  %y.7707 = bf16[] parameter(1)
  ROOT %add.7708 = bf16[] add(bf16[] %x.7706, bf16[] %y.7707)
}

%AddComputation.7753 (x.7754: bf16[], y.7755: bf16[]) -> bf16[] {
  %x.7754 = bf16[] parameter(0)
  %y.7755 = bf16[] parameter(1)
  ROOT %add.7756 = bf16[] add(bf16[] %x.7754, bf16[] %y.7755)
}

%AddComputation.7766 (x.7767: bf16[], y.7768: bf16[]) -> bf16[] {
  %x.7767 = bf16[] parameter(0)
  %y.7768 = bf16[] parameter(1)
  ROOT %add.7769 = bf16[] add(bf16[] %x.7767, bf16[] %y.7768)
}

%AddComputation.7814 (x.7815: bf16[], y.7816: bf16[]) -> bf16[] {
  %x.7815 = bf16[] parameter(0)
  %y.7816 = bf16[] parameter(1)
  ROOT %add.7817 = bf16[] add(bf16[] %x.7815, bf16[] %y.7816)
}

%AddComputation.7855 (x.7856: bf16[], y.7857: bf16[]) -> bf16[] {
  %x.7856 = bf16[] parameter(0)
  %y.7857 = bf16[] parameter(1)
  ROOT %add.7858 = bf16[] add(bf16[] %x.7856, bf16[] %y.7857)
}

%AddComputation.7896 (x.7897: bf16[], y.7898: bf16[]) -> bf16[] {
  %x.7897 = bf16[] parameter(0)
  %y.7898 = bf16[] parameter(1)
  ROOT %add.7899 = bf16[] add(bf16[] %x.7897, bf16[] %y.7898)
}

%AddComputation.7906 (x.7907: bf16[], y.7908: bf16[]) -> bf16[] {
  %x.7907 = bf16[] parameter(0)
  %y.7908 = bf16[] parameter(1)
  ROOT %add.7909 = bf16[] add(bf16[] %x.7907, bf16[] %y.7908)
}

%AddComputation.7921 (x.7922: bf16[], y.7923: bf16[]) -> bf16[] {
  %x.7922 = bf16[] parameter(0)
  %y.7923 = bf16[] parameter(1)
  ROOT %add.7924 = bf16[] add(bf16[] %x.7922, bf16[] %y.7923)
}

%AddComputation.7931 (x.7932: bf16[], y.7933: bf16[]) -> bf16[] {
  %x.7932 = bf16[] parameter(0)
  %y.7933 = bf16[] parameter(1)
  ROOT %add.7934 = bf16[] add(bf16[] %x.7932, bf16[] %y.7933)
}

%AddComputation.7966 (x.7967: bf16[], y.7968: bf16[]) -> bf16[] {
  %x.7967 = bf16[] parameter(0)
  %y.7968 = bf16[] parameter(1)
  ROOT %add.7969 = bf16[] add(bf16[] %x.7967, bf16[] %y.7968)
}

%AddComputation.7976 (x.7977: bf16[], y.7978: bf16[]) -> bf16[] {
  %x.7977 = bf16[] parameter(0)
  %y.7978 = bf16[] parameter(1)
  ROOT %add.7979 = bf16[] add(bf16[] %x.7977, bf16[] %y.7978)
}

%AddComputation.7990 (x.7991: bf16[], y.7992: bf16[]) -> bf16[] {
  %x.7991 = bf16[] parameter(0)
  %y.7992 = bf16[] parameter(1)
  ROOT %add.7993 = bf16[] add(bf16[] %x.7991, bf16[] %y.7992)
}

%AddComputation.8012 (x.8013: bf16[], y.8014: bf16[]) -> bf16[] {
  %x.8013 = bf16[] parameter(0)
  %y.8014 = bf16[] parameter(1)
  ROOT %add.8015 = bf16[] add(bf16[] %x.8013, bf16[] %y.8014)
}

%AddComputation.8022 (x.8023: bf16[], y.8024: bf16[]) -> bf16[] {
  %x.8023 = bf16[] parameter(0)
  %y.8024 = bf16[] parameter(1)
  ROOT %add.8025 = bf16[] add(bf16[] %x.8023, bf16[] %y.8024)
}

%AddComputation.8036 (x.8037: bf16[], y.8038: bf16[]) -> bf16[] {
  %x.8037 = bf16[] parameter(0)
  %y.8038 = bf16[] parameter(1)
  ROOT %add.8039 = bf16[] add(bf16[] %x.8037, bf16[] %y.8038)
}

%AddComputation.8055 (x.8056: bf16[], y.8057: bf16[]) -> bf16[] {
  %x.8056 = bf16[] parameter(0)
  %y.8057 = bf16[] parameter(1)
  ROOT %add.8058 = bf16[] add(bf16[] %x.8056, bf16[] %y.8057)
}

%AddComputation.8065 (x.8066: bf16[], y.8067: bf16[]) -> bf16[] {
  %x.8066 = bf16[] parameter(0)
  %y.8067 = bf16[] parameter(1)
  ROOT %add.8068 = bf16[] add(bf16[] %x.8066, bf16[] %y.8067)
}

%AddComputation.8080 (x.8081: bf16[], y.8082: bf16[]) -> bf16[] {
  %x.8081 = bf16[] parameter(0)
  %y.8082 = bf16[] parameter(1)
  ROOT %add.8083 = bf16[] add(bf16[] %x.8081, bf16[] %y.8082)
}

%AddComputation.8090 (x.8091: bf16[], y.8092: bf16[]) -> bf16[] {
  %x.8091 = bf16[] parameter(0)
  %y.8092 = bf16[] parameter(1)
  ROOT %add.8093 = bf16[] add(bf16[] %x.8091, bf16[] %y.8092)
}

%AddComputation.8125 (x.8126: bf16[], y.8127: bf16[]) -> bf16[] {
  %x.8126 = bf16[] parameter(0)
  %y.8127 = bf16[] parameter(1)
  ROOT %add.8128 = bf16[] add(bf16[] %x.8126, bf16[] %y.8127)
}

%AddComputation.8135 (x.8136: bf16[], y.8137: bf16[]) -> bf16[] {
  %x.8136 = bf16[] parameter(0)
  %y.8137 = bf16[] parameter(1)
  ROOT %add.8138 = bf16[] add(bf16[] %x.8136, bf16[] %y.8137)
}

%AddComputation.8152 (x.8153: bf16[], y.8154: bf16[]) -> bf16[] {
  %x.8153 = bf16[] parameter(0)
  %y.8154 = bf16[] parameter(1)
  ROOT %add.8155 = bf16[] add(bf16[] %x.8153, bf16[] %y.8154)
}

%AddComputation.8200 (x.8201: bf16[], y.8202: bf16[]) -> bf16[] {
  %x.8201 = bf16[] parameter(0)
  %y.8202 = bf16[] parameter(1)
  ROOT %add.8203 = bf16[] add(bf16[] %x.8201, bf16[] %y.8202)
}

%AddComputation.8213 (x.8214: bf16[], y.8215: bf16[]) -> bf16[] {
  %x.8214 = bf16[] parameter(0)
  %y.8215 = bf16[] parameter(1)
  ROOT %add.8216 = bf16[] add(bf16[] %x.8214, bf16[] %y.8215)
}

%AddComputation.8261 (x.8262: bf16[], y.8263: bf16[]) -> bf16[] {
  %x.8262 = bf16[] parameter(0)
  %y.8263 = bf16[] parameter(1)
  ROOT %add.8264 = bf16[] add(bf16[] %x.8262, bf16[] %y.8263)
}

%AddComputation.8302 (x.8303: bf16[], y.8304: bf16[]) -> bf16[] {
  %x.8303 = bf16[] parameter(0)
  %y.8304 = bf16[] parameter(1)
  ROOT %add.8305 = bf16[] add(bf16[] %x.8303, bf16[] %y.8304)
}

%AddComputation.8343 (x.8344: bf16[], y.8345: bf16[]) -> bf16[] {
  %x.8344 = bf16[] parameter(0)
  %y.8345 = bf16[] parameter(1)
  ROOT %add.8346 = bf16[] add(bf16[] %x.8344, bf16[] %y.8345)
}

%AddComputation.8353 (x.8354: bf16[], y.8355: bf16[]) -> bf16[] {
  %x.8354 = bf16[] parameter(0)
  %y.8355 = bf16[] parameter(1)
  ROOT %add.8356 = bf16[] add(bf16[] %x.8354, bf16[] %y.8355)
}

%AddComputation.8368 (x.8369: bf16[], y.8370: bf16[]) -> bf16[] {
  %x.8369 = bf16[] parameter(0)
  %y.8370 = bf16[] parameter(1)
  ROOT %add.8371 = bf16[] add(bf16[] %x.8369, bf16[] %y.8370)
}

%AddComputation.8378 (x.8379: bf16[], y.8380: bf16[]) -> bf16[] {
  %x.8379 = bf16[] parameter(0)
  %y.8380 = bf16[] parameter(1)
  ROOT %add.8381 = bf16[] add(bf16[] %x.8379, bf16[] %y.8380)
}

%AddComputation.8413 (x.8414: bf16[], y.8415: bf16[]) -> bf16[] {
  %x.8414 = bf16[] parameter(0)
  %y.8415 = bf16[] parameter(1)
  ROOT %add.8416 = bf16[] add(bf16[] %x.8414, bf16[] %y.8415)
}

%AddComputation.8423 (x.8424: bf16[], y.8425: bf16[]) -> bf16[] {
  %x.8424 = bf16[] parameter(0)
  %y.8425 = bf16[] parameter(1)
  ROOT %add.8426 = bf16[] add(bf16[] %x.8424, bf16[] %y.8425)
}

%AddComputation.8437 (x.8438: bf16[], y.8439: bf16[]) -> bf16[] {
  %x.8438 = bf16[] parameter(0)
  %y.8439 = bf16[] parameter(1)
  ROOT %add.8440 = bf16[] add(bf16[] %x.8438, bf16[] %y.8439)
}

%AddComputation.8459 (x.8460: bf16[], y.8461: bf16[]) -> bf16[] {
  %x.8460 = bf16[] parameter(0)
  %y.8461 = bf16[] parameter(1)
  ROOT %add.8462 = bf16[] add(bf16[] %x.8460, bf16[] %y.8461)
}

%AddComputation.8469 (x.8470: bf16[], y.8471: bf16[]) -> bf16[] {
  %x.8470 = bf16[] parameter(0)
  %y.8471 = bf16[] parameter(1)
  ROOT %add.8472 = bf16[] add(bf16[] %x.8470, bf16[] %y.8471)
}

%AddComputation.8483 (x.8484: bf16[], y.8485: bf16[]) -> bf16[] {
  %x.8484 = bf16[] parameter(0)
  %y.8485 = bf16[] parameter(1)
  ROOT %add.8486 = bf16[] add(bf16[] %x.8484, bf16[] %y.8485)
}

%AddComputation.8502 (x.8503: bf16[], y.8504: bf16[]) -> bf16[] {
  %x.8503 = bf16[] parameter(0)
  %y.8504 = bf16[] parameter(1)
  ROOT %add.8505 = bf16[] add(bf16[] %x.8503, bf16[] %y.8504)
}

%AddComputation.8512 (x.8513: bf16[], y.8514: bf16[]) -> bf16[] {
  %x.8513 = bf16[] parameter(0)
  %y.8514 = bf16[] parameter(1)
  ROOT %add.8515 = bf16[] add(bf16[] %x.8513, bf16[] %y.8514)
}

%AddComputation.8527 (x.8528: bf16[], y.8529: bf16[]) -> bf16[] {
  %x.8528 = bf16[] parameter(0)
  %y.8529 = bf16[] parameter(1)
  ROOT %add.8530 = bf16[] add(bf16[] %x.8528, bf16[] %y.8529)
}

%AddComputation.8537 (x.8538: bf16[], y.8539: bf16[]) -> bf16[] {
  %x.8538 = bf16[] parameter(0)
  %y.8539 = bf16[] parameter(1)
  ROOT %add.8540 = bf16[] add(bf16[] %x.8538, bf16[] %y.8539)
}

%AddComputation.8572 (x.8573: bf16[], y.8574: bf16[]) -> bf16[] {
  %x.8573 = bf16[] parameter(0)
  %y.8574 = bf16[] parameter(1)
  ROOT %add.8575 = bf16[] add(bf16[] %x.8573, bf16[] %y.8574)
}

%AddComputation.8582 (x.8583: bf16[], y.8584: bf16[]) -> bf16[] {
  %x.8583 = bf16[] parameter(0)
  %y.8584 = bf16[] parameter(1)
  ROOT %add.8585 = bf16[] add(bf16[] %x.8583, bf16[] %y.8584)
}

%AddComputation.8599 (x.8600: bf16[], y.8601: bf16[]) -> bf16[] {
  %x.8600 = bf16[] parameter(0)
  %y.8601 = bf16[] parameter(1)
  ROOT %add.8602 = bf16[] add(bf16[] %x.8600, bf16[] %y.8601)
}

%AddComputation.8647 (x.8648: bf16[], y.8649: bf16[]) -> bf16[] {
  %x.8648 = bf16[] parameter(0)
  %y.8649 = bf16[] parameter(1)
  ROOT %add.8650 = bf16[] add(bf16[] %x.8648, bf16[] %y.8649)
}

%AddComputation.8660 (x.8661: bf16[], y.8662: bf16[]) -> bf16[] {
  %x.8661 = bf16[] parameter(0)
  %y.8662 = bf16[] parameter(1)
  ROOT %add.8663 = bf16[] add(bf16[] %x.8661, bf16[] %y.8662)
}

%AddComputation.8708 (x.8709: bf16[], y.8710: bf16[]) -> bf16[] {
  %x.8709 = bf16[] parameter(0)
  %y.8710 = bf16[] parameter(1)
  ROOT %add.8711 = bf16[] add(bf16[] %x.8709, bf16[] %y.8710)
}

%AddComputation.8749 (x.8750: bf16[], y.8751: bf16[]) -> bf16[] {
  %x.8750 = bf16[] parameter(0)
  %y.8751 = bf16[] parameter(1)
  ROOT %add.8752 = bf16[] add(bf16[] %x.8750, bf16[] %y.8751)
}

%AddComputation.8790 (x.8791: bf16[], y.8792: bf16[]) -> bf16[] {
  %x.8791 = bf16[] parameter(0)
  %y.8792 = bf16[] parameter(1)
  ROOT %add.8793 = bf16[] add(bf16[] %x.8791, bf16[] %y.8792)
}

%AddComputation.8800 (x.8801: bf16[], y.8802: bf16[]) -> bf16[] {
  %x.8801 = bf16[] parameter(0)
  %y.8802 = bf16[] parameter(1)
  ROOT %add.8803 = bf16[] add(bf16[] %x.8801, bf16[] %y.8802)
}

%AddComputation.8815 (x.8816: bf16[], y.8817: bf16[]) -> bf16[] {
  %x.8816 = bf16[] parameter(0)
  %y.8817 = bf16[] parameter(1)
  ROOT %add.8818 = bf16[] add(bf16[] %x.8816, bf16[] %y.8817)
}

%AddComputation.8825 (x.8826: bf16[], y.8827: bf16[]) -> bf16[] {
  %x.8826 = bf16[] parameter(0)
  %y.8827 = bf16[] parameter(1)
  ROOT %add.8828 = bf16[] add(bf16[] %x.8826, bf16[] %y.8827)
}

%AddComputation.8860 (x.8861: bf16[], y.8862: bf16[]) -> bf16[] {
  %x.8861 = bf16[] parameter(0)
  %y.8862 = bf16[] parameter(1)
  ROOT %add.8863 = bf16[] add(bf16[] %x.8861, bf16[] %y.8862)
}

%AddComputation.8870 (x.8871: bf16[], y.8872: bf16[]) -> bf16[] {
  %x.8871 = bf16[] parameter(0)
  %y.8872 = bf16[] parameter(1)
  ROOT %add.8873 = bf16[] add(bf16[] %x.8871, bf16[] %y.8872)
}

%AddComputation.8884 (x.8885: bf16[], y.8886: bf16[]) -> bf16[] {
  %x.8885 = bf16[] parameter(0)
  %y.8886 = bf16[] parameter(1)
  ROOT %add.8887 = bf16[] add(bf16[] %x.8885, bf16[] %y.8886)
}

%AddComputation.8906 (x.8907: bf16[], y.8908: bf16[]) -> bf16[] {
  %x.8907 = bf16[] parameter(0)
  %y.8908 = bf16[] parameter(1)
  ROOT %add.8909 = bf16[] add(bf16[] %x.8907, bf16[] %y.8908)
}

%AddComputation.8916 (x.8917: bf16[], y.8918: bf16[]) -> bf16[] {
  %x.8917 = bf16[] parameter(0)
  %y.8918 = bf16[] parameter(1)
  ROOT %add.8919 = bf16[] add(bf16[] %x.8917, bf16[] %y.8918)
}

%AddComputation.8930 (x.8931: bf16[], y.8932: bf16[]) -> bf16[] {
  %x.8931 = bf16[] parameter(0)
  %y.8932 = bf16[] parameter(1)
  ROOT %add.8933 = bf16[] add(bf16[] %x.8931, bf16[] %y.8932)
}

%AddComputation.8949 (x.8950: bf16[], y.8951: bf16[]) -> bf16[] {
  %x.8950 = bf16[] parameter(0)
  %y.8951 = bf16[] parameter(1)
  ROOT %add.8952 = bf16[] add(bf16[] %x.8950, bf16[] %y.8951)
}

%AddComputation.8959 (x.8960: bf16[], y.8961: bf16[]) -> bf16[] {
  %x.8960 = bf16[] parameter(0)
  %y.8961 = bf16[] parameter(1)
  ROOT %add.8962 = bf16[] add(bf16[] %x.8960, bf16[] %y.8961)
}

%AddComputation.8974 (x.8975: bf16[], y.8976: bf16[]) -> bf16[] {
  %x.8975 = bf16[] parameter(0)
  %y.8976 = bf16[] parameter(1)
  ROOT %add.8977 = bf16[] add(bf16[] %x.8975, bf16[] %y.8976)
}

%AddComputation.8984 (x.8985: bf16[], y.8986: bf16[]) -> bf16[] {
  %x.8985 = bf16[] parameter(0)
  %y.8986 = bf16[] parameter(1)
  ROOT %add.8987 = bf16[] add(bf16[] %x.8985, bf16[] %y.8986)
}

%AddComputation.9019 (x.9020: bf16[], y.9021: bf16[]) -> bf16[] {
  %x.9020 = bf16[] parameter(0)
  %y.9021 = bf16[] parameter(1)
  ROOT %add.9022 = bf16[] add(bf16[] %x.9020, bf16[] %y.9021)
}

%AddComputation.9029 (x.9030: bf16[], y.9031: bf16[]) -> bf16[] {
  %x.9030 = bf16[] parameter(0)
  %y.9031 = bf16[] parameter(1)
  ROOT %add.9032 = bf16[] add(bf16[] %x.9030, bf16[] %y.9031)
}

%AddComputation.9046 (x.9047: bf16[], y.9048: bf16[]) -> bf16[] {
  %x.9047 = bf16[] parameter(0)
  %y.9048 = bf16[] parameter(1)
  ROOT %add.9049 = bf16[] add(bf16[] %x.9047, bf16[] %y.9048)
}

%AddComputation.9094 (x.9095: bf16[], y.9096: bf16[]) -> bf16[] {
  %x.9095 = bf16[] parameter(0)
  %y.9096 = bf16[] parameter(1)
  ROOT %add.9097 = bf16[] add(bf16[] %x.9095, bf16[] %y.9096)
}

%AddComputation.9107 (x.9108: bf16[], y.9109: bf16[]) -> bf16[] {
  %x.9108 = bf16[] parameter(0)
  %y.9109 = bf16[] parameter(1)
  ROOT %add.9110 = bf16[] add(bf16[] %x.9108, bf16[] %y.9109)
}

%AddComputation.9155 (x.9156: bf16[], y.9157: bf16[]) -> bf16[] {
  %x.9156 = bf16[] parameter(0)
  %y.9157 = bf16[] parameter(1)
  ROOT %add.9158 = bf16[] add(bf16[] %x.9156, bf16[] %y.9157)
}

%AddComputation.9196 (x.9197: bf16[], y.9198: bf16[]) -> bf16[] {
  %x.9197 = bf16[] parameter(0)
  %y.9198 = bf16[] parameter(1)
  ROOT %add.9199 = bf16[] add(bf16[] %x.9197, bf16[] %y.9198)
}

%AddComputation.9237 (x.9238: bf16[], y.9239: bf16[]) -> bf16[] {
  %x.9238 = bf16[] parameter(0)
  %y.9239 = bf16[] parameter(1)
  ROOT %add.9240 = bf16[] add(bf16[] %x.9238, bf16[] %y.9239)
}

%AddComputation.9247 (x.9248: bf16[], y.9249: bf16[]) -> bf16[] {
  %x.9248 = bf16[] parameter(0)
  %y.9249 = bf16[] parameter(1)
  ROOT %add.9250 = bf16[] add(bf16[] %x.9248, bf16[] %y.9249)
}

%AddComputation.9262 (x.9263: bf16[], y.9264: bf16[]) -> bf16[] {
  %x.9263 = bf16[] parameter(0)
  %y.9264 = bf16[] parameter(1)
  ROOT %add.9265 = bf16[] add(bf16[] %x.9263, bf16[] %y.9264)
}

%AddComputation.9272 (x.9273: bf16[], y.9274: bf16[]) -> bf16[] {
  %x.9273 = bf16[] parameter(0)
  %y.9274 = bf16[] parameter(1)
  ROOT %add.9275 = bf16[] add(bf16[] %x.9273, bf16[] %y.9274)
}

%AddComputation.9307 (x.9308: bf16[], y.9309: bf16[]) -> bf16[] {
  %x.9308 = bf16[] parameter(0)
  %y.9309 = bf16[] parameter(1)
  ROOT %add.9310 = bf16[] add(bf16[] %x.9308, bf16[] %y.9309)
}

%AddComputation.9317 (x.9318: bf16[], y.9319: bf16[]) -> bf16[] {
  %x.9318 = bf16[] parameter(0)
  %y.9319 = bf16[] parameter(1)
  ROOT %add.9320 = bf16[] add(bf16[] %x.9318, bf16[] %y.9319)
}

%AddComputation.9331 (x.9332: bf16[], y.9333: bf16[]) -> bf16[] {
  %x.9332 = bf16[] parameter(0)
  %y.9333 = bf16[] parameter(1)
  ROOT %add.9334 = bf16[] add(bf16[] %x.9332, bf16[] %y.9333)
}

%AddComputation.9353 (x.9354: bf16[], y.9355: bf16[]) -> bf16[] {
  %x.9354 = bf16[] parameter(0)
  %y.9355 = bf16[] parameter(1)
  ROOT %add.9356 = bf16[] add(bf16[] %x.9354, bf16[] %y.9355)
}

%AddComputation.9363 (x.9364: bf16[], y.9365: bf16[]) -> bf16[] {
  %x.9364 = bf16[] parameter(0)
  %y.9365 = bf16[] parameter(1)
  ROOT %add.9366 = bf16[] add(bf16[] %x.9364, bf16[] %y.9365)
}

%AddComputation.9377 (x.9378: bf16[], y.9379: bf16[]) -> bf16[] {
  %x.9378 = bf16[] parameter(0)
  %y.9379 = bf16[] parameter(1)
  ROOT %add.9380 = bf16[] add(bf16[] %x.9378, bf16[] %y.9379)
}

%AddComputation.9396 (x.9397: bf16[], y.9398: bf16[]) -> bf16[] {
  %x.9397 = bf16[] parameter(0)
  %y.9398 = bf16[] parameter(1)
  ROOT %add.9399 = bf16[] add(bf16[] %x.9397, bf16[] %y.9398)
}

%AddComputation.9406 (x.9407: bf16[], y.9408: bf16[]) -> bf16[] {
  %x.9407 = bf16[] parameter(0)
  %y.9408 = bf16[] parameter(1)
  ROOT %add.9409 = bf16[] add(bf16[] %x.9407, bf16[] %y.9408)
}

%AddComputation.9421 (x.9422: bf16[], y.9423: bf16[]) -> bf16[] {
  %x.9422 = bf16[] parameter(0)
  %y.9423 = bf16[] parameter(1)
  ROOT %add.9424 = bf16[] add(bf16[] %x.9422, bf16[] %y.9423)
}

%AddComputation.9431 (x.9432: bf16[], y.9433: bf16[]) -> bf16[] {
  %x.9432 = bf16[] parameter(0)
  %y.9433 = bf16[] parameter(1)
  ROOT %add.9434 = bf16[] add(bf16[] %x.9432, bf16[] %y.9433)
}

%AddComputation.9466 (x.9467: bf16[], y.9468: bf16[]) -> bf16[] {
  %x.9467 = bf16[] parameter(0)
  %y.9468 = bf16[] parameter(1)
  ROOT %add.9469 = bf16[] add(bf16[] %x.9467, bf16[] %y.9468)
}

%AddComputation.9476 (x.9477: bf16[], y.9478: bf16[]) -> bf16[] {
  %x.9477 = bf16[] parameter(0)
  %y.9478 = bf16[] parameter(1)
  ROOT %add.9479 = bf16[] add(bf16[] %x.9477, bf16[] %y.9478)
}

%AddComputation.9493 (x.9494: bf16[], y.9495: bf16[]) -> bf16[] {
  %x.9494 = bf16[] parameter(0)
  %y.9495 = bf16[] parameter(1)
  ROOT %add.9496 = bf16[] add(bf16[] %x.9494, bf16[] %y.9495)
}

%AddComputation.9541 (x.9542: bf16[], y.9543: bf16[]) -> bf16[] {
  %x.9542 = bf16[] parameter(0)
  %y.9543 = bf16[] parameter(1)
  ROOT %add.9544 = bf16[] add(bf16[] %x.9542, bf16[] %y.9543)
}

%AddComputation.9554 (x.9555: bf16[], y.9556: bf16[]) -> bf16[] {
  %x.9555 = bf16[] parameter(0)
  %y.9556 = bf16[] parameter(1)
  ROOT %add.9557 = bf16[] add(bf16[] %x.9555, bf16[] %y.9556)
}

%AddComputation.9602 (x.9603: bf16[], y.9604: bf16[]) -> bf16[] {
  %x.9603 = bf16[] parameter(0)
  %y.9604 = bf16[] parameter(1)
  ROOT %add.9605 = bf16[] add(bf16[] %x.9603, bf16[] %y.9604)
}

%AddComputation.9643 (x.9644: bf16[], y.9645: bf16[]) -> bf16[] {
  %x.9644 = bf16[] parameter(0)
  %y.9645 = bf16[] parameter(1)
  ROOT %add.9646 = bf16[] add(bf16[] %x.9644, bf16[] %y.9645)
}

%AddComputation.9684 (x.9685: bf16[], y.9686: bf16[]) -> bf16[] {
  %x.9685 = bf16[] parameter(0)
  %y.9686 = bf16[] parameter(1)
  ROOT %add.9687 = bf16[] add(bf16[] %x.9685, bf16[] %y.9686)
}

%AddComputation.9694 (x.9695: bf16[], y.9696: bf16[]) -> bf16[] {
  %x.9695 = bf16[] parameter(0)
  %y.9696 = bf16[] parameter(1)
  ROOT %add.9697 = bf16[] add(bf16[] %x.9695, bf16[] %y.9696)
}

%AddComputation.9709 (x.9710: bf16[], y.9711: bf16[]) -> bf16[] {
  %x.9710 = bf16[] parameter(0)
  %y.9711 = bf16[] parameter(1)
  ROOT %add.9712 = bf16[] add(bf16[] %x.9710, bf16[] %y.9711)
}

%AddComputation.9719 (x.9720: bf16[], y.9721: bf16[]) -> bf16[] {
  %x.9720 = bf16[] parameter(0)
  %y.9721 = bf16[] parameter(1)
  ROOT %add.9722 = bf16[] add(bf16[] %x.9720, bf16[] %y.9721)
}

%AddComputation.9754 (x.9755: bf16[], y.9756: bf16[]) -> bf16[] {
  %x.9755 = bf16[] parameter(0)
  %y.9756 = bf16[] parameter(1)
  ROOT %add.9757 = bf16[] add(bf16[] %x.9755, bf16[] %y.9756)
}

%AddComputation.9764 (x.9765: bf16[], y.9766: bf16[]) -> bf16[] {
  %x.9765 = bf16[] parameter(0)
  %y.9766 = bf16[] parameter(1)
  ROOT %add.9767 = bf16[] add(bf16[] %x.9765, bf16[] %y.9766)
}

%AddComputation.9778 (x.9779: bf16[], y.9780: bf16[]) -> bf16[] {
  %x.9779 = bf16[] parameter(0)
  %y.9780 = bf16[] parameter(1)
  ROOT %add.9781 = bf16[] add(bf16[] %x.9779, bf16[] %y.9780)
}

%AddComputation.9800 (x.9801: bf16[], y.9802: bf16[]) -> bf16[] {
  %x.9801 = bf16[] parameter(0)
  %y.9802 = bf16[] parameter(1)
  ROOT %add.9803 = bf16[] add(bf16[] %x.9801, bf16[] %y.9802)
}

%AddComputation.9810 (x.9811: bf16[], y.9812: bf16[]) -> bf16[] {
  %x.9811 = bf16[] parameter(0)
  %y.9812 = bf16[] parameter(1)
  ROOT %add.9813 = bf16[] add(bf16[] %x.9811, bf16[] %y.9812)
}

%AddComputation.9824 (x.9825: bf16[], y.9826: bf16[]) -> bf16[] {
  %x.9825 = bf16[] parameter(0)
  %y.9826 = bf16[] parameter(1)
  ROOT %add.9827 = bf16[] add(bf16[] %x.9825, bf16[] %y.9826)
}

%AddComputation.9843 (x.9844: bf16[], y.9845: bf16[]) -> bf16[] {
  %x.9844 = bf16[] parameter(0)
  %y.9845 = bf16[] parameter(1)
  ROOT %add.9846 = bf16[] add(bf16[] %x.9844, bf16[] %y.9845)
}

%AddComputation.9853 (x.9854: bf16[], y.9855: bf16[]) -> bf16[] {
  %x.9854 = bf16[] parameter(0)
  %y.9855 = bf16[] parameter(1)
  ROOT %add.9856 = bf16[] add(bf16[] %x.9854, bf16[] %y.9855)
}

%AddComputation.9868 (x.9869: bf16[], y.9870: bf16[]) -> bf16[] {
  %x.9869 = bf16[] parameter(0)
  %y.9870 = bf16[] parameter(1)
  ROOT %add.9871 = bf16[] add(bf16[] %x.9869, bf16[] %y.9870)
}

%AddComputation.9878 (x.9879: bf16[], y.9880: bf16[]) -> bf16[] {
  %x.9879 = bf16[] parameter(0)
  %y.9880 = bf16[] parameter(1)
  ROOT %add.9881 = bf16[] add(bf16[] %x.9879, bf16[] %y.9880)
}

%AddComputation.9913 (x.9914: bf16[], y.9915: bf16[]) -> bf16[] {
  %x.9914 = bf16[] parameter(0)
  %y.9915 = bf16[] parameter(1)
  ROOT %add.9916 = bf16[] add(bf16[] %x.9914, bf16[] %y.9915)
}

%AddComputation.9923 (x.9924: bf16[], y.9925: bf16[]) -> bf16[] {
  %x.9924 = bf16[] parameter(0)
  %y.9925 = bf16[] parameter(1)
  ROOT %add.9926 = bf16[] add(bf16[] %x.9924, bf16[] %y.9925)
}

%AddComputation.9940 (x.9941: bf16[], y.9942: bf16[]) -> bf16[] {
  %x.9941 = bf16[] parameter(0)
  %y.9942 = bf16[] parameter(1)
  ROOT %add.9943 = bf16[] add(bf16[] %x.9941, bf16[] %y.9942)
}

%AddComputation.9988 (x.9989: bf16[], y.9990: bf16[]) -> bf16[] {
  %x.9989 = bf16[] parameter(0)
  %y.9990 = bf16[] parameter(1)
  ROOT %add.9991 = bf16[] add(bf16[] %x.9989, bf16[] %y.9990)
}

%AddComputation.10001 (x.10002: bf16[], y.10003: bf16[]) -> bf16[] {
  %x.10002 = bf16[] parameter(0)
  %y.10003 = bf16[] parameter(1)
  ROOT %add.10004 = bf16[] add(bf16[] %x.10002, bf16[] %y.10003)
}

%AddComputation.10049 (x.10050: bf16[], y.10051: bf16[]) -> bf16[] {
  %x.10050 = bf16[] parameter(0)
  %y.10051 = bf16[] parameter(1)
  ROOT %add.10052 = bf16[] add(bf16[] %x.10050, bf16[] %y.10051)
}

%AddComputation.10090 (x.10091: bf16[], y.10092: bf16[]) -> bf16[] {
  %x.10091 = bf16[] parameter(0)
  %y.10092 = bf16[] parameter(1)
  ROOT %add.10093 = bf16[] add(bf16[] %x.10091, bf16[] %y.10092)
}

%AddComputation.10132 (x.10133: bf16[], y.10134: bf16[]) -> bf16[] {
  %x.10133 = bf16[] parameter(0)
  %y.10134 = bf16[] parameter(1)
  ROOT %add.10135 = bf16[] add(bf16[] %x.10133, bf16[] %y.10134)
}

%AddComputation.10142 (x.10143: bf16[], y.10144: bf16[]) -> bf16[] {
  %x.10143 = bf16[] parameter(0)
  %y.10144 = bf16[] parameter(1)
  ROOT %add.10145 = bf16[] add(bf16[] %x.10143, bf16[] %y.10144)
}

%AddComputation.10157 (x.10158: bf16[], y.10159: bf16[]) -> bf16[] {
  %x.10158 = bf16[] parameter(0)
  %y.10159 = bf16[] parameter(1)
  ROOT %add.10160 = bf16[] add(bf16[] %x.10158, bf16[] %y.10159)
}

%AddComputation.10167 (x.10168: bf16[], y.10169: bf16[]) -> bf16[] {
  %x.10168 = bf16[] parameter(0)
  %y.10169 = bf16[] parameter(1)
  ROOT %add.10170 = bf16[] add(bf16[] %x.10168, bf16[] %y.10169)
}

%AddComputation.10231 (x.10232: bf16[], y.10233: bf16[]) -> bf16[] {
  %x.10232 = bf16[] parameter(0)
  %y.10233 = bf16[] parameter(1)
  ROOT %add.10234 = bf16[] add(bf16[] %x.10232, bf16[] %y.10233)
}

%AddComputation.10241 (x.10242: bf16[], y.10243: bf16[]) -> bf16[] {
  %x.10242 = bf16[] parameter(0)
  %y.10243 = bf16[] parameter(1)
  ROOT %add.10244 = bf16[] add(bf16[] %x.10242, bf16[] %y.10243)
}

%AddComputation.10279 (x.10280: bf16[], y.10281: bf16[]) -> bf16[] {
  %x.10280 = bf16[] parameter(0)
  %y.10281 = bf16[] parameter(1)
  ROOT %add.10282 = bf16[] add(bf16[] %x.10280, bf16[] %y.10281)
}

%ScatterCombiner.10320 (p0.10321: bf16[], p1.10322: bf16[]) -> bf16[] {
  %p0.10321 = bf16[] parameter(0)
  %p1.10322 = bf16[] parameter(1)
  ROOT %add.10323 = bf16[] add(bf16[] %p0.10321, bf16[] %p1.10322)
}

%AddComputation.10336 (x.10337: bf16[], y.10338: bf16[]) -> bf16[] {
  %x.10337 = bf16[] parameter(0)
  %y.10338 = bf16[] parameter(1)
  ROOT %add.10339 = bf16[] add(bf16[] %x.10337, bf16[] %y.10338)
}

%AddComputation.10547 (x.10548: bf16[], y.10549: bf16[]) -> bf16[] {
  %x.10548 = bf16[] parameter(0)
  %y.10549 = bf16[] parameter(1)
  ROOT %add.10550 = bf16[] add(bf16[] %x.10548, bf16[] %y.10549)
}

%SimpleCrossEntropyLossForwardMax.4563 (p0.4564: bf16[], p1.4565: bf16[]) -> bf16[] {
  %p0.4564 = bf16[] parameter(0)
  %p1.4565 = bf16[] parameter(1)
  ROOT %maximum.4566 = bf16[] maximum(bf16[] %p0.4564, bf16[] %p1.4565)
}

%SimpleCrossEntropyLossForwardAdd.4567 (p0.4568: bf16[], p1.4569: bf16[]) -> bf16[] {
  %p0.4568 = bf16[] parameter(0)
  %p1.4569 = bf16[] parameter(1)
  ROOT %add.4570 = bf16[] add(bf16[] %p0.4568, bf16[] %p1.4569)
}

%SimpleCrossEntropyLossForwardAdd.4571 (p0.4572: bf16[], p1.4573: bf16[]) -> bf16[] {
  %p0.4572 = bf16[] parameter(0)
  %p1.4573 = bf16[] parameter(1)
  ROOT %add.4574 = bf16[] add(bf16[] %p0.4572, bf16[] %p1.4573)
}

%SimpleCrossEntropyLossForwardAdd.4575 (p0.4576: bf16[], p1.4577: bf16[]) -> bf16[] {
  %p0.4576 = bf16[] parameter(0)
  %p1.4577 = bf16[] parameter(1)
  ROOT %add.4578 = bf16[] add(bf16[] %p0.4576, bf16[] %p1.4577)
}

%SimpleCrossEntropyLossForwardAdd.4579 (p0.4580: bf16[], p1.4581: bf16[]) -> bf16[] {
  %p0.4580 = bf16[] parameter(0)
  %p1.4581 = bf16[] parameter(1)
  ROOT %add.4582 = bf16[] add(bf16[] %p0.4580, bf16[] %p1.4581)
}

%SimpleCrossEntropyLossBackwardAdd.4628 (p0.4629: bf16[], p1.4630: bf16[]) -> bf16[] {
  %p0.4629 = bf16[] parameter(0)
  %p1.4630 = bf16[] parameter(1)
  ROOT %add.4631 = bf16[] add(bf16[] %p0.4629, bf16[] %p1.4630)
}

%Int32PermissiveEmbeddingScatterCombiner.10247 (p0.10248: bf16[], p1.10249: bf16[]) -> bf16[] {
  %p0.10248 = bf16[] parameter(0)
  %p1.10249 = bf16[] parameter(1)
  ROOT %add.10250 = bf16[] add(bf16[] %p0.10248, bf16[] %p1.10249)
}

%Int32PermissiveEmbeddingScatterCombiner.10199 (p0.10200: bf16[], p1.10201: bf16[]) -> bf16[] {
  %p0.10200 = bf16[] parameter(0)
  %p1.10201 = bf16[] parameter(1)
  ROOT %add.10202 = bf16[] add(bf16[] %p0.10200, bf16[] %p1.10201)
}

%AddComputation.10063 (x.10064: bf16[], y.10065: bf16[]) -> bf16[] {
  %x.10064 = bf16[] parameter(0)
  %y.10065 = bf16[] parameter(1)
  ROOT %add.10066 = bf16[] add(bf16[] %x.10064, bf16[] %y.10065)
}

%AddComputation.10021 (x.10022: bf16[], y.10023: bf16[]) -> bf16[] {
  %x.10022 = bf16[] parameter(0)
  %y.10023 = bf16[] parameter(1)
  ROOT %add.10024 = bf16[] add(bf16[] %x.10022, bf16[] %y.10023)
}

%AddComputation.9961 (x.9962: bf16[], y.9963: bf16[]) -> bf16[] {
  %x.9962 = bf16[] parameter(0)
  %y.9963 = bf16[] parameter(1)
  ROOT %add.9964 = bf16[] add(bf16[] %x.9962, bf16[] %y.9963)
}

%AddComputation.9616 (x.9617: bf16[], y.9618: bf16[]) -> bf16[] {
  %x.9617 = bf16[] parameter(0)
  %y.9618 = bf16[] parameter(1)
  ROOT %add.9619 = bf16[] add(bf16[] %x.9617, bf16[] %y.9618)
}

%AddComputation.9574 (x.9575: bf16[], y.9576: bf16[]) -> bf16[] {
  %x.9575 = bf16[] parameter(0)
  %y.9576 = bf16[] parameter(1)
  ROOT %add.9577 = bf16[] add(bf16[] %x.9575, bf16[] %y.9576)
}

%AddComputation.9514 (x.9515: bf16[], y.9516: bf16[]) -> bf16[] {
  %x.9515 = bf16[] parameter(0)
  %y.9516 = bf16[] parameter(1)
  ROOT %add.9517 = bf16[] add(bf16[] %x.9515, bf16[] %y.9516)
}

%AddComputation.9169 (x.9170: bf16[], y.9171: bf16[]) -> bf16[] {
  %x.9170 = bf16[] parameter(0)
  %y.9171 = bf16[] parameter(1)
  ROOT %add.9172 = bf16[] add(bf16[] %x.9170, bf16[] %y.9171)
}

%AddComputation.9127 (x.9128: bf16[], y.9129: bf16[]) -> bf16[] {
  %x.9128 = bf16[] parameter(0)
  %y.9129 = bf16[] parameter(1)
  ROOT %add.9130 = bf16[] add(bf16[] %x.9128, bf16[] %y.9129)
}

%AddComputation.9067 (x.9068: bf16[], y.9069: bf16[]) -> bf16[] {
  %x.9068 = bf16[] parameter(0)
  %y.9069 = bf16[] parameter(1)
  ROOT %add.9070 = bf16[] add(bf16[] %x.9068, bf16[] %y.9069)
}

%AddComputation.8722 (x.8723: bf16[], y.8724: bf16[]) -> bf16[] {
  %x.8723 = bf16[] parameter(0)
  %y.8724 = bf16[] parameter(1)
  ROOT %add.8725 = bf16[] add(bf16[] %x.8723, bf16[] %y.8724)
}

%AddComputation.8680 (x.8681: bf16[], y.8682: bf16[]) -> bf16[] {
  %x.8681 = bf16[] parameter(0)
  %y.8682 = bf16[] parameter(1)
  ROOT %add.8683 = bf16[] add(bf16[] %x.8681, bf16[] %y.8682)
}

%AddComputation.8620 (x.8621: bf16[], y.8622: bf16[]) -> bf16[] {
  %x.8621 = bf16[] parameter(0)
  %y.8622 = bf16[] parameter(1)
  ROOT %add.8623 = bf16[] add(bf16[] %x.8621, bf16[] %y.8622)
}

%AddComputation.8275 (x.8276: bf16[], y.8277: bf16[]) -> bf16[] {
  %x.8276 = bf16[] parameter(0)
  %y.8277 = bf16[] parameter(1)
  ROOT %add.8278 = bf16[] add(bf16[] %x.8276, bf16[] %y.8277)
}

%AddComputation.8233 (x.8234: bf16[], y.8235: bf16[]) -> bf16[] {
  %x.8234 = bf16[] parameter(0)
  %y.8235 = bf16[] parameter(1)
  ROOT %add.8236 = bf16[] add(bf16[] %x.8234, bf16[] %y.8235)
}

%AddComputation.8173 (x.8174: bf16[], y.8175: bf16[]) -> bf16[] {
  %x.8174 = bf16[] parameter(0)
  %y.8175 = bf16[] parameter(1)
  ROOT %add.8176 = bf16[] add(bf16[] %x.8174, bf16[] %y.8175)
}

%AddComputation.7828 (x.7829: bf16[], y.7830: bf16[]) -> bf16[] {
  %x.7829 = bf16[] parameter(0)
  %y.7830 = bf16[] parameter(1)
  ROOT %add.7831 = bf16[] add(bf16[] %x.7829, bf16[] %y.7830)
}

%AddComputation.7786 (x.7787: bf16[], y.7788: bf16[]) -> bf16[] {
  %x.7787 = bf16[] parameter(0)
  %y.7788 = bf16[] parameter(1)
  ROOT %add.7789 = bf16[] add(bf16[] %x.7787, bf16[] %y.7788)
}

%AddComputation.7726 (x.7727: bf16[], y.7728: bf16[]) -> bf16[] {
  %x.7727 = bf16[] parameter(0)
  %y.7728 = bf16[] parameter(1)
  ROOT %add.7729 = bf16[] add(bf16[] %x.7727, bf16[] %y.7728)
}

%AddComputation.7381 (x.7382: bf16[], y.7383: bf16[]) -> bf16[] {
  %x.7382 = bf16[] parameter(0)
  %y.7383 = bf16[] parameter(1)
  ROOT %add.7384 = bf16[] add(bf16[] %x.7382, bf16[] %y.7383)
}

%AddComputation.7339 (x.7340: bf16[], y.7341: bf16[]) -> bf16[] {
  %x.7340 = bf16[] parameter(0)
  %y.7341 = bf16[] parameter(1)
  ROOT %add.7342 = bf16[] add(bf16[] %x.7340, bf16[] %y.7341)
}

%AddComputation.7279 (x.7280: bf16[], y.7281: bf16[]) -> bf16[] {
  %x.7280 = bf16[] parameter(0)
  %y.7281 = bf16[] parameter(1)
  ROOT %add.7282 = bf16[] add(bf16[] %x.7280, bf16[] %y.7281)
}

%AddComputation.6934 (x.6935: bf16[], y.6936: bf16[]) -> bf16[] {
  %x.6935 = bf16[] parameter(0)
  %y.6936 = bf16[] parameter(1)
  ROOT %add.6937 = bf16[] add(bf16[] %x.6935, bf16[] %y.6936)
}

%AddComputation.6892 (x.6893: bf16[], y.6894: bf16[]) -> bf16[] {
  %x.6893 = bf16[] parameter(0)
  %y.6894 = bf16[] parameter(1)
  ROOT %add.6895 = bf16[] add(bf16[] %x.6893, bf16[] %y.6894)
}

%AddComputation.6832 (x.6833: bf16[], y.6834: bf16[]) -> bf16[] {
  %x.6833 = bf16[] parameter(0)
  %y.6834 = bf16[] parameter(1)
  ROOT %add.6835 = bf16[] add(bf16[] %x.6833, bf16[] %y.6834)
}

%AddComputation.6487 (x.6488: bf16[], y.6489: bf16[]) -> bf16[] {
  %x.6488 = bf16[] parameter(0)
  %y.6489 = bf16[] parameter(1)
  ROOT %add.6490 = bf16[] add(bf16[] %x.6488, bf16[] %y.6489)
}

%AddComputation.6445 (x.6446: bf16[], y.6447: bf16[]) -> bf16[] {
  %x.6446 = bf16[] parameter(0)
  %y.6447 = bf16[] parameter(1)
  ROOT %add.6448 = bf16[] add(bf16[] %x.6446, bf16[] %y.6447)
}

%AddComputation.6385 (x.6386: bf16[], y.6387: bf16[]) -> bf16[] {
  %x.6386 = bf16[] parameter(0)
  %y.6387 = bf16[] parameter(1)
  ROOT %add.6388 = bf16[] add(bf16[] %x.6386, bf16[] %y.6387)
}

%AddComputation.6040 (x.6041: bf16[], y.6042: bf16[]) -> bf16[] {
  %x.6041 = bf16[] parameter(0)
  %y.6042 = bf16[] parameter(1)
  ROOT %add.6043 = bf16[] add(bf16[] %x.6041, bf16[] %y.6042)
}

%AddComputation.5998 (x.5999: bf16[], y.6000: bf16[]) -> bf16[] {
  %x.5999 = bf16[] parameter(0)
  %y.6000 = bf16[] parameter(1)
  ROOT %add.6001 = bf16[] add(bf16[] %x.5999, bf16[] %y.6000)
}

%AddComputation.5938 (x.5939: bf16[], y.5940: bf16[]) -> bf16[] {
  %x.5939 = bf16[] parameter(0)
  %y.5940 = bf16[] parameter(1)
  ROOT %add.5941 = bf16[] add(bf16[] %x.5939, bf16[] %y.5940)
}

%AddComputation.5593 (x.5594: bf16[], y.5595: bf16[]) -> bf16[] {
  %x.5594 = bf16[] parameter(0)
  %y.5595 = bf16[] parameter(1)
  ROOT %add.5596 = bf16[] add(bf16[] %x.5594, bf16[] %y.5595)
}

%AddComputation.5551 (x.5552: bf16[], y.5553: bf16[]) -> bf16[] {
  %x.5552 = bf16[] parameter(0)
  %y.5553 = bf16[] parameter(1)
  ROOT %add.5554 = bf16[] add(bf16[] %x.5552, bf16[] %y.5553)
}

%AddComputation.5491 (x.5492: bf16[], y.5493: bf16[]) -> bf16[] {
  %x.5492 = bf16[] parameter(0)
  %y.5493 = bf16[] parameter(1)
  ROOT %add.5494 = bf16[] add(bf16[] %x.5492, bf16[] %y.5493)
}

%AddComputation.5146 (x.5147: bf16[], y.5148: bf16[]) -> bf16[] {
  %x.5147 = bf16[] parameter(0)
  %y.5148 = bf16[] parameter(1)
  ROOT %add.5149 = bf16[] add(bf16[] %x.5147, bf16[] %y.5148)
}

%AddComputation.5104 (x.5105: bf16[], y.5106: bf16[]) -> bf16[] {
  %x.5105 = bf16[] parameter(0)
  %y.5106 = bf16[] parameter(1)
  ROOT %add.5107 = bf16[] add(bf16[] %x.5105, bf16[] %y.5106)
}

%AddComputation.5044 (x.5045: bf16[], y.5046: bf16[]) -> bf16[] {
  %x.5045 = bf16[] parameter(0)
  %y.5046 = bf16[] parameter(1)
  ROOT %add.5047 = bf16[] add(bf16[] %x.5045, bf16[] %y.5046)
}

%AddComputation.10073 (x.10074: bf16[], y.10075: bf16[]) -> bf16[] {
  %x.10074 = bf16[] parameter(0)
  %y.10075 = bf16[] parameter(1)
  ROOT %add.10076 = bf16[] add(bf16[] %x.10074, bf16[] %y.10075)
}

%AddComputation.10031 (x.10032: bf16[], y.10033: bf16[]) -> bf16[] {
  %x.10032 = bf16[] parameter(0)
  %y.10033 = bf16[] parameter(1)
  ROOT %add.10034 = bf16[] add(bf16[] %x.10032, bf16[] %y.10033)
}

%AddComputation.9971 (x.9972: bf16[], y.9973: bf16[]) -> bf16[] {
  %x.9972 = bf16[] parameter(0)
  %y.9973 = bf16[] parameter(1)
  ROOT %add.9974 = bf16[] add(bf16[] %x.9972, bf16[] %y.9973)
}

%AddComputation.9626 (x.9627: bf16[], y.9628: bf16[]) -> bf16[] {
  %x.9627 = bf16[] parameter(0)
  %y.9628 = bf16[] parameter(1)
  ROOT %add.9629 = bf16[] add(bf16[] %x.9627, bf16[] %y.9628)
}

%AddComputation.9584 (x.9585: bf16[], y.9586: bf16[]) -> bf16[] {
  %x.9585 = bf16[] parameter(0)
  %y.9586 = bf16[] parameter(1)
  ROOT %add.9587 = bf16[] add(bf16[] %x.9585, bf16[] %y.9586)
}

%AddComputation.9524 (x.9525: bf16[], y.9526: bf16[]) -> bf16[] {
  %x.9525 = bf16[] parameter(0)
  %y.9526 = bf16[] parameter(1)
  ROOT %add.9527 = bf16[] add(bf16[] %x.9525, bf16[] %y.9526)
}

%AddComputation.9179 (x.9180: bf16[], y.9181: bf16[]) -> bf16[] {
  %x.9180 = bf16[] parameter(0)
  %y.9181 = bf16[] parameter(1)
  ROOT %add.9182 = bf16[] add(bf16[] %x.9180, bf16[] %y.9181)
}

%AddComputation.9137 (x.9138: bf16[], y.9139: bf16[]) -> bf16[] {
  %x.9138 = bf16[] parameter(0)
  %y.9139 = bf16[] parameter(1)
  ROOT %add.9140 = bf16[] add(bf16[] %x.9138, bf16[] %y.9139)
}

%AddComputation.9077 (x.9078: bf16[], y.9079: bf16[]) -> bf16[] {
  %x.9078 = bf16[] parameter(0)
  %y.9079 = bf16[] parameter(1)
  ROOT %add.9080 = bf16[] add(bf16[] %x.9078, bf16[] %y.9079)
}

%AddComputation.8732 (x.8733: bf16[], y.8734: bf16[]) -> bf16[] {
  %x.8733 = bf16[] parameter(0)
  %y.8734 = bf16[] parameter(1)
  ROOT %add.8735 = bf16[] add(bf16[] %x.8733, bf16[] %y.8734)
}

%AddComputation.8690 (x.8691: bf16[], y.8692: bf16[]) -> bf16[] {
  %x.8691 = bf16[] parameter(0)
  %y.8692 = bf16[] parameter(1)
  ROOT %add.8693 = bf16[] add(bf16[] %x.8691, bf16[] %y.8692)
}

%AddComputation.8630 (x.8631: bf16[], y.8632: bf16[]) -> bf16[] {
  %x.8631 = bf16[] parameter(0)
  %y.8632 = bf16[] parameter(1)
  ROOT %add.8633 = bf16[] add(bf16[] %x.8631, bf16[] %y.8632)
}

%AddComputation.8285 (x.8286: bf16[], y.8287: bf16[]) -> bf16[] {
  %x.8286 = bf16[] parameter(0)
  %y.8287 = bf16[] parameter(1)
  ROOT %add.8288 = bf16[] add(bf16[] %x.8286, bf16[] %y.8287)
}

%AddComputation.8243 (x.8244: bf16[], y.8245: bf16[]) -> bf16[] {
  %x.8244 = bf16[] parameter(0)
  %y.8245 = bf16[] parameter(1)
  ROOT %add.8246 = bf16[] add(bf16[] %x.8244, bf16[] %y.8245)
}

%AddComputation.8183 (x.8184: bf16[], y.8185: bf16[]) -> bf16[] {
  %x.8184 = bf16[] parameter(0)
  %y.8185 = bf16[] parameter(1)
  ROOT %add.8186 = bf16[] add(bf16[] %x.8184, bf16[] %y.8185)
}

%AddComputation.7838 (x.7839: bf16[], y.7840: bf16[]) -> bf16[] {
  %x.7839 = bf16[] parameter(0)
  %y.7840 = bf16[] parameter(1)
  ROOT %add.7841 = bf16[] add(bf16[] %x.7839, bf16[] %y.7840)
}

%AddComputation.7796 (x.7797: bf16[], y.7798: bf16[]) -> bf16[] {
  %x.7797 = bf16[] parameter(0)
  %y.7798 = bf16[] parameter(1)
  ROOT %add.7799 = bf16[] add(bf16[] %x.7797, bf16[] %y.7798)
}

%AddComputation.7736 (x.7737: bf16[], y.7738: bf16[]) -> bf16[] {
  %x.7737 = bf16[] parameter(0)
  %y.7738 = bf16[] parameter(1)
  ROOT %add.7739 = bf16[] add(bf16[] %x.7737, bf16[] %y.7738)
}

%AddComputation.7391 (x.7392: bf16[], y.7393: bf16[]) -> bf16[] {
  %x.7392 = bf16[] parameter(0)
  %y.7393 = bf16[] parameter(1)
  ROOT %add.7394 = bf16[] add(bf16[] %x.7392, bf16[] %y.7393)
}

%AddComputation.7349 (x.7350: bf16[], y.7351: bf16[]) -> bf16[] {
  %x.7350 = bf16[] parameter(0)
  %y.7351 = bf16[] parameter(1)
  ROOT %add.7352 = bf16[] add(bf16[] %x.7350, bf16[] %y.7351)
}

%AddComputation.7289 (x.7290: bf16[], y.7291: bf16[]) -> bf16[] {
  %x.7290 = bf16[] parameter(0)
  %y.7291 = bf16[] parameter(1)
  ROOT %add.7292 = bf16[] add(bf16[] %x.7290, bf16[] %y.7291)
}

%AddComputation.6944 (x.6945: bf16[], y.6946: bf16[]) -> bf16[] {
  %x.6945 = bf16[] parameter(0)
  %y.6946 = bf16[] parameter(1)
  ROOT %add.6947 = bf16[] add(bf16[] %x.6945, bf16[] %y.6946)
}

%AddComputation.6902 (x.6903: bf16[], y.6904: bf16[]) -> bf16[] {
  %x.6903 = bf16[] parameter(0)
  %y.6904 = bf16[] parameter(1)
  ROOT %add.6905 = bf16[] add(bf16[] %x.6903, bf16[] %y.6904)
}

%AddComputation.6842 (x.6843: bf16[], y.6844: bf16[]) -> bf16[] {
  %x.6843 = bf16[] parameter(0)
  %y.6844 = bf16[] parameter(1)
  ROOT %add.6845 = bf16[] add(bf16[] %x.6843, bf16[] %y.6844)
}

%AddComputation.6497 (x.6498: bf16[], y.6499: bf16[]) -> bf16[] {
  %x.6498 = bf16[] parameter(0)
  %y.6499 = bf16[] parameter(1)
  ROOT %add.6500 = bf16[] add(bf16[] %x.6498, bf16[] %y.6499)
}

%AddComputation.6455 (x.6456: bf16[], y.6457: bf16[]) -> bf16[] {
  %x.6456 = bf16[] parameter(0)
  %y.6457 = bf16[] parameter(1)
  ROOT %add.6458 = bf16[] add(bf16[] %x.6456, bf16[] %y.6457)
}

%AddComputation.6395 (x.6396: bf16[], y.6397: bf16[]) -> bf16[] {
  %x.6396 = bf16[] parameter(0)
  %y.6397 = bf16[] parameter(1)
  ROOT %add.6398 = bf16[] add(bf16[] %x.6396, bf16[] %y.6397)
}

%AddComputation.6050 (x.6051: bf16[], y.6052: bf16[]) -> bf16[] {
  %x.6051 = bf16[] parameter(0)
  %y.6052 = bf16[] parameter(1)
  ROOT %add.6053 = bf16[] add(bf16[] %x.6051, bf16[] %y.6052)
}

%AddComputation.6008 (x.6009: bf16[], y.6010: bf16[]) -> bf16[] {
  %x.6009 = bf16[] parameter(0)
  %y.6010 = bf16[] parameter(1)
  ROOT %add.6011 = bf16[] add(bf16[] %x.6009, bf16[] %y.6010)
}

%AddComputation.5948 (x.5949: bf16[], y.5950: bf16[]) -> bf16[] {
  %x.5949 = bf16[] parameter(0)
  %y.5950 = bf16[] parameter(1)
  ROOT %add.5951 = bf16[] add(bf16[] %x.5949, bf16[] %y.5950)
}

%AddComputation.5603 (x.5604: bf16[], y.5605: bf16[]) -> bf16[] {
  %x.5604 = bf16[] parameter(0)
  %y.5605 = bf16[] parameter(1)
  ROOT %add.5606 = bf16[] add(bf16[] %x.5604, bf16[] %y.5605)
}

%AddComputation.5561 (x.5562: bf16[], y.5563: bf16[]) -> bf16[] {
  %x.5562 = bf16[] parameter(0)
  %y.5563 = bf16[] parameter(1)
  ROOT %add.5564 = bf16[] add(bf16[] %x.5562, bf16[] %y.5563)
}

%AddComputation.5501 (x.5502: bf16[], y.5503: bf16[]) -> bf16[] {
  %x.5502 = bf16[] parameter(0)
  %y.5503 = bf16[] parameter(1)
  ROOT %add.5504 = bf16[] add(bf16[] %x.5502, bf16[] %y.5503)
}

%AddComputation.5156 (x.5157: bf16[], y.5158: bf16[]) -> bf16[] {
  %x.5157 = bf16[] parameter(0)
  %y.5158 = bf16[] parameter(1)
  ROOT %add.5159 = bf16[] add(bf16[] %x.5157, bf16[] %y.5158)
}

%AddComputation.5114 (x.5115: bf16[], y.5116: bf16[]) -> bf16[] {
  %x.5115 = bf16[] parameter(0)
  %y.5116 = bf16[] parameter(1)
  ROOT %add.5117 = bf16[] add(bf16[] %x.5115, bf16[] %y.5116)
}

%AddComputation.5054 (x.5055: bf16[], y.5056: bf16[]) -> bf16[] {
  %x.5055 = bf16[] parameter(0)
  %y.5056 = bf16[] parameter(1)
  ROOT %add.5057 = bf16[] add(bf16[] %x.5055, bf16[] %y.5056)
}

ENTRY %SyncTensorsGraph.16388 (p0.1: bf16[], p1.3: bf16[], p2.5: bf16[], p3.8: bf16[], p4.10: s64[4], p5.11: bf16[2], p6.12: bf16[2,768], p7.14: bf16[], p8.15: s64[], p9.201: bf16[768], p10.202: bf16[768,768], p11.205: bf16[768], p12.232: bf16[768], p13.259: bf16[768], p14.286: bf16[768], p15.313: bf16[768], p16.340: bf16[768], p17.367: bf16[768], p18.394: bf16[768], p19.421: bf16[768], p20.448: bf16[768], p21.475: bf16[768], p22.502: bf16[768], p23.529: bf16[768], p24.556: bf16[768], p25.583: bf16[768], p26.610: bf16[768], p27.637: bf16[768], p28.664: bf16[768], p29.691: bf16[768], p30.718: bf16[768], p31.745: bf16[768], p32.772: bf16[768], p33.799: bf16[768], p34.826: bf16[768], p35.886: bf16[768], p36.912: s64[1,512], p37.916: bf16[512,768], p38.933: s64[4,128], p39.935: bf16[2,768], p40.947: s64[4,128], p41.949: bf16[28996,768], p42.975: bf16[768], p43.1023: bf16[768], p44.1025: bf16[768,768], p45.1032: bf16[768], p46.1034: bf16[768,768], p47.1085: bf16[], p48.1086: s64[4,128], p49.1107: bf16[], p50.1113: bf16[768], p51.1115: bf16[768,768], p52.1134: bf16[768], p53.1136: bf16[768,768], p54.1200: bf16[768], p55.1247: bf16[768], p56.1249: bf16[768,3072], p57.1256: bf16[3072], p58.1258: bf16[3072,768], p59.1293: bf16[768], p60.1340: bf16[768], p61.1342: bf16[768,768], p62.1349: bf16[768], p63.1351: bf16[768,768], p64.1408: bf16[768], p65.1410: bf16[768,768], p66.1429: bf16[768], p67.1431: bf16[768,768], p68.1495: bf16[768], p69.1542: bf16[768], p70.1544: bf16[768,3072], p71.1551: bf16[3072], p72.1553: bf16[3072,768], p73.1588: bf16[768], p74.1635: bf16[768], p75.1637: bf16[768,768], p76.1644: bf16[768], p77.1646: bf16[768,768], p78.1703: bf16[768], p79.1705: bf16[768,768], p80.1724: bf16[768], p81.1726: bf16[768,768], p82.1790: bf16[768], p83.1837: bf16[768], p84.1839: bf16[768,3072], p85.1846: bf16[3072], p86.1848: bf16[3072,768], p87.1883: bf16[768], p88.1930: bf16[768], p89.1932: bf16[768,768], p90.1939: bf16[768], p91.1941: bf16[768,768], p92.1998: bf16[768], p93.2000: bf16[768,768], p94.2019: bf16[768], p95.2021: bf16[768,768], p96.2085: bf16[768], p97.2132: bf16[768], p98.2134: bf16[768,3072], p99.2141: bf16[3072], p100.2143: bf16[3072,768], p101.2178: bf16[768], p102.2225: bf16[768], p103.2227: bf16[768,768], p104.2234: bf16[768], p105.2236: bf16[768,768], p106.2293: bf16[768], p107.2295: bf16[768,768], p108.2314: bf16[768], p109.2316: bf16[768,768], p110.2380: bf16[768], p111.2427: bf16[768], p112.2429: bf16[768,3072], p113.2436: bf16[3072], p114.2438: bf16[3072,768], p115.2473: bf16[768], p116.2520: bf16[768], p117.2522: bf16[768,768], p118.2529: bf16[768], p119.2531: bf16[768,768], p120.2588: bf16[768], p121.2590: bf16[768,768], p122.2609: bf16[768], p123.2611: bf16[768,768], p124.2675: bf16[768], p125.2722: bf16[768], p126.2724: bf16[768,3072], p127.2731: bf16[3072], p128.2733: bf16[3072,768], p129.2768: bf16[768], p130.2815: bf16[768], p131.2817: bf16[768,768], p132.2824: bf16[768], p133.2826: bf16[768,768], p134.2883: bf16[768], p135.2885: bf16[768,768], p136.2904: bf16[768], p137.2906: bf16[768,768], p138.2970: bf16[768], p139.3017: bf16[768], p140.3019: bf16[768,3072], p141.3026: bf16[3072], p142.3028: bf16[3072,768], p143.3063: bf16[768], p144.3110: bf16[768], p145.3112: bf16[768,768], p146.3119: bf16[768], p147.3121: bf16[768,768], p148.3178: bf16[768], p149.3180: bf16[768,768], p150.3199: bf16[768], p151.3201: bf16[768,768], p152.3265: bf16[768], p153.3312: bf16[768], p154.3314: bf16[768,3072], p155.3321: bf16[3072], p156.3323: bf16[3072,768], p157.3358: bf16[768], p158.3405: bf16[768], p159.3407: bf16[768,768], p160.3414: bf16[768], p161.3416: bf16[768,768], p162.3473: bf16[768], p163.3475: bf16[768,768], p164.3494: bf16[768], p165.3496: bf16[768,768], p166.3560: bf16[768], p167.3607: bf16[768], p168.3609: bf16[768,3072], p169.3616: bf16[3072], p170.3618: bf16[3072,768], p171.3653: bf16[768], p172.3700: bf16[768], p173.3702: bf16[768,768], p174.3709: bf16[768], p175.3711: bf16[768,768], p176.3768: bf16[768], p177.3770: bf16[768,768], p178.3789: bf16[768], p179.3791: bf16[768,768], p180.3855: bf16[768], p181.3902: bf16[768], p182.3904: bf16[768,3072], p183.3911: bf16[3072], p184.3913: bf16[3072,768], p185.3948: bf16[768], p186.3995: bf16[768], p187.3997: bf16[768,768], p188.4004: bf16[768], p189.4006: bf16[768,768], p190.4063: bf16[768], p191.4065: bf16[768,768], p192.4084: bf16[768], p193.4086: bf16[768,768], p194.4150: bf16[768], p195.4197: bf16[768], p196.4199: bf16[768,3072], p197.4206: bf16[3072], p198.4208: bf16[3072,768], p199.4243: bf16[768], p200.4290: bf16[768], p201.4292: bf16[768,768], p202.4299: bf16[768], p203.4301: bf16[768,768], p204.4358: bf16[768], p205.4360: bf16[768,768], p206.4379: bf16[768], p207.4381: bf16[768,768], p208.4445: bf16[768], p209.4492: bf16[768], p210.4494: bf16[768,3072], p211.4501: bf16[3072], p212.4503: bf16[3072,768], p213.4538: bf16[768], p214.10302: s64[], p215.10562: bf16[], p216.10563: bf16[28996,768], p217.10573: bf16[], p218.10579: bf16[28996,768], p219.10591: bf16[512,768], p220.10606: bf16[512,768], p221.10618: bf16[2,768], p222.10633: bf16[2,768], p223.10647: bf16[768], p224.10662: bf16[768], p225.10676: bf16[768], p226.10691: bf16[768], p227.10705: bf16[768,768], p228.10720: bf16[768,768], p229.10734: bf16[768], p230.10749: bf16[768], p231.10763: bf16[768,768], p232.10778: bf16[768,768], p233.10792: bf16[768], p234.10807: bf16[768], p235.10821: bf16[768,768], p236.10836: bf16[768,768], p237.10850: bf16[768], p238.10865: bf16[768], p239.10879: bf16[768,768], p240.10894: bf16[768,768], p241.10908: bf16[768], p242.10923: bf16[768], p243.10937: bf16[768], p244.10952: bf16[768], p245.10966: bf16[768], p246.10981: bf16[768], p247.10995: bf16[3072,768], p248.11010: bf16[3072,768], p249.11024: bf16[3072], p250.11039: bf16[3072], p251.11053: bf16[768,3072], p252.11068: bf16[768,3072], p253.11082: bf16[768], p254.11097: bf16[768], p255.11111: bf16[768], p256.11126: bf16[768], p257.11140: bf16[768], p258.11155: bf16[768], p259.11169: bf16[768,768], p260.11184: bf16[768,768], p261.11198: bf16[768], p262.11213: bf16[768], p263.11227: bf16[768,768], p264.11242: bf16[768,768], p265.11256: bf16[768], p266.11271: bf16[768], p267.11285: bf16[768,768], p268.11300: bf16[768,768], p269.11314: bf16[768], p270.11329: bf16[768], p271.11343: bf16[768,768], p272.11358: bf16[768,768], p273.11372: bf16[768], p274.11387: bf16[768], p275.11401: bf16[768], p276.11416: bf16[768], p277.11430: bf16[768], p278.11445: bf16[768], p279.11459: bf16[3072,768], p280.11474: bf16[3072,768], p281.11488: bf16[3072], p282.11503: bf16[3072], p283.11517: bf16[768,3072], p284.11532: bf16[768,3072], p285.11546: bf16[768], p286.11561: bf16[768], p287.11575: bf16[768], p288.11590: bf16[768], p289.11604: bf16[768], p290.11619: bf16[768], p291.11633: bf16[768,768], p292.11648: bf16[768,768], p293.11662: bf16[768], p294.11677: bf16[768], p295.11691: bf16[768,768], p296.11706: bf16[768,768], p297.11720: bf16[768], p298.11735: bf16[768], p299.11749: bf16[768,768], p300.11764: bf16[768,768], p301.11778: bf16[768], p302.11793: bf16[768], p303.11807: bf16[768,768], p304.11822: bf16[768,768], p305.11836: bf16[768], p306.11851: bf16[768], p307.11865: bf16[768], p308.11880: bf16[768], p309.11894: bf16[768], p310.11909: bf16[768], p311.11923: bf16[3072,768], p312.11938: bf16[3072,768], p313.11952: bf16[3072], p314.11967: bf16[3072], p315.11981: bf16[768,3072], p316.11996: bf16[768,3072], p317.12010: bf16[768], p318.12025: bf16[768], p319.12039: bf16[768], p320.12054: bf16[768], p321.12068: bf16[768], p322.12083: bf16[768], p323.12097: bf16[768,768], p324.12112: bf16[768,768], p325.12126: bf16[768], p326.12141: bf16[768], p327.12155: bf16[768,768], p328.12170: bf16[768,768], p329.12184: bf16[768], p330.12199: bf16[768], p331.12213: bf16[768,768], p332.12228: bf16[768,768], p333.12242: bf16[768], p334.12257: bf16[768], p335.12271: bf16[768,768], p336.12286: bf16[768,768], p337.12300: bf16[768], p338.12315: bf16[768], p339.12329: bf16[768], p340.12344: bf16[768], p341.12358: bf16[768], p342.12373: bf16[768], p343.12387: bf16[3072,768], p344.12402: bf16[3072,768], p345.12416: bf16[3072], p346.12431: bf16[3072], p347.12445: bf16[768,3072], p348.12460: bf16[768,3072], p349.12474: bf16[768], p350.12489: bf16[768], p351.12503: bf16[768], p352.12518: bf16[768], p353.12532: bf16[768], p354.12547: bf16[768], p355.12561: bf16[768,768], p356.12576: bf16[768,768], p357.12590: bf16[768], p358.12605: bf16[768], p359.12619: bf16[768,768], p360.12634: bf16[768,768], p361.12648: bf16[768], p362.12663: bf16[768], p363.12677: bf16[768,768], p364.12692: bf16[768,768], p365.12706: bf16[768], p366.12721: bf16[768], p367.12735: bf16[768,768], p368.12750: bf16[768,768], p369.12764: bf16[768], p370.12779: bf16[768], p371.12793: bf16[768], p372.12808: bf16[768], p373.12822: bf16[768], p374.12837: bf16[768], p375.12851: bf16[3072,768], p376.12866: bf16[3072,768], p377.12880: bf16[3072], p378.12895: bf16[3072], p379.12909: bf16[768,3072], p380.12924: bf16[768,3072], p381.12938: bf16[768], p382.12953: bf16[768], p383.12967: bf16[768], p384.12982: bf16[768], p385.12996: bf16[768], p386.13011: bf16[768], p387.13025: bf16[768,768], p388.13040: bf16[768,768], p389.13054: bf16[768], p390.13069: bf16[768], p391.13083: bf16[768,768], p392.13098: bf16[768,768], p393.13112: bf16[768], p394.13127: bf16[768], p395.13141: bf16[768,768], p396.13156: bf16[768,768], p397.13170: bf16[768], p398.13185: bf16[768], p399.13199: bf16[768,768], p400.13214: bf16[768,768], p401.13228: bf16[768], p402.13243: bf16[768], p403.13257: bf16[768], p404.13272: bf16[768], p405.13286: bf16[768], p406.13301: bf16[768], p407.13315: bf16[3072,768], p408.13330: bf16[3072,768], p409.13344: bf16[3072], p410.13359: bf16[3072], p411.13373: bf16[768,3072], p412.13388: bf16[768,3072], p413.13402: bf16[768], p414.13417: bf16[768], p415.13431: bf16[768], p416.13446: bf16[768], p417.13460: bf16[768], p418.13475: bf16[768], p419.13489: bf16[768,768], p420.13504: bf16[768,768], p421.13518: bf16[768], p422.13533: bf16[768], p423.13547: bf16[768,768], p424.13562: bf16[768,768], p425.13576: bf16[768], p426.13591: bf16[768], p427.13605: bf16[768,768], p428.13620: bf16[768,768], p429.13634: bf16[768], p430.13649: bf16[768], p431.13663: bf16[768,768], p432.13678: bf16[768,768], p433.13692: bf16[768], p434.13707: bf16[768], p435.13721: bf16[768], p436.13736: bf16[768], p437.13750: bf16[768], p438.13765: bf16[768], p439.13779: bf16[3072,768], p440.13794: bf16[3072,768], p441.13808: bf16[3072], p442.13823: bf16[3072], p443.13837: bf16[768,3072], p444.13852: bf16[768,3072], p445.13866: bf16[768], p446.13881: bf16[768], p447.13895: bf16[768], p448.13910: bf16[768], p449.13924: bf16[768], p450.13939: bf16[768], p451.13953: bf16[768,768], p452.13968: bf16[768,768], p453.13982: bf16[768], p454.13997: bf16[768], p455.14011: bf16[768,768], p456.14026: bf16[768,768], p457.14040: bf16[768], p458.14055: bf16[768], p459.14069: bf16[768,768], p460.14084: bf16[768,768], p461.14098: bf16[768], p462.14113: bf16[768], p463.14127: bf16[768,768], p464.14142: bf16[768,768], p465.14156: bf16[768], p466.14171: bf16[768], p467.14185: bf16[768], p468.14200: bf16[768], p469.14214: bf16[768], p470.14229: bf16[768], p471.14243: bf16[3072,768], p472.14258: bf16[3072,768], p473.14272: bf16[3072], p474.14287: bf16[3072], p475.14301: bf16[768,3072], p476.14316: bf16[768,3072], p477.14330: bf16[768], p478.14345: bf16[768], p479.14359: bf16[768], p480.14374: bf16[768], p481.14388: bf16[768], p482.14403: bf16[768], p483.14417: bf16[768,768], p484.14432: bf16[768,768], p485.14446: bf16[768], p486.14461: bf16[768], p487.14475: bf16[768,768], p488.14490: bf16[768,768], p489.14504: bf16[768], p490.14519: bf16[768], p491.14533: bf16[768,768], p492.14548: bf16[768,768], p493.14562: bf16[768], p494.14577: bf16[768], p495.14591: bf16[768,768], p496.14606: bf16[768,768], p497.14620: bf16[768], p498.14635: bf16[768], p499.14649: bf16[768], p500.14664: bf16[768], p501.14678: bf16[768], p502.14693: bf16[768], p503.14707: bf16[3072,768], p504.14722: bf16[3072,768], p505.14736: bf16[3072], p506.14751: bf16[3072], p507.14765: bf16[768,3072], p508.14780: bf16[768,3072], p509.14794: bf16[768], p510.14809: bf16[768], p511.14823: bf16[768], p512.14838: bf16[768], p513.14852: bf16[768], p514.14867: bf16[768], p515.14881: bf16[768,768], p516.14896: bf16[768,768], p517.14910: bf16[768], p518.14925: bf16[768], p519.14939: bf16[768,768], p520.14954: bf16[768,768], p521.14968: bf16[768], p522.14983: bf16[768], p523.14997: bf16[768,768], p524.15012: bf16[768,768], p525.15026: bf16[768], p526.15041: bf16[768], p527.15055: bf16[768,768], p528.15070: bf16[768,768], p529.15084: bf16[768], p530.15099: bf16[768], p531.15113: bf16[768], p532.15128: bf16[768], p533.15142: bf16[768], p534.15157: bf16[768], p535.15171: bf16[3072,768], p536.15186: bf16[3072,768], p537.15200: bf16[3072], p538.15215: bf16[3072], p539.15229: bf16[768,3072], p540.15244: bf16[768,3072], p541.15258: bf16[768], p542.15273: bf16[768], p543.15287: bf16[768], p544.15302: bf16[768], p545.15316: bf16[768], p546.15331: bf16[768], p547.15345: bf16[768,768], p548.15360: bf16[768,768], p549.15374: bf16[768], p550.15389: bf16[768], p551.15403: bf16[768,768], p552.15418: bf16[768,768], p553.15432: bf16[768], p554.15447: bf16[768], p555.15461: bf16[768,768], p556.15476: bf16[768,768], p557.15490: bf16[768], p558.15505: bf16[768], p559.15519: bf16[768,768], p560.15534: bf16[768,768], p561.15548: bf16[768], p562.15563: bf16[768], p563.15577: bf16[768], p564.15592: bf16[768], p565.15606: bf16[768], p566.15621: bf16[768], p567.15635: bf16[3072,768], p568.15650: bf16[3072,768], p569.15664: bf16[3072], p570.15679: bf16[3072], p571.15693: bf16[768,3072], p572.15708: bf16[768,3072], p573.15722: bf16[768], p574.15737: bf16[768], p575.15751: bf16[768], p576.15766: bf16[768], p577.15780: bf16[768], p578.15795: bf16[768], p579.15809: bf16[768,768], p580.15824: bf16[768,768], p581.15838: bf16[768], p582.15853: bf16[768], p583.15867: bf16[768,768], p584.15882: bf16[768,768], p585.15896: bf16[768], p586.15911: bf16[768], p587.15925: bf16[768,768], p588.15940: bf16[768,768], p589.15954: bf16[768], p590.15969: bf16[768], p591.15983: bf16[768,768], p592.15998: bf16[768,768], p593.16012: bf16[768], p594.16027: bf16[768], p595.16041: bf16[768], p596.16056: bf16[768], p597.16070: bf16[768], p598.16085: bf16[768], p599.16099: bf16[3072,768], p600.16114: bf16[3072,768], p601.16128: bf16[3072], p602.16143: bf16[3072], p603.16157: bf16[768,3072], p604.16172: bf16[768,3072], p605.16186: bf16[768], p606.16201: bf16[768], p607.16215: bf16[768], p608.16230: bf16[768], p609.16244: bf16[768], p610.16259: bf16[768], p611.16273: bf16[768,768], p612.16288: bf16[768,768], p613.16302: bf16[768], p614.16317: bf16[768], p615.16331: bf16[2,768], p616.16346: bf16[2,768], p617.16360: bf16[2], p618.16375: bf16[2], p619.16385: bf16[]) -> (bf16[28996,768], bf16[512,768], bf16[2,768], bf16[768], bf16[768], /*index=5*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=10*/bf16[768], bf16[768,768], bf16[768], bf16[768], bf16[768], /*index=15*/bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], bf16[768], /*index=20*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=25*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768], /*index=30*/bf16[768], bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], /*index=35*/bf16[768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=40*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=45*/bf16[768], bf16[768], bf16[3072,768], bf16[3072], bf16[768,3072], /*index=50*/bf16[768], bf16[768], bf16[768], bf16[768,768], bf16[768], /*index=55*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=60*/bf16[768], bf16[768], bf16[768], bf16[3072,768], bf16[3072], /*index=65*/bf16[768,3072], bf16[768], bf16[768], bf16[768], bf16[768,768], /*index=70*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=75*/bf16[768,768], bf16[768], bf16[768], bf16[768], bf16[3072,768], /*index=80*/bf16[3072], bf16[768,3072], bf16[768], bf16[768], bf16[768], /*index=85*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=90*/bf16[768], bf16[768,768], bf16[768], bf16[768], bf16[768], /*index=95*/bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], bf16[768], /*index=100*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=105*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768], /*index=110*/bf16[768], bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], /*index=115*/bf16[768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=120*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=125*/bf16[768], bf16[768], bf16[3072,768], bf16[3072], bf16[768,3072], /*index=130*/bf16[768], bf16[768], bf16[768], bf16[768,768], bf16[768], /*index=135*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=140*/bf16[768], bf16[768], bf16[768], bf16[3072,768], bf16[3072], /*index=145*/bf16[768,3072], bf16[768], bf16[768], bf16[768], bf16[768,768], /*index=150*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=155*/bf16[768,768], bf16[768], bf16[768], bf16[768], bf16[3072,768], /*index=160*/bf16[3072], bf16[768,3072], bf16[768], bf16[768], bf16[768], /*index=165*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768,768], /*index=170*/bf16[768], bf16[768,768], bf16[768], bf16[768], bf16[768], /*index=175*/bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], bf16[768], /*index=180*/bf16[768], bf16[768,768], bf16[768], bf16[768,768], bf16[768], /*index=185*/bf16[768,768], bf16[768], bf16[768,768], bf16[768], bf16[768], /*index=190*/bf16[768], bf16[3072,768], bf16[3072], bf16[768,3072], bf16[768], /*index=195*/bf16[768], bf16[768], bf16[768,768], bf16[768], bf16[2,768], /*index=200*/bf16[2], bf16[], bf16[28996,768], bf16[28996,768], bf16[512,768], /*index=205*/bf16[512,768], bf16[2,768], bf16[2,768], bf16[768,768], bf16[768,768], /*index=210*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=215*/bf16[768,768], bf16[3072,768], bf16[3072,768], bf16[768,3072], bf16[768,3072], /*index=220*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=225*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[3072,768], bf16[3072,768], /*index=230*/bf16[768,3072], bf16[768,3072], bf16[768,768], bf16[768,768], bf16[768,768], /*index=235*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=240*/bf16[3072,768], bf16[3072,768], bf16[768,3072], bf16[768,3072], bf16[768,768], /*index=245*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=250*/bf16[768,768], bf16[768,768], bf16[3072,768], bf16[3072,768], bf16[768,3072], /*index=255*/bf16[768,3072], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=260*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[3072,768], /*index=265*/bf16[3072,768], bf16[768,3072], bf16[768,3072], bf16[768,768], bf16[768,768], /*index=270*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=275*/bf16[768,768], bf16[3072,768], bf16[3072,768], bf16[768,3072], bf16[768,3072], /*index=280*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=285*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[3072,768], bf16[3072,768], /*index=290*/bf16[768,3072], bf16[768,3072], bf16[768,768], bf16[768,768], bf16[768,768], /*index=295*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=300*/bf16[3072,768], bf16[3072,768], bf16[768,3072], bf16[768,3072], bf16[768,768], /*index=305*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=310*/bf16[768,768], bf16[768,768], bf16[3072,768], bf16[3072,768], bf16[768,3072], /*index=315*/bf16[768,3072], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=320*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[3072,768], /*index=325*/bf16[3072,768], bf16[768,3072], bf16[768,3072], bf16[768,768], bf16[768,768], /*index=330*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=335*/bf16[768,768], bf16[3072,768], bf16[3072,768], bf16[768,3072], bf16[768,3072], /*index=340*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], bf16[768,768], /*index=345*/bf16[768,768], bf16[768,768], bf16[768,768], bf16[3072,768], bf16[3072,768], /*index=350*/bf16[768,3072], bf16[768,3072], bf16[768,768], bf16[768,768], bf16[2,768], /*index=355*/bf16[2,768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=360*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=365*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=370*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=375*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=380*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=385*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=390*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=395*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=400*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=405*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=410*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=415*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=420*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=425*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=430*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=435*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=440*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=445*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=450*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=455*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=460*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=465*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=470*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=475*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=480*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=485*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=490*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=495*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=500*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=505*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=510*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=515*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=520*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=525*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=530*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=535*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=540*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=545*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=550*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=555*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=560*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=565*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=570*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=575*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=580*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=585*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=590*/bf16[768], bf16[768], bf16[3072], bf16[3072], bf16[768], /*index=595*/bf16[768], bf16[768], bf16[768], bf16[768], bf16[768], /*index=600*/bf16[768], bf16[768], bf16[2], bf16[2], bf16[]) {
  %p41.949 = bf16[28996,768]{1,0} parameter(41), frontend_attributes={neff_input_names="input41"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %p218.10579 = bf16[28996,768]{1,0} parameter(218), frontend_attributes={neff_input_names="input218"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p7.14 = bf16[] parameter(7), frontend_attributes={neff_input_names="input7"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.10580 = bf16[28996,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10581 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %p218.10579, bf16[28996,768]{1,0} %broadcast.10580), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %constant.10314 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.10318 = bf16[28996,768]{1,0} broadcast(bf16[] %constant.10314), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %p40.947 = s64[4,128]{1,0} parameter(40), frontend_attributes={neff_input_names="input40"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2233}
  %reshape.10293 = s64[512]{0} reshape(s64[4,128]{1,0} %p40.947), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.10308 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.10309 = s64[512]{0} broadcast(s64[] %constant.10308), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt"}
  %compare.10310 = pred[512]{0} compare(s64[512]{0} %reshape.10293, s64[512]{0} %broadcast.10309), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt"}
  %p214.10302 = s64[] parameter(214), frontend_attributes={neff_input_names="input214"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %broadcast.10306 = s64[512]{0} broadcast(s64[] %p214.10302), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %add.10307 = s64[512]{0} add(s64[512]{0} %reshape.10293, s64[512]{0} %broadcast.10306), metadata={op_type="aten__add" op_name="aten__add"}
  %select.10311 = s64[512]{0} select(pred[512]{0} %compare.10310, s64[512]{0} %add.10307, s64[512]{0} %reshape.10293), metadata={op_type="aten__where" op_name="aten__where"}
  %reshape.10312 = s64[512,1]{1,0} reshape(s64[512]{0} %select.10311), metadata={op_type="aten__stack" op_name="aten__stack"}
  %convert.10294 = bf16[512]{0} convert(s64[512]{0} %reshape.10293), metadata={op_type="aten__ne" op_name="aten__ne"}
  %constant.10292 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.10295 = bf16[512]{0} broadcast(bf16[] %constant.10292), dimensions={}, metadata={op_type="aten__ne" op_name="aten__ne"}
  %compare.10296 = pred[512]{0} compare(bf16[512]{0} %convert.10294, bf16[512]{0} %broadcast.10295), direction=NE, metadata={op_type="aten__ne" op_name="aten__ne"}
  %broadcast.10300 = pred[512,768]{1,0} broadcast(pred[512]{0} %compare.10296), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %custom-call.30 = bf16[28996,768]{1,0} custom-call(bf16[28996,768]{1,0} %p41.949), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %convert.5 = u32[4,128]{1,0} convert(s64[4,128]{1,0} %p40.947)
  %reshape.2292 = u32[512]{0} reshape(u32[4,128]{1,0} %convert.5), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2233}
  %gather.958 = bf16[512,768]{1,0} gather(bf16[28996,768]{1,0} %custom-call.30, u32[512]{0} %reshape.2292), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2233}
  %p39.935 = bf16[2,768]{1,0} parameter(39), frontend_attributes={neff_input_names="input39"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.31 = bf16[2,768]{1,0} custom-call(bf16[2,768]{1,0} %p39.935), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %p38.933 = s64[4,128]{1,0} parameter(38), frontend_attributes={neff_input_names="input38"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %convert.4 = u32[4,128]{1,0} convert(s64[4,128]{1,0} %p38.933)
  %reshape.2290 = u32[512]{0} reshape(u32[4,128]{1,0} %convert.4), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %gather.944 = bf16[512,768]{1,0} gather(bf16[2,768]{1,0} %custom-call.31, u32[512]{0} %reshape.2290), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %add.82 = bf16[512,768]{1,0} add(bf16[512,768]{1,0} %gather.958, bf16[512,768]{1,0} %gather.944), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=233}
  %reshape.3005 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %add.82), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=233}
  %p37.916 = bf16[512,768]{1,0} parameter(37), frontend_attributes={neff_input_names="input37"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.32 = bf16[512,768]{1,0} custom-call(bf16[512,768]{1,0} %p37.916), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %p36.912 = s64[1,512]{1,0} parameter(36), frontend_attributes={neff_input_names="input36"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=216}
  %convert.3 = u32[1,512]{1,0} convert(s64[1,512]{1,0} %p36.912)
  %slice.0 = u32[1,128]{1,0} slice(u32[1,512]{1,0} %convert.3), slice={[0:1], [0:128]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %reshape.2288 = u32[128]{0} reshape(u32[1,128]{1,0} %slice.0), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %gather.925 = bf16[128,768]{1,0} gather(bf16[512,768]{1,0} %custom-call.32, u32[128]{0} %reshape.2288), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1137}
  %broadcast.963 = bf16[4,128,768]{2,1,0} broadcast(bf16[128,768]{1,0} %gather.925), dimensions={1,2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=236}
  %add.964 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3005, bf16[4,128,768]{2,1,0} %broadcast.963), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=236}
  %reshape.965 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.964), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.10173 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.10177 = bf16[512]{0} broadcast(bf16[] %constant.10173), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.902 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.906 = bf16[512]{0} broadcast(bf16[] %constant.902), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.897 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.901 = bf16[512]{0} broadcast(bf16[] %constant.897), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.966 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.965, bf16[512]{0} %broadcast.906, bf16[512]{0} %broadcast.901), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.968 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.966), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.969 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.966), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.970 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.971 = bf16[512]{0} broadcast(bf16[] %constant.970), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.972 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.969, bf16[512]{0} %broadcast.971), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt = bf16[512]{0} sqrt(bf16[512]{0} %add.972), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.10191 = bf16[512]{0} multiply(bf16[512]{0} %sqrt, bf16[512]{0} %sqrt), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.170 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.24 = bf16[512]{0} broadcast(bf16[] %constant.170), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.3 = bf16[512]{0} add(bf16[512]{0} %multiply.10191, bf16[512]{0} %broadcast.24), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p42.975 = bf16[768]{0} parameter(42), frontend_attributes={neff_input_names="input42"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.981 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p42.975), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.967 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.966), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.974 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.967), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p35.886 = bf16[768]{0} parameter(35), frontend_attributes={neff_input_names="input35"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.977 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p35.886), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.980 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.974, bf16[4,128,768]{2,1,0} %broadcast.977), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.982 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.981, bf16[4,128,768]{2,1,0} %multiply.980), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.16 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %p8.15 = s64[] parameter(8), frontend_attributes={neff_input_names="input8"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.17 = s64[] multiply(s64[] %constant.16, s64[] %p8.15), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.18 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.19 = s64[] add(s64[] %multiply.17, s64[] %constant.18), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.858 = u64[] convert(s64[] %add.19), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.862 = u64[1]{0} reshape(u64[] %convert.858), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.174 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.864 = u64[2]{0} concatenate(u64[1]{0} %reshape.862, u64[1]{0} %constant.174), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.865 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.864), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.866 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.865), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.868 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.869 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.868), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.870 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.866, u32[4,128,768]{2,1,0} %broadcast.869), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.871 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.870), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.179 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.25 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.179), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.877 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.871, f32[4,128,768]{2,1,0} %broadcast.25), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.880 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.877), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.855 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.881 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.880, bf16[4,128,768]{2,1,0} %broadcast.855), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.17 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.2 = bf16[] divide(bf16[] %constant.17, bf16[] %p7.14)
  %broadcast.26 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.2), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.168 = bf16[] constant(0)
  %broadcast.191 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.168), dimensions={}
  %select.42 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.881, bf16[4,128,768]{2,1,0} %broadcast.26, bf16[4,128,768]{2,1,0} %broadcast.191), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.984 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.982, bf16[4,128,768]{2,1,0} %select.42), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1138 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p53.1136 = bf16[768,768]{1,0} parameter(53), frontend_attributes={neff_input_names="input53"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1137 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p53.1136), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1139 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1138, bf16[768,768]{0,1} %transpose.1137), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1140 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1139), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p52.1134 = bf16[768]{0} parameter(52), frontend_attributes={neff_input_names="input52"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1141 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p52.1134), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1142 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1140, bf16[4,128,768]{2,1,0} %broadcast.1141), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1145 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1142), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1146 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1145), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1148 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1146), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1117 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p51.1115 = bf16[768,768]{1,0} parameter(51), frontend_attributes={neff_input_names="input51"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1116 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p51.1115), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1118 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1117, bf16[768,768]{0,1} %transpose.1116), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1119 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1118), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p50.1113 = bf16[768]{0} parameter(50), frontend_attributes={neff_input_names="input50"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1120 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p50.1113), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1121 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1119, bf16[4,128,768]{2,1,0} %broadcast.1120), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1124 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1121), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1126 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1124), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1128 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1126), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.1149 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.1148, bf16[48,64,128]{2,1,0} %reshape.1128), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %p49.1107 = bf16[] parameter(49), frontend_attributes={neff_input_names="input49"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.989 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.40 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.1149, bf16[48,128,128]{2,1,0} %broadcast.989), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3011 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.40), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %constant.1098 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/_tensor.py" source_line=909}
  %broadcast.991 = bf16[4,128]{1,0} broadcast(bf16[] %constant.1098), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/_tensor.py" source_line=909}
  %p48.1086 = s64[4,128]{1,0} parameter(48), frontend_attributes={neff_input_names="input48"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/modeling_utils.py" source_line=846}
  %convert.1 = bf16[4,128]{1,0} convert(s64[4,128]{1,0} %p48.1086), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/modeling_utils.py" source_line=857}
  %subtract.2 = bf16[4,128]{1,0} subtract(bf16[4,128]{1,0} %broadcast.991, bf16[4,128]{1,0} %convert.1), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/_tensor.py" source_line=909}
  %p47.1085 = bf16[] parameter(47), frontend_attributes={neff_input_names="input47"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/modeling_utils.py" source_line=858}
  %broadcast.1114 = bf16[4,128]{1,0} broadcast(bf16[] %p47.1085), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/modeling_utils.py" source_line=858}
  %multiply.282 = bf16[4,128]{1,0} multiply(bf16[4,128]{1,0} %subtract.2, bf16[4,128]{1,0} %broadcast.1114), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/modeling_utils.py" source_line=858}
  %broadcast.1155 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.1156 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3011, bf16[4,12,128,128]{3,2,1,0} %broadcast.1155), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.1157 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1162 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.1156, bf16[] %constant.1157), dimensions={3}, to_apply=%MaxComputation.1158, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1163 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1162), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.1164 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.1156, bf16[4,12,128,128]{3,2,1,0} %broadcast.1163), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.1165 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.1164), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.1166 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1171 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.1165, bf16[] %constant.1166), dimensions={3}, to_apply=%AddComputation.1167, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1172 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1171), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.1173 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.1165, bf16[4,12,128,128]{3,2,1,0} %broadcast.1172), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.20 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.21 = s64[] multiply(s64[] %constant.20, s64[] %add.19), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.22 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.23 = s64[] add(s64[] %multiply.21, s64[] %constant.22), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1053 = u64[] convert(s64[] %add.23), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1057 = u64[1]{0} reshape(u64[] %convert.1053), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.180 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1059 = u64[2]{0} concatenate(u64[1]{0} %reshape.1057, u64[1]{0} %constant.180), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1060 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1059), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1061 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.1060), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1063 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1064 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.1063), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1065 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.1061, u32[4,12,128,128]{3,2,1,0} %broadcast.1064), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1066 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.1065), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.185 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.30 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.185), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1072 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.1066, f32[4,12,128,128]{3,2,1,0} %broadcast.30), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1075 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.1072), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1050 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1076 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.1075, bf16[4,12,128,128]{3,2,1,0} %broadcast.1050), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.19 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.3 = bf16[] divide(bf16[] %constant.19, bf16[] %p7.14)
  %broadcast.31 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.3), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.167 = bf16[] constant(0)
  %broadcast.178 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.167), dimensions={}
  %select.41 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.1076, bf16[4,12,128,128]{3,2,1,0} %broadcast.31, bf16[4,12,128,128]{3,2,1,0} %broadcast.178), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1174 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1173, bf16[4,12,128,128]{3,2,1,0} %select.41), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1176 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1174), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1036 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p46.1034 = bf16[768,768]{1,0} parameter(46), frontend_attributes={neff_input_names="input46"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1035 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p46.1034), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1037 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1036, bf16[768,768]{0,1} %transpose.1035), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1038 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1037), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p45.1032 = bf16[768]{0} parameter(45), frontend_attributes={neff_input_names="input45"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1039 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p45.1032), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1040 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1038, bf16[4,128,768]{2,1,0} %broadcast.1039), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1043 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1040), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.1044 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1043), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1046 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1044), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.1177 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.1176, bf16[48,128,64]{2,1,0} %reshape.1046), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1178 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1177), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1179 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.1178), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1181 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.1179), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p44.1025 = bf16[768,768]{1,0} parameter(44), frontend_attributes={neff_input_names="input44"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1026 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p44.1025), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1182 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1181, bf16[768,768]{0,1} %transpose.1026), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1183 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1182), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p43.1023 = bf16[768]{0} parameter(43), frontend_attributes={neff_input_names="input43"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1184 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p43.1023), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1185 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1183, bf16[4,128,768]{2,1,0} %broadcast.1184), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.24 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.25 = s64[] multiply(s64[] %constant.24, s64[] %add.23), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.26 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.27 = s64[] add(s64[] %multiply.25, s64[] %constant.26), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.991 = u64[] convert(s64[] %add.27), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.995 = u64[1]{0} reshape(u64[] %convert.991), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.186 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.997 = u64[2]{0} concatenate(u64[1]{0} %reshape.995, u64[1]{0} %constant.186), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.998 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.997), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.999 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.998), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1001 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1002 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1001), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1003 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.999, u32[4,128,768]{2,1,0} %broadcast.1002), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1004 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1003), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.191 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.32 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.191), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1010 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1004, f32[4,128,768]{2,1,0} %broadcast.32), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1013 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1010), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.988 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1014 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1013, bf16[4,128,768]{2,1,0} %broadcast.988), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.21 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.4 = bf16[] divide(bf16[] %constant.21, bf16[] %p7.14)
  %broadcast.33 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.4), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.165 = bf16[] constant(0)
  %broadcast.177 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.165), dimensions={}
  %select.40 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1014, bf16[4,128,768]{2,1,0} %broadcast.33, bf16[4,128,768]{2,1,0} %broadcast.177), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1188 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1185, bf16[4,128,768]{2,1,0} %select.40), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1189 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1188, bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.1190 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1189), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.9884 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.9888 = bf16[512]{0} broadcast(bf16[] %constant.9884), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.842 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.846 = bf16[512]{0} broadcast(bf16[] %constant.842), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.837 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.841 = bf16[512]{0} broadcast(bf16[] %constant.837), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1191 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1190, bf16[512]{0} %broadcast.846, bf16[512]{0} %broadcast.841), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1193 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1191), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1194 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1191), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1195 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1196 = bf16[512]{0} broadcast(bf16[] %constant.1195), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1197 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1194, bf16[512]{0} %broadcast.1196), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.1 = bf16[512]{0} sqrt(bf16[512]{0} %add.1197), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.9902 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.1, bf16[512]{0} %sqrt.1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.192 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.34 = bf16[512]{0} broadcast(bf16[] %constant.192), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.7 = bf16[512]{0} add(bf16[512]{0} %multiply.9902, bf16[512]{0} %broadcast.34), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p54.1200 = bf16[768]{0} parameter(54), frontend_attributes={neff_input_names="input54"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1206 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p54.1200), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1192 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1191), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1199 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1192), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p34.826 = bf16[768]{0} parameter(34), frontend_attributes={neff_input_names="input34"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1202 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p34.826), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1205 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1199, bf16[4,128,768]{2,1,0} %broadcast.1202), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1207 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1206, bf16[4,128,768]{2,1,0} %multiply.1205), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1260 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1207), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p58.1258 = bf16[3072,768]{1,0} parameter(58), frontend_attributes={neff_input_names="input58"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1259 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p58.1258), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1261 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.1260, bf16[768,3072]{0,1} %transpose.1259), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1262 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.1261), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p57.1256 = bf16[3072]{0} parameter(57), frontend_attributes={neff_input_names="input57"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1263 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p57.1256), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1264 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.1262, bf16[4,128,3072]{2,1,0} %broadcast.1263), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.33 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1264), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.1274 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.33), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p56.1249 = bf16[768,3072]{1,0} parameter(56), frontend_attributes={neff_input_names="input56"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1250 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p56.1249), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1275 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.1274, bf16[3072,768]{0,1} %transpose.1250), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1276 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1275), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p55.1247 = bf16[768]{0} parameter(55), frontend_attributes={neff_input_names="input55"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1277 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p55.1247), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1278 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1276, bf16[4,128,768]{2,1,0} %broadcast.1277), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.28 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.29 = s64[] multiply(s64[] %constant.28, s64[] %add.27), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.30 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.31 = s64[] add(s64[] %multiply.29, s64[] %constant.30), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1215 = u64[] convert(s64[] %add.31), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1219 = u64[1]{0} reshape(u64[] %convert.1215), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.194 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1221 = u64[2]{0} concatenate(u64[1]{0} %reshape.1219, u64[1]{0} %constant.194), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1222 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1221), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1223 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1222), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1225 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1226 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1225), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1227 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1223, u32[4,128,768]{2,1,0} %broadcast.1226), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1228 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1227), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.198 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.35 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.198), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1234 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1228, f32[4,128,768]{2,1,0} %broadcast.35), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1237 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1234), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1212 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1238 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1237, bf16[4,128,768]{2,1,0} %broadcast.1212), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.23 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.5 = bf16[] divide(bf16[] %constant.23, bf16[] %p7.14)
  %broadcast.36 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.5), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.163 = bf16[] constant(0)
  %broadcast.176 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.163), dimensions={}
  %select.39 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1238, bf16[4,128,768]{2,1,0} %broadcast.36, bf16[4,128,768]{2,1,0} %broadcast.176), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1281 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1278, bf16[4,128,768]{2,1,0} %select.39), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1282 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1281, bf16[4,128,768]{2,1,0} %add.1207), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.1283 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1282), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.9725 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.9729 = bf16[512]{0} broadcast(bf16[] %constant.9725), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.815 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.819 = bf16[512]{0} broadcast(bf16[] %constant.815), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.810 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.814 = bf16[512]{0} broadcast(bf16[] %constant.810), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1284 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1283, bf16[512]{0} %broadcast.819, bf16[512]{0} %broadcast.814), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1286 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1284), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1287 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1284), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1288 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1289 = bf16[512]{0} broadcast(bf16[] %constant.1288), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1290 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1287, bf16[512]{0} %broadcast.1289), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.2 = bf16[512]{0} sqrt(bf16[512]{0} %add.1290), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.9743 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.2, bf16[512]{0} %sqrt.2), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.199 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.37 = bf16[512]{0} broadcast(bf16[] %constant.199), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.9 = bf16[512]{0} add(bf16[512]{0} %multiply.9743, bf16[512]{0} %broadcast.37), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p59.1293 = bf16[768]{0} parameter(59), frontend_attributes={neff_input_names="input59"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1299 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p59.1293), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1285 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1284), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1292 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1285), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p33.799 = bf16[768]{0} parameter(33), frontend_attributes={neff_input_names="input33"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1295 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p33.799), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1298 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1292, bf16[4,128,768]{2,1,0} %broadcast.1295), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1300 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1299, bf16[4,128,768]{2,1,0} %multiply.1298), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1433 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p67.1431 = bf16[768,768]{1,0} parameter(67), frontend_attributes={neff_input_names="input67"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1432 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p67.1431), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1434 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1433, bf16[768,768]{0,1} %transpose.1432), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1435 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1434), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p66.1429 = bf16[768]{0} parameter(66), frontend_attributes={neff_input_names="input66"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1436 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p66.1429), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1437 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1435, bf16[4,128,768]{2,1,0} %broadcast.1436), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1440 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1437), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1441 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1440), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1443 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1441), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1412 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p65.1410 = bf16[768,768]{1,0} parameter(65), frontend_attributes={neff_input_names="input65"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1411 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p65.1410), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1413 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1412, bf16[768,768]{0,1} %transpose.1411), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1414 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1413), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p64.1408 = bf16[768]{0} parameter(64), frontend_attributes={neff_input_names="input64"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1415 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p64.1408), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1416 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1414, bf16[4,128,768]{2,1,0} %broadcast.1415), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1419 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1416), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1421 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1419), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1423 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1421), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.1444 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.1443, bf16[48,64,128]{2,1,0} %reshape.1423), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.993 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.41 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.1444, bf16[48,128,128]{2,1,0} %broadcast.993), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3016 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.41), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.1450 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.1451 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3016, bf16[4,12,128,128]{3,2,1,0} %broadcast.1450), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.1452 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1457 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.1451, bf16[] %constant.1452), dimensions={3}, to_apply=%MaxComputation.1453, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1458 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1457), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.1459 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.1451, bf16[4,12,128,128]{3,2,1,0} %broadcast.1458), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.1460 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.1459), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.1461 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1466 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.1460, bf16[] %constant.1461), dimensions={3}, to_apply=%AddComputation.1462, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1467 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1466), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.1468 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.1460, bf16[4,12,128,128]{3,2,1,0} %broadcast.1467), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.32 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.33 = s64[] multiply(s64[] %constant.32, s64[] %add.31), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.34 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.35 = s64[] add(s64[] %multiply.33, s64[] %constant.34), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1370 = u64[] convert(s64[] %add.35), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1374 = u64[1]{0} reshape(u64[] %convert.1370), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.201 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1376 = u64[2]{0} concatenate(u64[1]{0} %reshape.1374, u64[1]{0} %constant.201), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1377 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1376), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1378 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.1377), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1380 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1381 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.1380), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1382 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.1378, u32[4,12,128,128]{3,2,1,0} %broadcast.1381), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1383 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.1382), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.207 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.39 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.207), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1389 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.1383, f32[4,12,128,128]{3,2,1,0} %broadcast.39), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1392 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.1389), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1367 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1393 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.1392, bf16[4,12,128,128]{3,2,1,0} %broadcast.1367), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.25 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.6 = bf16[] divide(bf16[] %constant.25, bf16[] %p7.14)
  %broadcast.40 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.6), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.161 = bf16[] constant(0)
  %broadcast.175 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.161), dimensions={}
  %select.38 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.1393, bf16[4,12,128,128]{3,2,1,0} %broadcast.40, bf16[4,12,128,128]{3,2,1,0} %broadcast.175), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1469 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1468, bf16[4,12,128,128]{3,2,1,0} %select.38), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1471 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1469), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1353 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p63.1351 = bf16[768,768]{1,0} parameter(63), frontend_attributes={neff_input_names="input63"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1352 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p63.1351), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1354 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1353, bf16[768,768]{0,1} %transpose.1352), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1355 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1354), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p62.1349 = bf16[768]{0} parameter(62), frontend_attributes={neff_input_names="input62"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1356 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p62.1349), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1357 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1355, bf16[4,128,768]{2,1,0} %broadcast.1356), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1360 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1357), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.1361 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1360), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1363 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1361), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.1472 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.1471, bf16[48,128,64]{2,1,0} %reshape.1363), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1473 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1472), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1474 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.1473), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1476 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.1474), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p61.1342 = bf16[768,768]{1,0} parameter(61), frontend_attributes={neff_input_names="input61"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1343 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p61.1342), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1477 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1476, bf16[768,768]{0,1} %transpose.1343), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1478 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1477), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p60.1340 = bf16[768]{0} parameter(60), frontend_attributes={neff_input_names="input60"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1479 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p60.1340), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1480 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1478, bf16[4,128,768]{2,1,0} %broadcast.1479), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.36 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.37 = s64[] multiply(s64[] %constant.36, s64[] %add.35), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.38 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.39 = s64[] add(s64[] %multiply.37, s64[] %constant.38), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1308 = u64[] convert(s64[] %add.39), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1312 = u64[1]{0} reshape(u64[] %convert.1308), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.208 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1314 = u64[2]{0} concatenate(u64[1]{0} %reshape.1312, u64[1]{0} %constant.208), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1315 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1314), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1316 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1315), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1318 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1319 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1318), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1320 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1316, u32[4,128,768]{2,1,0} %broadcast.1319), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1321 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1320), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.213 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.41 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.213), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1327 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1321, f32[4,128,768]{2,1,0} %broadcast.41), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1330 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1327), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1305 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1331 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1330, bf16[4,128,768]{2,1,0} %broadcast.1305), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.27 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.7 = bf16[] divide(bf16[] %constant.27, bf16[] %p7.14)
  %broadcast.42 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.7), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.159 = bf16[] constant(0)
  %broadcast.174 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.159), dimensions={}
  %select.37 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1331, bf16[4,128,768]{2,1,0} %broadcast.42, bf16[4,128,768]{2,1,0} %broadcast.174), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1483 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1480, bf16[4,128,768]{2,1,0} %select.37), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1484 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1483, bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.1485 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1484), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.9437 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.9441 = bf16[512]{0} broadcast(bf16[] %constant.9437), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.788 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.792 = bf16[512]{0} broadcast(bf16[] %constant.788), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.783 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.787 = bf16[512]{0} broadcast(bf16[] %constant.783), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1486 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1485, bf16[512]{0} %broadcast.792, bf16[512]{0} %broadcast.787), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1488 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1486), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1489 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1486), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1490 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1491 = bf16[512]{0} broadcast(bf16[] %constant.1490), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1492 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1489, bf16[512]{0} %broadcast.1491), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.3 = bf16[512]{0} sqrt(bf16[512]{0} %add.1492), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.9455 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.3, bf16[512]{0} %sqrt.3), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.214 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.43 = bf16[512]{0} broadcast(bf16[] %constant.214), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.12 = bf16[512]{0} add(bf16[512]{0} %multiply.9455, bf16[512]{0} %broadcast.43), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p68.1495 = bf16[768]{0} parameter(68), frontend_attributes={neff_input_names="input68"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1501 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p68.1495), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1487 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1486), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1494 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1487), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p32.772 = bf16[768]{0} parameter(32), frontend_attributes={neff_input_names="input32"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1497 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p32.772), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1500 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1494, bf16[4,128,768]{2,1,0} %broadcast.1497), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1502 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1501, bf16[4,128,768]{2,1,0} %multiply.1500), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1555 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1502), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p72.1553 = bf16[3072,768]{1,0} parameter(72), frontend_attributes={neff_input_names="input72"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1554 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p72.1553), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1556 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.1555, bf16[768,3072]{0,1} %transpose.1554), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1557 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.1556), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p71.1551 = bf16[3072]{0} parameter(71), frontend_attributes={neff_input_names="input71"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1558 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p71.1551), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1559 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.1557, bf16[4,128,3072]{2,1,0} %broadcast.1558), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.34 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1559), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.1569 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.34), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p70.1544 = bf16[768,3072]{1,0} parameter(70), frontend_attributes={neff_input_names="input70"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1545 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p70.1544), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1570 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.1569, bf16[3072,768]{0,1} %transpose.1545), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1571 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1570), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p69.1542 = bf16[768]{0} parameter(69), frontend_attributes={neff_input_names="input69"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1572 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p69.1542), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1573 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1571, bf16[4,128,768]{2,1,0} %broadcast.1572), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.40 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.41 = s64[] multiply(s64[] %constant.40, s64[] %add.39), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.42 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.43 = s64[] add(s64[] %multiply.41, s64[] %constant.42), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1510 = u64[] convert(s64[] %add.43), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1514 = u64[1]{0} reshape(u64[] %convert.1510), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.217 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1516 = u64[2]{0} concatenate(u64[1]{0} %reshape.1514, u64[1]{0} %constant.217), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1517 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1516), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1518 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1517), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1520 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1521 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1520), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1522 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1518, u32[4,128,768]{2,1,0} %broadcast.1521), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1523 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1522), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.222 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.44 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.222), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1529 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1523, f32[4,128,768]{2,1,0} %broadcast.44), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1532 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1529), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1507 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1533 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1532, bf16[4,128,768]{2,1,0} %broadcast.1507), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.29 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.8 = bf16[] divide(bf16[] %constant.29, bf16[] %p7.14)
  %broadcast.45 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.8), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.157 = bf16[] constant(0)
  %broadcast.173 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.157), dimensions={}
  %select.36 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1533, bf16[4,128,768]{2,1,0} %broadcast.45, bf16[4,128,768]{2,1,0} %broadcast.173), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1576 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1573, bf16[4,128,768]{2,1,0} %select.36), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1577 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1576, bf16[4,128,768]{2,1,0} %add.1502), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.1578 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1577), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.9278 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.9282 = bf16[512]{0} broadcast(bf16[] %constant.9278), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.761 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.765 = bf16[512]{0} broadcast(bf16[] %constant.761), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.756 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.760 = bf16[512]{0} broadcast(bf16[] %constant.756), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1579 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1578, bf16[512]{0} %broadcast.765, bf16[512]{0} %broadcast.760), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1581 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1579), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1582 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1579), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1583 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1584 = bf16[512]{0} broadcast(bf16[] %constant.1583), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1585 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1582, bf16[512]{0} %broadcast.1584), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.4 = bf16[512]{0} sqrt(bf16[512]{0} %add.1585), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.9296 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.4, bf16[512]{0} %sqrt.4), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.223 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.46 = bf16[512]{0} broadcast(bf16[] %constant.223), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.14 = bf16[512]{0} add(bf16[512]{0} %multiply.9296, bf16[512]{0} %broadcast.46), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p73.1588 = bf16[768]{0} parameter(73), frontend_attributes={neff_input_names="input73"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1594 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p73.1588), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1580 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1579), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1587 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1580), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p31.745 = bf16[768]{0} parameter(31), frontend_attributes={neff_input_names="input31"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1590 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p31.745), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1593 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1587, bf16[4,128,768]{2,1,0} %broadcast.1590), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1595 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1594, bf16[4,128,768]{2,1,0} %multiply.1593), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1728 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p81.1726 = bf16[768,768]{1,0} parameter(81), frontend_attributes={neff_input_names="input81"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1727 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p81.1726), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1729 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1728, bf16[768,768]{0,1} %transpose.1727), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1730 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1729), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p80.1724 = bf16[768]{0} parameter(80), frontend_attributes={neff_input_names="input80"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1731 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p80.1724), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1732 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1730, bf16[4,128,768]{2,1,0} %broadcast.1731), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1735 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1732), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1736 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1735), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1738 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1736), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1707 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p79.1705 = bf16[768,768]{1,0} parameter(79), frontend_attributes={neff_input_names="input79"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1706 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p79.1705), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1708 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1707, bf16[768,768]{0,1} %transpose.1706), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1709 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1708), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p78.1703 = bf16[768]{0} parameter(78), frontend_attributes={neff_input_names="input78"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1710 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p78.1703), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1711 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1709, bf16[4,128,768]{2,1,0} %broadcast.1710), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1714 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1711), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.1716 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1714), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.1718 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1716), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.1739 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.1738, bf16[48,64,128]{2,1,0} %reshape.1718), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.995 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.42 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.1739, bf16[48,128,128]{2,1,0} %broadcast.995), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3019 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.42), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.1745 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.1746 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3019, bf16[4,12,128,128]{3,2,1,0} %broadcast.1745), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.1747 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1752 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.1746, bf16[] %constant.1747), dimensions={3}, to_apply=%MaxComputation.1748, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1753 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1752), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.1754 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.1746, bf16[4,12,128,128]{3,2,1,0} %broadcast.1753), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.1755 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.1754), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.1756 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.1761 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.1755, bf16[] %constant.1756), dimensions={3}, to_apply=%AddComputation.1757, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.1762 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.1761), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.1763 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.1755, bf16[4,12,128,128]{3,2,1,0} %broadcast.1762), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.44 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.45 = s64[] multiply(s64[] %constant.44, s64[] %add.43), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.46 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.47 = s64[] add(s64[] %multiply.45, s64[] %constant.46), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1665 = u64[] convert(s64[] %add.47), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1669 = u64[1]{0} reshape(u64[] %convert.1665), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.225 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1671 = u64[2]{0} concatenate(u64[1]{0} %reshape.1669, u64[1]{0} %constant.225), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1672 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1671), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1673 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.1672), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1675 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1676 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.1675), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1677 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.1673, u32[4,12,128,128]{3,2,1,0} %broadcast.1676), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1678 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.1677), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.230 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.48 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.230), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1684 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.1678, f32[4,12,128,128]{3,2,1,0} %broadcast.48), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1687 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.1684), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1662 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1688 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.1687, bf16[4,12,128,128]{3,2,1,0} %broadcast.1662), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.31 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.9 = bf16[] divide(bf16[] %constant.31, bf16[] %p7.14)
  %broadcast.49 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.9), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.155 = bf16[] constant(0)
  %broadcast.172 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.155), dimensions={}
  %select.35 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.1688, bf16[4,12,128,128]{3,2,1,0} %broadcast.49, bf16[4,12,128,128]{3,2,1,0} %broadcast.172), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1764 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1763, bf16[4,12,128,128]{3,2,1,0} %select.35), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1766 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1764), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1648 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p77.1646 = bf16[768,768]{1,0} parameter(77), frontend_attributes={neff_input_names="input77"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1647 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p77.1646), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1649 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1648, bf16[768,768]{0,1} %transpose.1647), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1650 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1649), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p76.1644 = bf16[768]{0} parameter(76), frontend_attributes={neff_input_names="input76"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1651 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p76.1644), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1652 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1650, bf16[4,128,768]{2,1,0} %broadcast.1651), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1655 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1652), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.1656 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1655), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1658 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1656), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.1767 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.1766, bf16[48,128,64]{2,1,0} %reshape.1658), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1768 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1767), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1769 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.1768), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1771 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.1769), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p75.1637 = bf16[768,768]{1,0} parameter(75), frontend_attributes={neff_input_names="input75"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1638 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p75.1637), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1772 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1771, bf16[768,768]{0,1} %transpose.1638), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1773 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1772), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p74.1635 = bf16[768]{0} parameter(74), frontend_attributes={neff_input_names="input74"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1774 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p74.1635), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1775 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1773, bf16[4,128,768]{2,1,0} %broadcast.1774), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.48 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.49 = s64[] multiply(s64[] %constant.48, s64[] %add.47), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.50 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.51 = s64[] add(s64[] %multiply.49, s64[] %constant.50), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1603 = u64[] convert(s64[] %add.51), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1607 = u64[1]{0} reshape(u64[] %convert.1603), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.232 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1609 = u64[2]{0} concatenate(u64[1]{0} %reshape.1607, u64[1]{0} %constant.232), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1610 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1609), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1611 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1610), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1613 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1614 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1613), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1615 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1611, u32[4,128,768]{2,1,0} %broadcast.1614), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1616 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1615), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.237 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.50 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.237), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1622 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1616, f32[4,128,768]{2,1,0} %broadcast.50), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1625 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1622), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1600 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1626 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1625, bf16[4,128,768]{2,1,0} %broadcast.1600), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.33 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.10 = bf16[] divide(bf16[] %constant.33, bf16[] %p7.14)
  %broadcast.51 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.10), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.153 = bf16[] constant(0)
  %broadcast.170 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.153), dimensions={}
  %select.34 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1626, bf16[4,128,768]{2,1,0} %broadcast.51, bf16[4,128,768]{2,1,0} %broadcast.170), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1778 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1775, bf16[4,128,768]{2,1,0} %select.34), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1779 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1778, bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.1780 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1779), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.8990 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.8994 = bf16[512]{0} broadcast(bf16[] %constant.8990), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.734 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.738 = bf16[512]{0} broadcast(bf16[] %constant.734), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.729 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.733 = bf16[512]{0} broadcast(bf16[] %constant.729), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1781 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1780, bf16[512]{0} %broadcast.738, bf16[512]{0} %broadcast.733), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1783 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1781), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1784 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1781), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1785 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1786 = bf16[512]{0} broadcast(bf16[] %constant.1785), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1787 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1784, bf16[512]{0} %broadcast.1786), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.5 = bf16[512]{0} sqrt(bf16[512]{0} %add.1787), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.9008 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.5, bf16[512]{0} %sqrt.5), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.239 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.52 = bf16[512]{0} broadcast(bf16[] %constant.239), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.17 = bf16[512]{0} add(bf16[512]{0} %multiply.9008, bf16[512]{0} %broadcast.52), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p82.1790 = bf16[768]{0} parameter(82), frontend_attributes={neff_input_names="input82"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1796 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p82.1790), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1782 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1781), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1789 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1782), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p30.718 = bf16[768]{0} parameter(30), frontend_attributes={neff_input_names="input30"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1792 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p30.718), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1795 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1789, bf16[4,128,768]{2,1,0} %broadcast.1792), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1797 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1796, bf16[4,128,768]{2,1,0} %multiply.1795), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1850 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1797), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p86.1848 = bf16[3072,768]{1,0} parameter(86), frontend_attributes={neff_input_names="input86"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1849 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p86.1848), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1851 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.1850, bf16[768,3072]{0,1} %transpose.1849), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1852 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.1851), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p85.1846 = bf16[3072]{0} parameter(85), frontend_attributes={neff_input_names="input85"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1853 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p85.1846), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1854 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.1852, bf16[4,128,3072]{2,1,0} %broadcast.1853), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.35 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1854), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.1864 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.35), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p84.1839 = bf16[768,3072]{1,0} parameter(84), frontend_attributes={neff_input_names="input84"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1840 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p84.1839), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1865 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.1864, bf16[3072,768]{0,1} %transpose.1840), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1866 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1865), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p83.1837 = bf16[768]{0} parameter(83), frontend_attributes={neff_input_names="input83"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1867 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p83.1837), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1868 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1866, bf16[4,128,768]{2,1,0} %broadcast.1867), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.52 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.53 = s64[] multiply(s64[] %constant.52, s64[] %add.51), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.54 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.55 = s64[] add(s64[] %multiply.53, s64[] %constant.54), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1805 = u64[] convert(s64[] %add.55), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1809 = u64[1]{0} reshape(u64[] %convert.1805), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.241 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1811 = u64[2]{0} concatenate(u64[1]{0} %reshape.1809, u64[1]{0} %constant.241), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1812 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1811), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1813 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1812), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1815 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1816 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1815), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1817 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1813, u32[4,128,768]{2,1,0} %broadcast.1816), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1818 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1817), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.246 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.53 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.246), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1824 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1818, f32[4,128,768]{2,1,0} %broadcast.53), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1827 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1824), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1802 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1828 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1827, bf16[4,128,768]{2,1,0} %broadcast.1802), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.35 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.11 = bf16[] divide(bf16[] %constant.35, bf16[] %p7.14)
  %broadcast.54 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.11), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.151 = bf16[] constant(0)
  %broadcast.168 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.151), dimensions={}
  %select.33 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1828, bf16[4,128,768]{2,1,0} %broadcast.54, bf16[4,128,768]{2,1,0} %broadcast.168), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1871 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.1868, bf16[4,128,768]{2,1,0} %select.33), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.1872 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.1871, bf16[4,128,768]{2,1,0} %add.1797), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.1873 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1872), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.8831 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.8835 = bf16[512]{0} broadcast(bf16[] %constant.8831), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.707 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.711 = bf16[512]{0} broadcast(bf16[] %constant.707), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.702 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.706 = bf16[512]{0} broadcast(bf16[] %constant.702), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.1874 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.1873, bf16[512]{0} %broadcast.711, bf16[512]{0} %broadcast.706), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1876 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1874), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1877 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1874), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.1878 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1879 = bf16[512]{0} broadcast(bf16[] %constant.1878), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1880 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.1877, bf16[512]{0} %broadcast.1879), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.6 = bf16[512]{0} sqrt(bf16[512]{0} %add.1880), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.8849 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.6, bf16[512]{0} %sqrt.6), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.247 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.55 = bf16[512]{0} broadcast(bf16[] %constant.247), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.20 = bf16[512]{0} add(bf16[512]{0} %multiply.8849, bf16[512]{0} %broadcast.55), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p87.1883 = bf16[768]{0} parameter(87), frontend_attributes={neff_input_names="input87"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1889 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p87.1883), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.1875 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.1874), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.1882 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.1875), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p29.691 = bf16[768]{0} parameter(29), frontend_attributes={neff_input_names="input29"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.1885 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p29.691), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.1888 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.1882, bf16[4,128,768]{2,1,0} %broadcast.1885), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.1890 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.1889, bf16[4,128,768]{2,1,0} %multiply.1888), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2023 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p95.2021 = bf16[768,768]{1,0} parameter(95), frontend_attributes={neff_input_names="input95"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2022 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p95.2021), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2024 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2023, bf16[768,768]{0,1} %transpose.2022), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2025 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2024), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p94.2019 = bf16[768]{0} parameter(94), frontend_attributes={neff_input_names="input94"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2026 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p94.2019), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2027 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2025, bf16[4,128,768]{2,1,0} %broadcast.2026), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2030 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2027), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2031 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2030), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2033 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2031), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2002 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p93.2000 = bf16[768,768]{1,0} parameter(93), frontend_attributes={neff_input_names="input93"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2001 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p93.2000), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2003 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2002, bf16[768,768]{0,1} %transpose.2001), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2004 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2003), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p92.1998 = bf16[768]{0} parameter(92), frontend_attributes={neff_input_names="input92"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2005 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p92.1998), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2006 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2004, bf16[4,128,768]{2,1,0} %broadcast.2005), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2009 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2006), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2011 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2009), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2013 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2011), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.2034 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.2033, bf16[48,64,128]{2,1,0} %reshape.2013), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.997 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.43 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.2034, bf16[48,128,128]{2,1,0} %broadcast.997), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3023 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.43), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.2040 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.2041 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3023, bf16[4,12,128,128]{3,2,1,0} %broadcast.2040), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.2042 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2047 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.2041, bf16[] %constant.2042), dimensions={3}, to_apply=%MaxComputation.2043, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2048 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2047), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.2049 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.2041, bf16[4,12,128,128]{3,2,1,0} %broadcast.2048), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.2050 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.2049), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.2051 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2056 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.2050, bf16[] %constant.2051), dimensions={3}, to_apply=%AddComputation.2052, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2057 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2056), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.2058 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.2050, bf16[4,12,128,128]{3,2,1,0} %broadcast.2057), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.56 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.57 = s64[] multiply(s64[] %constant.56, s64[] %add.55), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.58 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.59 = s64[] add(s64[] %multiply.57, s64[] %constant.58), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1960 = u64[] convert(s64[] %add.59), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1964 = u64[1]{0} reshape(u64[] %convert.1960), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.250 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1966 = u64[2]{0} concatenate(u64[1]{0} %reshape.1964, u64[1]{0} %constant.250), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1967 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1966), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1968 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.1967), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1970 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1971 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.1970), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1972 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.1968, u32[4,12,128,128]{3,2,1,0} %broadcast.1971), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1973 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.1972), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.255 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.57 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.255), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1979 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.1973, f32[4,12,128,128]{3,2,1,0} %broadcast.57), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1982 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.1979), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1957 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1983 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.1982, bf16[4,12,128,128]{3,2,1,0} %broadcast.1957), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.37 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.12 = bf16[] divide(bf16[] %constant.37, bf16[] %p7.14)
  %broadcast.58 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.12), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.149 = bf16[] constant(0)
  %broadcast.167 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.149), dimensions={}
  %select.32 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.1983, bf16[4,12,128,128]{3,2,1,0} %broadcast.58, bf16[4,12,128,128]{3,2,1,0} %broadcast.167), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2059 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2058, bf16[4,12,128,128]{3,2,1,0} %select.32), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2061 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2059), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1943 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p91.1941 = bf16[768,768]{1,0} parameter(91), frontend_attributes={neff_input_names="input91"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1942 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p91.1941), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.1944 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.1943, bf16[768,768]{0,1} %transpose.1942), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1945 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.1944), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p90.1939 = bf16[768]{0} parameter(90), frontend_attributes={neff_input_names="input90"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.1946 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p90.1939), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.1947 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.1945, bf16[4,128,768]{2,1,0} %broadcast.1946), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.1950 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.1947), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.1951 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.1950), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.1953 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1951), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.2062 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.2061, bf16[48,128,64]{2,1,0} %reshape.1953), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2063 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2062), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2064 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.2063), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2066 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.2064), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p89.1932 = bf16[768,768]{1,0} parameter(89), frontend_attributes={neff_input_names="input89"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.1933 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p89.1932), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2067 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2066, bf16[768,768]{0,1} %transpose.1933), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2068 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2067), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p88.1930 = bf16[768]{0} parameter(88), frontend_attributes={neff_input_names="input88"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2069 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p88.1930), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2070 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2068, bf16[4,128,768]{2,1,0} %broadcast.2069), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.60 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.61 = s64[] multiply(s64[] %constant.60, s64[] %add.59), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.62 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.63 = s64[] add(s64[] %multiply.61, s64[] %constant.62), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1898 = u64[] convert(s64[] %add.63), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.1902 = u64[1]{0} reshape(u64[] %convert.1898), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.256 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.1904 = u64[2]{0} concatenate(u64[1]{0} %reshape.1902, u64[1]{0} %constant.256), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.1905 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.1904), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.1906 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.1905), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.1908 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1909 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.1908), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.1910 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.1906, u32[4,128,768]{2,1,0} %broadcast.1909), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1911 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.1910), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.262 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.59 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.262), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.1917 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.1911, f32[4,128,768]{2,1,0} %broadcast.59), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.1920 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.1917), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.1895 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.1921 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.1920, bf16[4,128,768]{2,1,0} %broadcast.1895), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.39 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.13 = bf16[] divide(bf16[] %constant.39, bf16[] %p7.14)
  %broadcast.60 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.13), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.147 = bf16[] constant(0)
  %broadcast.166 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.147), dimensions={}
  %select.31 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.1921, bf16[4,128,768]{2,1,0} %broadcast.60, bf16[4,128,768]{2,1,0} %broadcast.166), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2073 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2070, bf16[4,128,768]{2,1,0} %select.31), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2074 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2073, bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.2075 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2074), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.8543 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.8547 = bf16[512]{0} broadcast(bf16[] %constant.8543), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.680 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.684 = bf16[512]{0} broadcast(bf16[] %constant.680), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.675 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.679 = bf16[512]{0} broadcast(bf16[] %constant.675), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2076 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2075, bf16[512]{0} %broadcast.684, bf16[512]{0} %broadcast.679), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2078 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2076), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2079 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2076), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2080 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2081 = bf16[512]{0} broadcast(bf16[] %constant.2080), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2082 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2079, bf16[512]{0} %broadcast.2081), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.7 = bf16[512]{0} sqrt(bf16[512]{0} %add.2082), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.8561 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.7, bf16[512]{0} %sqrt.7), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.263 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.61 = bf16[512]{0} broadcast(bf16[] %constant.263), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.24 = bf16[512]{0} add(bf16[512]{0} %multiply.8561, bf16[512]{0} %broadcast.61), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p96.2085 = bf16[768]{0} parameter(96), frontend_attributes={neff_input_names="input96"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2091 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p96.2085), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2077 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2076), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2084 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2077), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p28.664 = bf16[768]{0} parameter(28), frontend_attributes={neff_input_names="input28"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2087 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p28.664), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2090 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2084, bf16[4,128,768]{2,1,0} %broadcast.2087), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2092 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2091, bf16[4,128,768]{2,1,0} %multiply.2090), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2145 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2092), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p100.2143 = bf16[3072,768]{1,0} parameter(100), frontend_attributes={neff_input_names="input100"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2144 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p100.2143), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2146 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.2145, bf16[768,3072]{0,1} %transpose.2144), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2147 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.2146), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p99.2141 = bf16[3072]{0} parameter(99), frontend_attributes={neff_input_names="input99"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2148 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p99.2141), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2149 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.2147, bf16[4,128,3072]{2,1,0} %broadcast.2148), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.36 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2149), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.2159 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.36), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p98.2134 = bf16[768,3072]{1,0} parameter(98), frontend_attributes={neff_input_names="input98"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2135 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p98.2134), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2160 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.2159, bf16[3072,768]{0,1} %transpose.2135), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2161 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2160), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p97.2132 = bf16[768]{0} parameter(97), frontend_attributes={neff_input_names="input97"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2162 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p97.2132), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2163 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2161, bf16[4,128,768]{2,1,0} %broadcast.2162), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.64 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.65 = s64[] multiply(s64[] %constant.64, s64[] %add.63), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.66 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.67 = s64[] add(s64[] %multiply.65, s64[] %constant.66), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2100 = u64[] convert(s64[] %add.67), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2104 = u64[1]{0} reshape(u64[] %convert.2100), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.266 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2106 = u64[2]{0} concatenate(u64[1]{0} %reshape.2104, u64[1]{0} %constant.266), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2107 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2106), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2108 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2107), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2110 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2111 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2110), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2112 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2108, u32[4,128,768]{2,1,0} %broadcast.2111), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2113 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2112), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.271 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.62 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.271), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2119 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2113, f32[4,128,768]{2,1,0} %broadcast.62), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2122 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2119), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2097 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2123 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2122, bf16[4,128,768]{2,1,0} %broadcast.2097), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.41 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.14 = bf16[] divide(bf16[] %constant.41, bf16[] %p7.14)
  %broadcast.63 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.14), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.145 = bf16[] constant(0)
  %broadcast.165 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.145), dimensions={}
  %select.30 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2123, bf16[4,128,768]{2,1,0} %broadcast.63, bf16[4,128,768]{2,1,0} %broadcast.165), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2166 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2163, bf16[4,128,768]{2,1,0} %select.30), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2167 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2166, bf16[4,128,768]{2,1,0} %add.2092), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.2168 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2167), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.8384 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.8388 = bf16[512]{0} broadcast(bf16[] %constant.8384), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.653 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.657 = bf16[512]{0} broadcast(bf16[] %constant.653), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.648 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.652 = bf16[512]{0} broadcast(bf16[] %constant.648), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2169 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2168, bf16[512]{0} %broadcast.657, bf16[512]{0} %broadcast.652), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2171 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2169), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2172 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2169), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2173 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2174 = bf16[512]{0} broadcast(bf16[] %constant.2173), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2175 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2172, bf16[512]{0} %broadcast.2174), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.8 = bf16[512]{0} sqrt(bf16[512]{0} %add.2175), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.8402 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.8, bf16[512]{0} %sqrt.8), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.272 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.64 = bf16[512]{0} broadcast(bf16[] %constant.272), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.26 = bf16[512]{0} add(bf16[512]{0} %multiply.8402, bf16[512]{0} %broadcast.64), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p101.2178 = bf16[768]{0} parameter(101), frontend_attributes={neff_input_names="input101"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2184 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p101.2178), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2170 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2169), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2177 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2170), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p27.637 = bf16[768]{0} parameter(27), frontend_attributes={neff_input_names="input27"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2180 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p27.637), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2183 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2177, bf16[4,128,768]{2,1,0} %broadcast.2180), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2185 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2184, bf16[4,128,768]{2,1,0} %multiply.2183), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2318 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p109.2316 = bf16[768,768]{1,0} parameter(109), frontend_attributes={neff_input_names="input109"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2317 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p109.2316), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2319 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2318, bf16[768,768]{0,1} %transpose.2317), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2320 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2319), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p108.2314 = bf16[768]{0} parameter(108), frontend_attributes={neff_input_names="input108"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2321 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p108.2314), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2322 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2320, bf16[4,128,768]{2,1,0} %broadcast.2321), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2325 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2322), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2326 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2325), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2328 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2326), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2297 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p107.2295 = bf16[768,768]{1,0} parameter(107), frontend_attributes={neff_input_names="input107"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2296 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p107.2295), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2298 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2297, bf16[768,768]{0,1} %transpose.2296), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2299 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2298), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p106.2293 = bf16[768]{0} parameter(106), frontend_attributes={neff_input_names="input106"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2300 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p106.2293), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2301 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2299, bf16[4,128,768]{2,1,0} %broadcast.2300), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2304 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2301), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2306 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2304), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2308 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2306), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.2329 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.2328, bf16[48,64,128]{2,1,0} %reshape.2308), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.999 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.44 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.2329, bf16[48,128,128]{2,1,0} %broadcast.999), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3027 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.44), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.2335 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.2336 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3027, bf16[4,12,128,128]{3,2,1,0} %broadcast.2335), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.2337 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2342 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.2336, bf16[] %constant.2337), dimensions={3}, to_apply=%MaxComputation.2338, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2343 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2342), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.2344 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.2336, bf16[4,12,128,128]{3,2,1,0} %broadcast.2343), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.2345 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.2344), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.2346 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2351 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.2345, bf16[] %constant.2346), dimensions={3}, to_apply=%AddComputation.2347, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2352 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2351), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.2353 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.2345, bf16[4,12,128,128]{3,2,1,0} %broadcast.2352), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.68 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.69 = s64[] multiply(s64[] %constant.68, s64[] %add.67), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.70 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.71 = s64[] add(s64[] %multiply.69, s64[] %constant.70), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2255 = u64[] convert(s64[] %add.71), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2259 = u64[1]{0} reshape(u64[] %convert.2255), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.274 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2261 = u64[2]{0} concatenate(u64[1]{0} %reshape.2259, u64[1]{0} %constant.274), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2262 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2261), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2263 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.2262), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2265 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2266 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.2265), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2267 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.2263, u32[4,12,128,128]{3,2,1,0} %broadcast.2266), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2268 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.2267), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.279 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.66 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.279), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2274 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.2268, f32[4,12,128,128]{3,2,1,0} %broadcast.66), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2277 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.2274), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2252 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2278 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.2277, bf16[4,12,128,128]{3,2,1,0} %broadcast.2252), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.43 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.15 = bf16[] divide(bf16[] %constant.43, bf16[] %p7.14)
  %broadcast.67 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.15), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.143 = bf16[] constant(0)
  %broadcast.164 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.143), dimensions={}
  %select.29 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.2278, bf16[4,12,128,128]{3,2,1,0} %broadcast.67, bf16[4,12,128,128]{3,2,1,0} %broadcast.164), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2354 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2353, bf16[4,12,128,128]{3,2,1,0} %select.29), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2356 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2354), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2238 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p105.2236 = bf16[768,768]{1,0} parameter(105), frontend_attributes={neff_input_names="input105"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2237 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p105.2236), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2239 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2238, bf16[768,768]{0,1} %transpose.2237), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2240 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2239), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p104.2234 = bf16[768]{0} parameter(104), frontend_attributes={neff_input_names="input104"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2241 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p104.2234), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2242 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2240, bf16[4,128,768]{2,1,0} %broadcast.2241), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2245 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2242), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.2246 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2245), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2248 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2246), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.2357 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.2356, bf16[48,128,64]{2,1,0} %reshape.2248), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2358 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2357), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2359 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.2358), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2361 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.2359), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p103.2227 = bf16[768,768]{1,0} parameter(103), frontend_attributes={neff_input_names="input103"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2228 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p103.2227), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2362 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2361, bf16[768,768]{0,1} %transpose.2228), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2363 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2362), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p102.2225 = bf16[768]{0} parameter(102), frontend_attributes={neff_input_names="input102"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2364 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p102.2225), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2365 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2363, bf16[4,128,768]{2,1,0} %broadcast.2364), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.72 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.73 = s64[] multiply(s64[] %constant.72, s64[] %add.71), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.74 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.75 = s64[] add(s64[] %multiply.73, s64[] %constant.74), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2193 = u64[] convert(s64[] %add.75), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2197 = u64[1]{0} reshape(u64[] %convert.2193), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.281 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2199 = u64[2]{0} concatenate(u64[1]{0} %reshape.2197, u64[1]{0} %constant.281), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2200 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2199), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2201 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2200), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2203 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2204 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2203), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2205 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2201, u32[4,128,768]{2,1,0} %broadcast.2204), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2206 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2205), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.286 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.68 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.286), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2212 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2206, f32[4,128,768]{2,1,0} %broadcast.68), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2215 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2212), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2190 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2216 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2215, bf16[4,128,768]{2,1,0} %broadcast.2190), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.45 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.16 = bf16[] divide(bf16[] %constant.45, bf16[] %p7.14)
  %broadcast.69 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.16), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.141 = bf16[] constant(0)
  %broadcast.163 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.141), dimensions={}
  %select.28 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2216, bf16[4,128,768]{2,1,0} %broadcast.69, bf16[4,128,768]{2,1,0} %broadcast.163), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2368 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2365, bf16[4,128,768]{2,1,0} %select.28), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2369 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2368, bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.2370 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2369), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.8096 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.8100 = bf16[512]{0} broadcast(bf16[] %constant.8096), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.626 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.630 = bf16[512]{0} broadcast(bf16[] %constant.626), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.621 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.625 = bf16[512]{0} broadcast(bf16[] %constant.621), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2371 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2370, bf16[512]{0} %broadcast.630, bf16[512]{0} %broadcast.625), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2373 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2371), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2374 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2371), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2375 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2376 = bf16[512]{0} broadcast(bf16[] %constant.2375), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2377 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2374, bf16[512]{0} %broadcast.2376), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.9 = bf16[512]{0} sqrt(bf16[512]{0} %add.2377), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.8114 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.9, bf16[512]{0} %sqrt.9), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.288 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.70 = bf16[512]{0} broadcast(bf16[] %constant.288), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.30 = bf16[512]{0} add(bf16[512]{0} %multiply.8114, bf16[512]{0} %broadcast.70), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p110.2380 = bf16[768]{0} parameter(110), frontend_attributes={neff_input_names="input110"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2386 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p110.2380), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2372 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2371), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2379 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2372), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p26.610 = bf16[768]{0} parameter(26), frontend_attributes={neff_input_names="input26"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2382 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p26.610), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2385 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2379, bf16[4,128,768]{2,1,0} %broadcast.2382), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2387 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2386, bf16[4,128,768]{2,1,0} %multiply.2385), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2440 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2387), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p114.2438 = bf16[3072,768]{1,0} parameter(114), frontend_attributes={neff_input_names="input114"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2439 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p114.2438), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2441 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.2440, bf16[768,3072]{0,1} %transpose.2439), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2442 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.2441), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p113.2436 = bf16[3072]{0} parameter(113), frontend_attributes={neff_input_names="input113"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2443 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p113.2436), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2444 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.2442, bf16[4,128,3072]{2,1,0} %broadcast.2443), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.37 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2444), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.2454 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.37), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p112.2429 = bf16[768,3072]{1,0} parameter(112), frontend_attributes={neff_input_names="input112"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2430 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p112.2429), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2455 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.2454, bf16[3072,768]{0,1} %transpose.2430), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2456 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2455), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p111.2427 = bf16[768]{0} parameter(111), frontend_attributes={neff_input_names="input111"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2457 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p111.2427), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2458 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2456, bf16[4,128,768]{2,1,0} %broadcast.2457), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.76 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.77 = s64[] multiply(s64[] %constant.76, s64[] %add.75), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.78 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.79 = s64[] add(s64[] %multiply.77, s64[] %constant.78), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2395 = u64[] convert(s64[] %add.79), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2399 = u64[1]{0} reshape(u64[] %convert.2395), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.290 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2401 = u64[2]{0} concatenate(u64[1]{0} %reshape.2399, u64[1]{0} %constant.290), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2402 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2401), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2403 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2402), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2405 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2406 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2405), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2407 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2403, u32[4,128,768]{2,1,0} %broadcast.2406), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2408 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2407), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.295 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.71 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.295), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2414 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2408, f32[4,128,768]{2,1,0} %broadcast.71), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2417 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2414), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2392 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2418 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2417, bf16[4,128,768]{2,1,0} %broadcast.2392), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.47 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.17 = bf16[] divide(bf16[] %constant.47, bf16[] %p7.14)
  %broadcast.72 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.17), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.139 = bf16[] constant(0)
  %broadcast.162 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.139), dimensions={}
  %select.27 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2418, bf16[4,128,768]{2,1,0} %broadcast.72, bf16[4,128,768]{2,1,0} %broadcast.162), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2461 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2458, bf16[4,128,768]{2,1,0} %select.27), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2462 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2461, bf16[4,128,768]{2,1,0} %add.2387), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.2463 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2462), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.7937 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.7941 = bf16[512]{0} broadcast(bf16[] %constant.7937), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.599 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.603 = bf16[512]{0} broadcast(bf16[] %constant.599), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.594 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.598 = bf16[512]{0} broadcast(bf16[] %constant.594), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2464 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2463, bf16[512]{0} %broadcast.603, bf16[512]{0} %broadcast.598), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2466 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2464), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2467 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2464), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2468 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2469 = bf16[512]{0} broadcast(bf16[] %constant.2468), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2470 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2467, bf16[512]{0} %broadcast.2469), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.10 = bf16[512]{0} sqrt(bf16[512]{0} %add.2470), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.7955 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.10, bf16[512]{0} %sqrt.10), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.296 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.73 = bf16[512]{0} broadcast(bf16[] %constant.296), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.33 = bf16[512]{0} add(bf16[512]{0} %multiply.7955, bf16[512]{0} %broadcast.73), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p115.2473 = bf16[768]{0} parameter(115), frontend_attributes={neff_input_names="input115"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2479 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p115.2473), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2465 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2464), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2472 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2465), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p25.583 = bf16[768]{0} parameter(25), frontend_attributes={neff_input_names="input25"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2475 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p25.583), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2478 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2472, bf16[4,128,768]{2,1,0} %broadcast.2475), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2480 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2479, bf16[4,128,768]{2,1,0} %multiply.2478), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2613 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p123.2611 = bf16[768,768]{1,0} parameter(123), frontend_attributes={neff_input_names="input123"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2612 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p123.2611), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2614 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2613, bf16[768,768]{0,1} %transpose.2612), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2615 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2614), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p122.2609 = bf16[768]{0} parameter(122), frontend_attributes={neff_input_names="input122"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2616 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p122.2609), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2617 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2615, bf16[4,128,768]{2,1,0} %broadcast.2616), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2620 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2617), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2621 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2620), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2623 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2621), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2592 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p121.2590 = bf16[768,768]{1,0} parameter(121), frontend_attributes={neff_input_names="input121"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2591 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p121.2590), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2593 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2592, bf16[768,768]{0,1} %transpose.2591), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2594 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2593), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p120.2588 = bf16[768]{0} parameter(120), frontend_attributes={neff_input_names="input120"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2595 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p120.2588), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2596 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2594, bf16[4,128,768]{2,1,0} %broadcast.2595), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2599 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2596), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2601 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2599), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2603 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2601), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.2624 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.2623, bf16[48,64,128]{2,1,0} %reshape.2603), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1001 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.45 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.2624, bf16[48,128,128]{2,1,0} %broadcast.1001), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3031 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.45), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.2630 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.2631 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3031, bf16[4,12,128,128]{3,2,1,0} %broadcast.2630), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.2632 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2637 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.2631, bf16[] %constant.2632), dimensions={3}, to_apply=%MaxComputation.2633, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2638 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2637), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.2639 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.2631, bf16[4,12,128,128]{3,2,1,0} %broadcast.2638), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.2640 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.2639), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.2641 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2646 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.2640, bf16[] %constant.2641), dimensions={3}, to_apply=%AddComputation.2642, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2647 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2646), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.2648 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.2640, bf16[4,12,128,128]{3,2,1,0} %broadcast.2647), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.80 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.81 = s64[] multiply(s64[] %constant.80, s64[] %add.79), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.82 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.83 = s64[] add(s64[] %multiply.81, s64[] %constant.82), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2550 = u64[] convert(s64[] %add.83), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2554 = u64[1]{0} reshape(u64[] %convert.2550), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.299 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2556 = u64[2]{0} concatenate(u64[1]{0} %reshape.2554, u64[1]{0} %constant.299), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2557 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2556), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2558 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.2557), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2560 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2561 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.2560), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2562 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.2558, u32[4,12,128,128]{3,2,1,0} %broadcast.2561), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2563 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.2562), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.304 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.75 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.304), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2569 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.2563, f32[4,12,128,128]{3,2,1,0} %broadcast.75), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2572 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.2569), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2547 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2573 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.2572, bf16[4,12,128,128]{3,2,1,0} %broadcast.2547), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.49 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.18 = bf16[] divide(bf16[] %constant.49, bf16[] %p7.14)
  %broadcast.76 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.18), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.137 = bf16[] constant(0)
  %broadcast.161 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.137), dimensions={}
  %select.26 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.2573, bf16[4,12,128,128]{3,2,1,0} %broadcast.76, bf16[4,12,128,128]{3,2,1,0} %broadcast.161), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2649 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2648, bf16[4,12,128,128]{3,2,1,0} %select.26), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2651 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2649), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2533 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p119.2531 = bf16[768,768]{1,0} parameter(119), frontend_attributes={neff_input_names="input119"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2532 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p119.2531), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2534 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2533, bf16[768,768]{0,1} %transpose.2532), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2535 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2534), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p118.2529 = bf16[768]{0} parameter(118), frontend_attributes={neff_input_names="input118"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2536 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p118.2529), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2537 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2535, bf16[4,128,768]{2,1,0} %broadcast.2536), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2540 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2537), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.2541 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2540), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2543 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2541), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.2652 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.2651, bf16[48,128,64]{2,1,0} %reshape.2543), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2653 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2652), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2654 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.2653), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2656 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.2654), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p117.2522 = bf16[768,768]{1,0} parameter(117), frontend_attributes={neff_input_names="input117"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2523 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p117.2522), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2657 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2656, bf16[768,768]{0,1} %transpose.2523), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2658 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2657), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p116.2520 = bf16[768]{0} parameter(116), frontend_attributes={neff_input_names="input116"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2659 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p116.2520), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2660 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2658, bf16[4,128,768]{2,1,0} %broadcast.2659), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.84 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.85 = s64[] multiply(s64[] %constant.84, s64[] %add.83), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.86 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.87 = s64[] add(s64[] %multiply.85, s64[] %constant.86), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2488 = u64[] convert(s64[] %add.87), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2492 = u64[1]{0} reshape(u64[] %convert.2488), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.305 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2494 = u64[2]{0} concatenate(u64[1]{0} %reshape.2492, u64[1]{0} %constant.305), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2495 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2494), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2496 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2495), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2498 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2499 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2498), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2500 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2496, u32[4,128,768]{2,1,0} %broadcast.2499), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2501 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2500), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.310 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.77 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.310), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2507 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2501, f32[4,128,768]{2,1,0} %broadcast.77), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2510 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2507), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2485 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2511 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2510, bf16[4,128,768]{2,1,0} %broadcast.2485), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.51 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.19 = bf16[] divide(bf16[] %constant.51, bf16[] %p7.14)
  %broadcast.78 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.19), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.135 = bf16[] constant(0)
  %broadcast.160 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.135), dimensions={}
  %select.25 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2511, bf16[4,128,768]{2,1,0} %broadcast.78, bf16[4,128,768]{2,1,0} %broadcast.160), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2663 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2660, bf16[4,128,768]{2,1,0} %select.25), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2664 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2663, bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.2665 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2664), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.7649 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.7653 = bf16[512]{0} broadcast(bf16[] %constant.7649), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.572 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.576 = bf16[512]{0} broadcast(bf16[] %constant.572), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.567 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.571 = bf16[512]{0} broadcast(bf16[] %constant.567), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2666 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2665, bf16[512]{0} %broadcast.576, bf16[512]{0} %broadcast.571), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2668 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2666), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2669 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2666), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2670 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2671 = bf16[512]{0} broadcast(bf16[] %constant.2670), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2672 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2669, bf16[512]{0} %broadcast.2671), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.11 = bf16[512]{0} sqrt(bf16[512]{0} %add.2672), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.7667 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.11, bf16[512]{0} %sqrt.11), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.311 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.79 = bf16[512]{0} broadcast(bf16[] %constant.311), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.37 = bf16[512]{0} add(bf16[512]{0} %multiply.7667, bf16[512]{0} %broadcast.79), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p124.2675 = bf16[768]{0} parameter(124), frontend_attributes={neff_input_names="input124"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2681 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p124.2675), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2667 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2666), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2674 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2667), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p24.556 = bf16[768]{0} parameter(24), frontend_attributes={neff_input_names="input24"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2677 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p24.556), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2680 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2674, bf16[4,128,768]{2,1,0} %broadcast.2677), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2682 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2681, bf16[4,128,768]{2,1,0} %multiply.2680), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2735 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2682), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p128.2733 = bf16[3072,768]{1,0} parameter(128), frontend_attributes={neff_input_names="input128"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2734 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p128.2733), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2736 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.2735, bf16[768,3072]{0,1} %transpose.2734), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2737 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.2736), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p127.2731 = bf16[3072]{0} parameter(127), frontend_attributes={neff_input_names="input127"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2738 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p127.2731), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2739 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.2737, bf16[4,128,3072]{2,1,0} %broadcast.2738), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.38 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2739), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.2749 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.38), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p126.2724 = bf16[768,3072]{1,0} parameter(126), frontend_attributes={neff_input_names="input126"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2725 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p126.2724), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2750 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.2749, bf16[3072,768]{0,1} %transpose.2725), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2751 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2750), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p125.2722 = bf16[768]{0} parameter(125), frontend_attributes={neff_input_names="input125"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2752 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p125.2722), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2753 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2751, bf16[4,128,768]{2,1,0} %broadcast.2752), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.88 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.89 = s64[] multiply(s64[] %constant.88, s64[] %add.87), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.90 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.91 = s64[] add(s64[] %multiply.89, s64[] %constant.90), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2690 = u64[] convert(s64[] %add.91), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2694 = u64[1]{0} reshape(u64[] %convert.2690), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.315 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2696 = u64[2]{0} concatenate(u64[1]{0} %reshape.2694, u64[1]{0} %constant.315), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2697 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2696), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2698 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2697), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2700 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2701 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2700), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2702 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2698, u32[4,128,768]{2,1,0} %broadcast.2701), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2703 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2702), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.320 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.80 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.320), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2709 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2703, f32[4,128,768]{2,1,0} %broadcast.80), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2712 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2709), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2687 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2713 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2712, bf16[4,128,768]{2,1,0} %broadcast.2687), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.53 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.20 = bf16[] divide(bf16[] %constant.53, bf16[] %p7.14)
  %broadcast.81 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.20), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.133 = bf16[] constant(0)
  %broadcast.159 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.133), dimensions={}
  %select.24 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2713, bf16[4,128,768]{2,1,0} %broadcast.81, bf16[4,128,768]{2,1,0} %broadcast.159), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2756 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2753, bf16[4,128,768]{2,1,0} %select.24), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2757 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2756, bf16[4,128,768]{2,1,0} %add.2682), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.2758 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2757), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.7490 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.7494 = bf16[512]{0} broadcast(bf16[] %constant.7490), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.545 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.549 = bf16[512]{0} broadcast(bf16[] %constant.545), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.540 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.544 = bf16[512]{0} broadcast(bf16[] %constant.540), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2759 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2758, bf16[512]{0} %broadcast.549, bf16[512]{0} %broadcast.544), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2761 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2759), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2762 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2759), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2763 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2764 = bf16[512]{0} broadcast(bf16[] %constant.2763), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2765 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2762, bf16[512]{0} %broadcast.2764), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.12 = bf16[512]{0} sqrt(bf16[512]{0} %add.2765), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.7508 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.12, bf16[512]{0} %sqrt.12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.321 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.82 = bf16[512]{0} broadcast(bf16[] %constant.321), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.40 = bf16[512]{0} add(bf16[512]{0} %multiply.7508, bf16[512]{0} %broadcast.82), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p129.2768 = bf16[768]{0} parameter(129), frontend_attributes={neff_input_names="input129"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2774 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p129.2768), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2760 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2759), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2767 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2760), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p23.529 = bf16[768]{0} parameter(23), frontend_attributes={neff_input_names="input23"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2770 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p23.529), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2773 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2767, bf16[4,128,768]{2,1,0} %broadcast.2770), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2775 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2774, bf16[4,128,768]{2,1,0} %multiply.2773), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2908 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p137.2906 = bf16[768,768]{1,0} parameter(137), frontend_attributes={neff_input_names="input137"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2907 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p137.2906), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2909 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2908, bf16[768,768]{0,1} %transpose.2907), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2910 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2909), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p136.2904 = bf16[768]{0} parameter(136), frontend_attributes={neff_input_names="input136"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2911 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p136.2904), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2912 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2910, bf16[4,128,768]{2,1,0} %broadcast.2911), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2915 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2912), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2916 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2915), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2918 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2916), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2887 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p135.2885 = bf16[768,768]{1,0} parameter(135), frontend_attributes={neff_input_names="input135"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2886 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p135.2885), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2888 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2887, bf16[768,768]{0,1} %transpose.2886), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2889 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2888), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p134.2883 = bf16[768]{0} parameter(134), frontend_attributes={neff_input_names="input134"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2890 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p134.2883), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2891 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2889, bf16[4,128,768]{2,1,0} %broadcast.2890), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2894 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2891), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.2896 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2894), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.2898 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2896), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.2919 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.2918, bf16[48,64,128]{2,1,0} %reshape.2898), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1004 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.46 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.2919, bf16[48,128,128]{2,1,0} %broadcast.1004), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3037 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.46), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.2925 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.2926 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3037, bf16[4,12,128,128]{3,2,1,0} %broadcast.2925), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.2927 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2932 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.2926, bf16[] %constant.2927), dimensions={3}, to_apply=%MaxComputation.2928, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2933 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2932), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.2934 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.2926, bf16[4,12,128,128]{3,2,1,0} %broadcast.2933), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.2935 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.2934), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.2936 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.2941 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.2935, bf16[] %constant.2936), dimensions={3}, to_apply=%AddComputation.2937, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.2942 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.2941), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.2943 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.2935, bf16[4,12,128,128]{3,2,1,0} %broadcast.2942), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.92 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.93 = s64[] multiply(s64[] %constant.92, s64[] %add.91), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.94 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.95 = s64[] add(s64[] %multiply.93, s64[] %constant.94), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2845 = u64[] convert(s64[] %add.95), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2849 = u64[1]{0} reshape(u64[] %convert.2845), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.323 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2851 = u64[2]{0} concatenate(u64[1]{0} %reshape.2849, u64[1]{0} %constant.323), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2852 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2851), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2853 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.2852), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2855 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2856 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.2855), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2857 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.2853, u32[4,12,128,128]{3,2,1,0} %broadcast.2856), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2858 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.2857), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.328 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.84 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.328), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2864 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.2858, f32[4,12,128,128]{3,2,1,0} %broadcast.84), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2867 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.2864), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2842 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2868 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.2867, bf16[4,12,128,128]{3,2,1,0} %broadcast.2842), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.55 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.21 = bf16[] divide(bf16[] %constant.55, bf16[] %p7.14)
  %broadcast.85 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.21), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.131 = bf16[] constant(0)
  %broadcast.158 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.131), dimensions={}
  %select.23 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.2868, bf16[4,12,128,128]{3,2,1,0} %broadcast.85, bf16[4,12,128,128]{3,2,1,0} %broadcast.158), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2944 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2943, bf16[4,12,128,128]{3,2,1,0} %select.23), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2946 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2944), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2828 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p133.2826 = bf16[768,768]{1,0} parameter(133), frontend_attributes={neff_input_names="input133"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2827 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p133.2826), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2829 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2828, bf16[768,768]{0,1} %transpose.2827), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2830 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2829), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p132.2824 = bf16[768]{0} parameter(132), frontend_attributes={neff_input_names="input132"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2831 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p132.2824), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2832 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2830, bf16[4,128,768]{2,1,0} %broadcast.2831), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2835 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2832), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.2836 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.2835), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2838 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2836), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.2947 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.2946, bf16[48,128,64]{2,1,0} %reshape.2838), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.2948 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2947), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2949 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.2948), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2951 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.2949), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p131.2817 = bf16[768,768]{1,0} parameter(131), frontend_attributes={neff_input_names="input131"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.2818 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p131.2817), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.2952 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.2951, bf16[768,768]{0,1} %transpose.2818), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.2953 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.2952), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p130.2815 = bf16[768]{0} parameter(130), frontend_attributes={neff_input_names="input130"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.2954 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p130.2815), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.2955 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.2953, bf16[4,128,768]{2,1,0} %broadcast.2954), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.96 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.97 = s64[] multiply(s64[] %constant.96, s64[] %add.95), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.98 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.99 = s64[] add(s64[] %multiply.97, s64[] %constant.98), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2783 = u64[] convert(s64[] %add.99), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2787 = u64[1]{0} reshape(u64[] %convert.2783), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.330 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2789 = u64[2]{0} concatenate(u64[1]{0} %reshape.2787, u64[1]{0} %constant.330), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2790 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2789), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2791 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2790), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2793 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2794 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2793), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2795 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2791, u32[4,128,768]{2,1,0} %broadcast.2794), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2796 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2795), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.335 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.86 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.335), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2802 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2796, f32[4,128,768]{2,1,0} %broadcast.86), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2805 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.2802), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2780 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.2806 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.2805, bf16[4,128,768]{2,1,0} %broadcast.2780), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.57 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.22 = bf16[] divide(bf16[] %constant.57, bf16[] %p7.14)
  %broadcast.87 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.22), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.129 = bf16[] constant(0)
  %broadcast.157 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.129), dimensions={}
  %select.22 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.2806, bf16[4,128,768]{2,1,0} %broadcast.87, bf16[4,128,768]{2,1,0} %broadcast.157), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.2958 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.2955, bf16[4,128,768]{2,1,0} %select.22), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.2959 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.2958, bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.2960 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.2959), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.7202 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.7206 = bf16[512]{0} broadcast(bf16[] %constant.7202), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.518 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.522 = bf16[512]{0} broadcast(bf16[] %constant.518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.513 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.517 = bf16[512]{0} broadcast(bf16[] %constant.513), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.2961 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.2960, bf16[512]{0} %broadcast.522, bf16[512]{0} %broadcast.517), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2963 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2961), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2964 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2961), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.2965 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2966 = bf16[512]{0} broadcast(bf16[] %constant.2965), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2967 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.2964, bf16[512]{0} %broadcast.2966), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.13 = bf16[512]{0} sqrt(bf16[512]{0} %add.2967), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.7220 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.13, bf16[512]{0} %sqrt.13), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.336 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.88 = bf16[512]{0} broadcast(bf16[] %constant.336), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.44 = bf16[512]{0} add(bf16[512]{0} %multiply.7220, bf16[512]{0} %broadcast.88), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p138.2970 = bf16[768]{0} parameter(138), frontend_attributes={neff_input_names="input138"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2976 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p138.2970), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.2962 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.2961), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.2969 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.2962), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p22.502 = bf16[768]{0} parameter(22), frontend_attributes={neff_input_names="input22"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.2972 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p22.502), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.2975 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.2969, bf16[4,128,768]{2,1,0} %broadcast.2972), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.2977 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.2976, bf16[4,128,768]{2,1,0} %multiply.2975), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3030 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2977), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p142.3028 = bf16[3072,768]{1,0} parameter(142), frontend_attributes={neff_input_names="input142"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3029 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p142.3028), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3031 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.3030, bf16[768,3072]{0,1} %transpose.3029), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3032 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.3031), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p141.3026 = bf16[3072]{0} parameter(141), frontend_attributes={neff_input_names="input141"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3033 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p141.3026), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3034 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.3032, bf16[4,128,3072]{2,1,0} %broadcast.3033), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.39 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3034), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.3044 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.39), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p140.3019 = bf16[768,3072]{1,0} parameter(140), frontend_attributes={neff_input_names="input140"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3020 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p140.3019), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3045 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.3044, bf16[3072,768]{0,1} %transpose.3020), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3046 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3045), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p139.3017 = bf16[768]{0} parameter(139), frontend_attributes={neff_input_names="input139"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3047 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p139.3017), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3048 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3046, bf16[4,128,768]{2,1,0} %broadcast.3047), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.100 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.101 = s64[] multiply(s64[] %constant.100, s64[] %add.99), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.102 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.103 = s64[] add(s64[] %multiply.101, s64[] %constant.102), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2985 = u64[] convert(s64[] %add.103), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.2989 = u64[1]{0} reshape(u64[] %convert.2985), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.338 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.2991 = u64[2]{0} concatenate(u64[1]{0} %reshape.2989, u64[1]{0} %constant.338), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.2992 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.2991), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.2993 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.2992), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.2995 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2996 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.2995), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.2997 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.2993, u32[4,128,768]{2,1,0} %broadcast.2996), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.2998 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.2997), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.344 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.89 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.344), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3004 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.2998, f32[4,128,768]{2,1,0} %broadcast.89), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3007 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3004), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.2982 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3008 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3007, bf16[4,128,768]{2,1,0} %broadcast.2982), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.59 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.23 = bf16[] divide(bf16[] %constant.59, bf16[] %p7.14)
  %broadcast.90 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.23), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.127 = bf16[] constant(0)
  %broadcast.156 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.127), dimensions={}
  %select.21 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3008, bf16[4,128,768]{2,1,0} %broadcast.90, bf16[4,128,768]{2,1,0} %broadcast.156), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3051 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3048, bf16[4,128,768]{2,1,0} %select.21), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3052 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3051, bf16[4,128,768]{2,1,0} %add.2977), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.3053 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3052), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.7043 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.7047 = bf16[512]{0} broadcast(bf16[] %constant.7043), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.491 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.495 = bf16[512]{0} broadcast(bf16[] %constant.491), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.486 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.490 = bf16[512]{0} broadcast(bf16[] %constant.486), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3054 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3053, bf16[512]{0} %broadcast.495, bf16[512]{0} %broadcast.490), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3056 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3054), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3057 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3054), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3058 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3059 = bf16[512]{0} broadcast(bf16[] %constant.3058), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3060 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3057, bf16[512]{0} %broadcast.3059), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.14 = bf16[512]{0} sqrt(bf16[512]{0} %add.3060), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.7061 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.14, bf16[512]{0} %sqrt.14), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.345 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.91 = bf16[512]{0} broadcast(bf16[] %constant.345), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.46 = bf16[512]{0} add(bf16[512]{0} %multiply.7061, bf16[512]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p143.3063 = bf16[768]{0} parameter(143), frontend_attributes={neff_input_names="input143"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3069 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p143.3063), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3055 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3054), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3062 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3055), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p21.475 = bf16[768]{0} parameter(21), frontend_attributes={neff_input_names="input21"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3065 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p21.475), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3068 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3062, bf16[4,128,768]{2,1,0} %broadcast.3065), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3070 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3069, bf16[4,128,768]{2,1,0} %multiply.3068), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3203 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p151.3201 = bf16[768,768]{1,0} parameter(151), frontend_attributes={neff_input_names="input151"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3202 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p151.3201), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3204 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3203, bf16[768,768]{0,1} %transpose.3202), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3205 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3204), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p150.3199 = bf16[768]{0} parameter(150), frontend_attributes={neff_input_names="input150"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3206 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p150.3199), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3207 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3205, bf16[4,128,768]{2,1,0} %broadcast.3206), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3210 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3207), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3211 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3210), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3213 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3211), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3182 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p149.3180 = bf16[768,768]{1,0} parameter(149), frontend_attributes={neff_input_names="input149"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3181 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p149.3180), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3183 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3182, bf16[768,768]{0,1} %transpose.3181), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3184 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3183), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p148.3178 = bf16[768]{0} parameter(148), frontend_attributes={neff_input_names="input148"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3185 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p148.3178), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3186 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3184, bf16[4,128,768]{2,1,0} %broadcast.3185), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3189 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3186), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3191 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3189), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3193 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3191), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.3214 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.3213, bf16[48,64,128]{2,1,0} %reshape.3193), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1007 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.47 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.3214, bf16[48,128,128]{2,1,0} %broadcast.1007), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3040 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.47), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.3220 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.3221 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3040, bf16[4,12,128,128]{3,2,1,0} %broadcast.3220), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.3222 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3227 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.3221, bf16[] %constant.3222), dimensions={3}, to_apply=%MaxComputation.3223, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3228 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3227), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.3229 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.3221, bf16[4,12,128,128]{3,2,1,0} %broadcast.3228), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.3230 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.3229), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.3231 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3236 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.3230, bf16[] %constant.3231), dimensions={3}, to_apply=%AddComputation.3232, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3237 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3236), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.3238 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.3230, bf16[4,12,128,128]{3,2,1,0} %broadcast.3237), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.104 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.105 = s64[] multiply(s64[] %constant.104, s64[] %add.103), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.106 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.107 = s64[] add(s64[] %multiply.105, s64[] %constant.106), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3140 = u64[] convert(s64[] %add.107), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3144 = u64[1]{0} reshape(u64[] %convert.3140), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.348 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3146 = u64[2]{0} concatenate(u64[1]{0} %reshape.3144, u64[1]{0} %constant.348), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3147 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3146), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3148 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.3147), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3150 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3151 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.3150), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3152 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.3148, u32[4,12,128,128]{3,2,1,0} %broadcast.3151), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3153 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.3152), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.353 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.93 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.353), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3159 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.3153, f32[4,12,128,128]{3,2,1,0} %broadcast.93), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3162 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.3159), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3137 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3163 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.3162, bf16[4,12,128,128]{3,2,1,0} %broadcast.3137), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.61 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.24 = bf16[] divide(bf16[] %constant.61, bf16[] %p7.14)
  %broadcast.94 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.24), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.125 = bf16[] constant(0)
  %broadcast.155 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.125), dimensions={}
  %select.20 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.3163, bf16[4,12,128,128]{3,2,1,0} %broadcast.94, bf16[4,12,128,128]{3,2,1,0} %broadcast.155), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3239 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3238, bf16[4,12,128,128]{3,2,1,0} %select.20), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3241 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3239), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3123 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p147.3121 = bf16[768,768]{1,0} parameter(147), frontend_attributes={neff_input_names="input147"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3122 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p147.3121), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3124 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3123, bf16[768,768]{0,1} %transpose.3122), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3125 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3124), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p146.3119 = bf16[768]{0} parameter(146), frontend_attributes={neff_input_names="input146"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3126 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p146.3119), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3127 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3125, bf16[4,128,768]{2,1,0} %broadcast.3126), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3130 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3127), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.3131 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3130), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3133 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3131), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.3242 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.3241, bf16[48,128,64]{2,1,0} %reshape.3133), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3243 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3242), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3244 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.3243), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3246 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.3244), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p145.3112 = bf16[768,768]{1,0} parameter(145), frontend_attributes={neff_input_names="input145"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3113 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p145.3112), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3247 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3246, bf16[768,768]{0,1} %transpose.3113), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3248 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3247), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p144.3110 = bf16[768]{0} parameter(144), frontend_attributes={neff_input_names="input144"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3249 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p144.3110), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3250 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3248, bf16[4,128,768]{2,1,0} %broadcast.3249), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.108 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.109 = s64[] multiply(s64[] %constant.108, s64[] %add.107), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.110 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.111 = s64[] add(s64[] %multiply.109, s64[] %constant.110), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3078 = u64[] convert(s64[] %add.111), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3082 = u64[1]{0} reshape(u64[] %convert.3078), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.354 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3084 = u64[2]{0} concatenate(u64[1]{0} %reshape.3082, u64[1]{0} %constant.354), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3085 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3084), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3086 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3085), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3088 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3089 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3088), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3090 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3086, u32[4,128,768]{2,1,0} %broadcast.3089), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3091 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3090), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.359 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.95 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.359), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3097 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3091, f32[4,128,768]{2,1,0} %broadcast.95), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3100 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3097), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3075 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3101 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3100, bf16[4,128,768]{2,1,0} %broadcast.3075), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.63 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.25 = bf16[] divide(bf16[] %constant.63, bf16[] %p7.14)
  %broadcast.96 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.25), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.123 = bf16[] constant(0)
  %broadcast.154 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.123), dimensions={}
  %select.19 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3101, bf16[4,128,768]{2,1,0} %broadcast.96, bf16[4,128,768]{2,1,0} %broadcast.154), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3253 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3250, bf16[4,128,768]{2,1,0} %select.19), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3254 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3253, bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.3255 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3254), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.6755 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.6759 = bf16[512]{0} broadcast(bf16[] %constant.6755), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.464 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.468 = bf16[512]{0} broadcast(bf16[] %constant.464), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.459 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.463 = bf16[512]{0} broadcast(bf16[] %constant.459), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3256 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3255, bf16[512]{0} %broadcast.468, bf16[512]{0} %broadcast.463), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3258 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3256), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3259 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3256), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3260 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3261 = bf16[512]{0} broadcast(bf16[] %constant.3260), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3262 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3259, bf16[512]{0} %broadcast.3261), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.15 = bf16[512]{0} sqrt(bf16[512]{0} %add.3262), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.6773 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.15, bf16[512]{0} %sqrt.15), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.360 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.97 = bf16[512]{0} broadcast(bf16[] %constant.360), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.50 = bf16[512]{0} add(bf16[512]{0} %multiply.6773, bf16[512]{0} %broadcast.97), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p152.3265 = bf16[768]{0} parameter(152), frontend_attributes={neff_input_names="input152"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3271 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p152.3265), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3257 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3256), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3264 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3257), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p20.448 = bf16[768]{0} parameter(20), frontend_attributes={neff_input_names="input20"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3267 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p20.448), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3270 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3264, bf16[4,128,768]{2,1,0} %broadcast.3267), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3272 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3271, bf16[4,128,768]{2,1,0} %multiply.3270), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3325 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3272), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p156.3323 = bf16[3072,768]{1,0} parameter(156), frontend_attributes={neff_input_names="input156"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3324 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p156.3323), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3326 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.3325, bf16[768,3072]{0,1} %transpose.3324), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3327 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.3326), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p155.3321 = bf16[3072]{0} parameter(155), frontend_attributes={neff_input_names="input155"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3328 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p155.3321), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3329 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.3327, bf16[4,128,3072]{2,1,0} %broadcast.3328), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.40 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3329), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.3339 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.40), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p154.3314 = bf16[768,3072]{1,0} parameter(154), frontend_attributes={neff_input_names="input154"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3315 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p154.3314), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3340 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.3339, bf16[3072,768]{0,1} %transpose.3315), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3341 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3340), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p153.3312 = bf16[768]{0} parameter(153), frontend_attributes={neff_input_names="input153"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3342 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p153.3312), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3343 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3341, bf16[4,128,768]{2,1,0} %broadcast.3342), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.112 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.113 = s64[] multiply(s64[] %constant.112, s64[] %add.111), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.114 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.115 = s64[] add(s64[] %multiply.113, s64[] %constant.114), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3280 = u64[] convert(s64[] %add.115), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3284 = u64[1]{0} reshape(u64[] %convert.3280), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.363 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3286 = u64[2]{0} concatenate(u64[1]{0} %reshape.3284, u64[1]{0} %constant.363), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3287 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3286), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3288 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3287), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3290 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3291 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3290), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3292 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3288, u32[4,128,768]{2,1,0} %broadcast.3291), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3293 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3292), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.369 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.98 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.369), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3299 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3293, f32[4,128,768]{2,1,0} %broadcast.98), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3302 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3299), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3277 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3303 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3302, bf16[4,128,768]{2,1,0} %broadcast.3277), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.65 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.26 = bf16[] divide(bf16[] %constant.65, bf16[] %p7.14)
  %broadcast.99 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.26), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.121 = bf16[] constant(0)
  %broadcast.153 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.121), dimensions={}
  %select.18 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3303, bf16[4,128,768]{2,1,0} %broadcast.99, bf16[4,128,768]{2,1,0} %broadcast.153), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3346 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3343, bf16[4,128,768]{2,1,0} %select.18), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3347 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3346, bf16[4,128,768]{2,1,0} %add.3272), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.3348 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3347), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.6596 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.6600 = bf16[512]{0} broadcast(bf16[] %constant.6596), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.437 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.441 = bf16[512]{0} broadcast(bf16[] %constant.437), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.432 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.436 = bf16[512]{0} broadcast(bf16[] %constant.432), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3349 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3348, bf16[512]{0} %broadcast.441, bf16[512]{0} %broadcast.436), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3351 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3349), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3352 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3349), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3353 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3354 = bf16[512]{0} broadcast(bf16[] %constant.3353), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3355 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3352, bf16[512]{0} %broadcast.3354), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.16 = bf16[512]{0} sqrt(bf16[512]{0} %add.3355), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.6614 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.16, bf16[512]{0} %sqrt.16), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.370 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.100 = bf16[512]{0} broadcast(bf16[] %constant.370), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.53 = bf16[512]{0} add(bf16[512]{0} %multiply.6614, bf16[512]{0} %broadcast.100), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p157.3358 = bf16[768]{0} parameter(157), frontend_attributes={neff_input_names="input157"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3364 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p157.3358), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3350 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3349), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3357 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3350), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p19.421 = bf16[768]{0} parameter(19), frontend_attributes={neff_input_names="input19"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3360 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p19.421), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3363 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3357, bf16[4,128,768]{2,1,0} %broadcast.3360), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3365 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3364, bf16[4,128,768]{2,1,0} %multiply.3363), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3498 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p165.3496 = bf16[768,768]{1,0} parameter(165), frontend_attributes={neff_input_names="input165"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3497 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p165.3496), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3499 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3498, bf16[768,768]{0,1} %transpose.3497), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3500 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3499), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p164.3494 = bf16[768]{0} parameter(164), frontend_attributes={neff_input_names="input164"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3501 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p164.3494), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3502 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3500, bf16[4,128,768]{2,1,0} %broadcast.3501), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3505 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3502), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3506 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3505), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3508 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3506), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3477 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p163.3475 = bf16[768,768]{1,0} parameter(163), frontend_attributes={neff_input_names="input163"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3476 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p163.3475), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3478 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3477, bf16[768,768]{0,1} %transpose.3476), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3479 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3478), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p162.3473 = bf16[768]{0} parameter(162), frontend_attributes={neff_input_names="input162"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3480 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p162.3473), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3481 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3479, bf16[4,128,768]{2,1,0} %broadcast.3480), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3484 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3481), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3486 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3484), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3488 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3486), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.3509 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.3508, bf16[48,64,128]{2,1,0} %reshape.3488), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1010 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.48 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.3509, bf16[48,128,128]{2,1,0} %broadcast.1010), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3043 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.48), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.3515 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.3516 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3043, bf16[4,12,128,128]{3,2,1,0} %broadcast.3515), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.3517 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3522 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.3516, bf16[] %constant.3517), dimensions={3}, to_apply=%MaxComputation.3518, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3523 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3522), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.3524 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.3516, bf16[4,12,128,128]{3,2,1,0} %broadcast.3523), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.3525 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.3524), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.3526 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3531 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.3525, bf16[] %constant.3526), dimensions={3}, to_apply=%AddComputation.3527, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3532 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3531), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.3533 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.3525, bf16[4,12,128,128]{3,2,1,0} %broadcast.3532), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.116 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.117 = s64[] multiply(s64[] %constant.116, s64[] %add.115), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.118 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.119 = s64[] add(s64[] %multiply.117, s64[] %constant.118), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3435 = u64[] convert(s64[] %add.119), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3439 = u64[1]{0} reshape(u64[] %convert.3435), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.372 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3441 = u64[2]{0} concatenate(u64[1]{0} %reshape.3439, u64[1]{0} %constant.372), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3442 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3441), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3443 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.3442), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3445 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3446 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.3445), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3447 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.3443, u32[4,12,128,128]{3,2,1,0} %broadcast.3446), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3448 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.3447), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.377 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.102 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.377), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3454 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.3448, f32[4,12,128,128]{3,2,1,0} %broadcast.102), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3457 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.3454), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3432 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3458 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.3457, bf16[4,12,128,128]{3,2,1,0} %broadcast.3432), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.67 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.27 = bf16[] divide(bf16[] %constant.67, bf16[] %p7.14)
  %broadcast.103 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.27), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.119 = bf16[] constant(0)
  %broadcast.152 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.119), dimensions={}
  %select.17 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.3458, bf16[4,12,128,128]{3,2,1,0} %broadcast.103, bf16[4,12,128,128]{3,2,1,0} %broadcast.152), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3534 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3533, bf16[4,12,128,128]{3,2,1,0} %select.17), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3536 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3534), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3418 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p161.3416 = bf16[768,768]{1,0} parameter(161), frontend_attributes={neff_input_names="input161"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3417 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p161.3416), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3419 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3418, bf16[768,768]{0,1} %transpose.3417), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3420 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3419), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p160.3414 = bf16[768]{0} parameter(160), frontend_attributes={neff_input_names="input160"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3421 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p160.3414), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3422 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3420, bf16[4,128,768]{2,1,0} %broadcast.3421), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3425 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3422), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.3426 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3425), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3428 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3426), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.3537 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.3536, bf16[48,128,64]{2,1,0} %reshape.3428), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3538 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3537), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3539 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.3538), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3541 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.3539), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p159.3407 = bf16[768,768]{1,0} parameter(159), frontend_attributes={neff_input_names="input159"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3408 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p159.3407), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3542 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3541, bf16[768,768]{0,1} %transpose.3408), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3543 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3542), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p158.3405 = bf16[768]{0} parameter(158), frontend_attributes={neff_input_names="input158"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3544 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p158.3405), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3545 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3543, bf16[4,128,768]{2,1,0} %broadcast.3544), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.120 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.121 = s64[] multiply(s64[] %constant.120, s64[] %add.119), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.122 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.123 = s64[] add(s64[] %multiply.121, s64[] %constant.122), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3373 = u64[] convert(s64[] %add.123), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3377 = u64[1]{0} reshape(u64[] %convert.3373), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.379 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3379 = u64[2]{0} concatenate(u64[1]{0} %reshape.3377, u64[1]{0} %constant.379), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3380 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3379), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3381 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3380), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3383 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3384 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3383), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3385 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3381, u32[4,128,768]{2,1,0} %broadcast.3384), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3386 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3385), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.384 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.104 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.384), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3392 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3386, f32[4,128,768]{2,1,0} %broadcast.104), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3395 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3392), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3370 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3396 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3395, bf16[4,128,768]{2,1,0} %broadcast.3370), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.69 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.28 = bf16[] divide(bf16[] %constant.69, bf16[] %p7.14)
  %broadcast.105 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.28), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.117 = bf16[] constant(0)
  %broadcast.151 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.117), dimensions={}
  %select.16 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3396, bf16[4,128,768]{2,1,0} %broadcast.105, bf16[4,128,768]{2,1,0} %broadcast.151), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3548 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3545, bf16[4,128,768]{2,1,0} %select.16), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3549 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3548, bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.3550 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3549), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.6308 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.6312 = bf16[512]{0} broadcast(bf16[] %constant.6308), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.410 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.414 = bf16[512]{0} broadcast(bf16[] %constant.410), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.405 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.409 = bf16[512]{0} broadcast(bf16[] %constant.405), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3551 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3550, bf16[512]{0} %broadcast.414, bf16[512]{0} %broadcast.409), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3553 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3551), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3554 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3551), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3555 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3556 = bf16[512]{0} broadcast(bf16[] %constant.3555), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3557 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3554, bf16[512]{0} %broadcast.3556), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.17 = bf16[512]{0} sqrt(bf16[512]{0} %add.3557), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.6326 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.17, bf16[512]{0} %sqrt.17), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.385 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.106 = bf16[512]{0} broadcast(bf16[] %constant.385), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.57 = bf16[512]{0} add(bf16[512]{0} %multiply.6326, bf16[512]{0} %broadcast.106), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p166.3560 = bf16[768]{0} parameter(166), frontend_attributes={neff_input_names="input166"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3566 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p166.3560), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3552 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3551), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3559 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3552), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p18.394 = bf16[768]{0} parameter(18), frontend_attributes={neff_input_names="input18"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3562 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p18.394), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3565 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3559, bf16[4,128,768]{2,1,0} %broadcast.3562), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3567 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3566, bf16[4,128,768]{2,1,0} %multiply.3565), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3620 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3567), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p170.3618 = bf16[3072,768]{1,0} parameter(170), frontend_attributes={neff_input_names="input170"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3619 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p170.3618), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3621 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.3620, bf16[768,3072]{0,1} %transpose.3619), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3622 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.3621), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p169.3616 = bf16[3072]{0} parameter(169), frontend_attributes={neff_input_names="input169"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3623 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p169.3616), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3624 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.3622, bf16[4,128,3072]{2,1,0} %broadcast.3623), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.41 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3624), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.3634 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.41), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p168.3609 = bf16[768,3072]{1,0} parameter(168), frontend_attributes={neff_input_names="input168"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3610 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p168.3609), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3635 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.3634, bf16[3072,768]{0,1} %transpose.3610), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3636 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3635), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p167.3607 = bf16[768]{0} parameter(167), frontend_attributes={neff_input_names="input167"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3637 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p167.3607), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3638 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3636, bf16[4,128,768]{2,1,0} %broadcast.3637), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.124 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.125 = s64[] multiply(s64[] %constant.124, s64[] %add.123), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.126 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.127 = s64[] add(s64[] %multiply.125, s64[] %constant.126), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3575 = u64[] convert(s64[] %add.127), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3579 = u64[1]{0} reshape(u64[] %convert.3575), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.387 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3581 = u64[2]{0} concatenate(u64[1]{0} %reshape.3579, u64[1]{0} %constant.387), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3582 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3581), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3583 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3582), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3585 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3586 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3585), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3587 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3583, u32[4,128,768]{2,1,0} %broadcast.3586), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3588 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3587), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.392 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.107 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.392), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3594 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3588, f32[4,128,768]{2,1,0} %broadcast.107), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3597 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3594), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3572 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3598 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3597, bf16[4,128,768]{2,1,0} %broadcast.3572), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.71 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.29 = bf16[] divide(bf16[] %constant.71, bf16[] %p7.14)
  %broadcast.108 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.29), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.115 = bf16[] constant(0)
  %broadcast.150 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.115), dimensions={}
  %select.15 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3598, bf16[4,128,768]{2,1,0} %broadcast.108, bf16[4,128,768]{2,1,0} %broadcast.150), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3641 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3638, bf16[4,128,768]{2,1,0} %select.15), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3642 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3641, bf16[4,128,768]{2,1,0} %add.3567), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.3643 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3642), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.6149 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.6153 = bf16[512]{0} broadcast(bf16[] %constant.6149), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.383 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.387 = bf16[512]{0} broadcast(bf16[] %constant.383), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.378 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.382 = bf16[512]{0} broadcast(bf16[] %constant.378), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3644 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3643, bf16[512]{0} %broadcast.387, bf16[512]{0} %broadcast.382), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3646 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3644), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3647 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3644), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3648 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3649 = bf16[512]{0} broadcast(bf16[] %constant.3648), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3650 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3647, bf16[512]{0} %broadcast.3649), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.18 = bf16[512]{0} sqrt(bf16[512]{0} %add.3650), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.6167 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.18, bf16[512]{0} %sqrt.18), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.394 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.109 = bf16[512]{0} broadcast(bf16[] %constant.394), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.60 = bf16[512]{0} add(bf16[512]{0} %multiply.6167, bf16[512]{0} %broadcast.109), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p171.3653 = bf16[768]{0} parameter(171), frontend_attributes={neff_input_names="input171"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3659 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p171.3653), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3645 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3644), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3652 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3645), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p17.367 = bf16[768]{0} parameter(17), frontend_attributes={neff_input_names="input17"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3655 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p17.367), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3658 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3652, bf16[4,128,768]{2,1,0} %broadcast.3655), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3660 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3659, bf16[4,128,768]{2,1,0} %multiply.3658), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3793 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p179.3791 = bf16[768,768]{1,0} parameter(179), frontend_attributes={neff_input_names="input179"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3792 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p179.3791), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3794 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3793, bf16[768,768]{0,1} %transpose.3792), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3795 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3794), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p178.3789 = bf16[768]{0} parameter(178), frontend_attributes={neff_input_names="input178"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3796 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p178.3789), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3797 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3795, bf16[4,128,768]{2,1,0} %broadcast.3796), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3800 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3797), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3801 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3800), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3803 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3801), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3772 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p177.3770 = bf16[768,768]{1,0} parameter(177), frontend_attributes={neff_input_names="input177"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3771 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p177.3770), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3773 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3772, bf16[768,768]{0,1} %transpose.3771), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3774 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3773), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p176.3768 = bf16[768]{0} parameter(176), frontend_attributes={neff_input_names="input176"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3775 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p176.3768), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3776 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3774, bf16[4,128,768]{2,1,0} %broadcast.3775), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3779 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3776), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.3781 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3779), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.3783 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3781), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.3804 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.3803, bf16[48,64,128]{2,1,0} %reshape.3783), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1013 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.49 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.3804, bf16[48,128,128]{2,1,0} %broadcast.1013), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3048 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.49), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.3810 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.3811 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3048, bf16[4,12,128,128]{3,2,1,0} %broadcast.3810), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.3812 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3817 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.3811, bf16[] %constant.3812), dimensions={3}, to_apply=%MaxComputation.3813, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3818 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3817), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.3819 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.3811, bf16[4,12,128,128]{3,2,1,0} %broadcast.3818), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.3820 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.3819), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.3821 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.3826 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.3820, bf16[] %constant.3821), dimensions={3}, to_apply=%AddComputation.3822, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.3827 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.3826), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.3828 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.3820, bf16[4,12,128,128]{3,2,1,0} %broadcast.3827), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.128 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.129 = s64[] multiply(s64[] %constant.128, s64[] %add.127), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.130 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.131 = s64[] add(s64[] %multiply.129, s64[] %constant.130), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3730 = u64[] convert(s64[] %add.131), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3734 = u64[1]{0} reshape(u64[] %convert.3730), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.397 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3736 = u64[2]{0} concatenate(u64[1]{0} %reshape.3734, u64[1]{0} %constant.397), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3737 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3736), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3738 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.3737), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3740 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3741 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.3740), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3742 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.3738, u32[4,12,128,128]{3,2,1,0} %broadcast.3741), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3743 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.3742), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.402 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.111 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.402), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3749 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.3743, f32[4,12,128,128]{3,2,1,0} %broadcast.111), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3752 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.3749), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3727 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3753 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.3752, bf16[4,12,128,128]{3,2,1,0} %broadcast.3727), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.73 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.30 = bf16[] divide(bf16[] %constant.73, bf16[] %p7.14)
  %broadcast.112 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.30), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.113 = bf16[] constant(0)
  %broadcast.149 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.113), dimensions={}
  %select.14 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.3753, bf16[4,12,128,128]{3,2,1,0} %broadcast.112, bf16[4,12,128,128]{3,2,1,0} %broadcast.149), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3829 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3828, bf16[4,12,128,128]{3,2,1,0} %select.14), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3831 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3829), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3713 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p175.3711 = bf16[768,768]{1,0} parameter(175), frontend_attributes={neff_input_names="input175"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3712 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p175.3711), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3714 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3713, bf16[768,768]{0,1} %transpose.3712), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3715 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3714), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p174.3709 = bf16[768]{0} parameter(174), frontend_attributes={neff_input_names="input174"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3716 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p174.3709), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3717 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3715, bf16[4,128,768]{2,1,0} %broadcast.3716), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3720 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3717), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.3721 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.3720), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3723 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3721), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.3832 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.3831, bf16[48,128,64]{2,1,0} %reshape.3723), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.3833 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3832), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3834 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.3833), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3836 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.3834), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p173.3702 = bf16[768,768]{1,0} parameter(173), frontend_attributes={neff_input_names="input173"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3703 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p173.3702), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3837 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.3836, bf16[768,768]{0,1} %transpose.3703), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3838 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3837), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p172.3700 = bf16[768]{0} parameter(172), frontend_attributes={neff_input_names="input172"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3839 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p172.3700), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3840 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3838, bf16[4,128,768]{2,1,0} %broadcast.3839), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.132 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.133 = s64[] multiply(s64[] %constant.132, s64[] %add.131), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.134 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.135 = s64[] add(s64[] %multiply.133, s64[] %constant.134), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3668 = u64[] convert(s64[] %add.135), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3672 = u64[1]{0} reshape(u64[] %convert.3668), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.403 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3674 = u64[2]{0} concatenate(u64[1]{0} %reshape.3672, u64[1]{0} %constant.403), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3675 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3674), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3676 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3675), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3678 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3679 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3678), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3680 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3676, u32[4,128,768]{2,1,0} %broadcast.3679), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3681 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3680), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.408 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.113 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.408), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3687 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3681, f32[4,128,768]{2,1,0} %broadcast.113), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3690 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3687), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3665 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3691 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3690, bf16[4,128,768]{2,1,0} %broadcast.3665), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.75 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.31 = bf16[] divide(bf16[] %constant.75, bf16[] %p7.14)
  %broadcast.114 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.31), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.111 = bf16[] constant(0)
  %broadcast.148 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.111), dimensions={}
  %select.13 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3691, bf16[4,128,768]{2,1,0} %broadcast.114, bf16[4,128,768]{2,1,0} %broadcast.148), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3843 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3840, bf16[4,128,768]{2,1,0} %select.13), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3844 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3843, bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.3845 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3844), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.5861 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.5865 = bf16[512]{0} broadcast(bf16[] %constant.5861), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.356 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.360 = bf16[512]{0} broadcast(bf16[] %constant.356), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.351 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.355 = bf16[512]{0} broadcast(bf16[] %constant.351), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3846 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3845, bf16[512]{0} %broadcast.360, bf16[512]{0} %broadcast.355), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3848 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3846), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3849 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3846), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3850 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3851 = bf16[512]{0} broadcast(bf16[] %constant.3850), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3852 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3849, bf16[512]{0} %broadcast.3851), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.19 = bf16[512]{0} sqrt(bf16[512]{0} %add.3852), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.5879 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.19, bf16[512]{0} %sqrt.19), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.409 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.115 = bf16[512]{0} broadcast(bf16[] %constant.409), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.64 = bf16[512]{0} add(bf16[512]{0} %multiply.5879, bf16[512]{0} %broadcast.115), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p180.3855 = bf16[768]{0} parameter(180), frontend_attributes={neff_input_names="input180"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3861 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p180.3855), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3847 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3846), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3854 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3847), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p16.340 = bf16[768]{0} parameter(16), frontend_attributes={neff_input_names="input16"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3857 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p16.340), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3860 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3854, bf16[4,128,768]{2,1,0} %broadcast.3857), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3862 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3861, bf16[4,128,768]{2,1,0} %multiply.3860), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3915 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3862), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p184.3913 = bf16[3072,768]{1,0} parameter(184), frontend_attributes={neff_input_names="input184"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3914 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p184.3913), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3916 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.3915, bf16[768,3072]{0,1} %transpose.3914), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3917 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.3916), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p183.3911 = bf16[3072]{0} parameter(183), frontend_attributes={neff_input_names="input183"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3918 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p183.3911), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3919 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.3917, bf16[4,128,3072]{2,1,0} %broadcast.3918), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.42 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3919), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.3929 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.42), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p182.3904 = bf16[768,3072]{1,0} parameter(182), frontend_attributes={neff_input_names="input182"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3905 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p182.3904), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.3930 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.3929, bf16[3072,768]{0,1} %transpose.3905), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.3931 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.3930), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p181.3902 = bf16[768]{0} parameter(181), frontend_attributes={neff_input_names="input181"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.3932 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p181.3902), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.3933 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.3931, bf16[4,128,768]{2,1,0} %broadcast.3932), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.136 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.137 = s64[] multiply(s64[] %constant.136, s64[] %add.135), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.138 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.139 = s64[] add(s64[] %multiply.137, s64[] %constant.138), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3870 = u64[] convert(s64[] %add.139), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3874 = u64[1]{0} reshape(u64[] %convert.3870), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.412 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3876 = u64[2]{0} concatenate(u64[1]{0} %reshape.3874, u64[1]{0} %constant.412), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3877 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3876), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3878 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3877), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3880 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3881 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3880), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3882 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3878, u32[4,128,768]{2,1,0} %broadcast.3881), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3883 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3882), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.417 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.116 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.417), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3889 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3883, f32[4,128,768]{2,1,0} %broadcast.116), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3892 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3889), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3867 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3893 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3892, bf16[4,128,768]{2,1,0} %broadcast.3867), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.77 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.32 = bf16[] divide(bf16[] %constant.77, bf16[] %p7.14)
  %broadcast.117 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.32), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.109 = bf16[] constant(0)
  %broadcast.147 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.109), dimensions={}
  %select.12 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3893, bf16[4,128,768]{2,1,0} %broadcast.117, bf16[4,128,768]{2,1,0} %broadcast.147), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3936 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.3933, bf16[4,128,768]{2,1,0} %select.12), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.3937 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.3936, bf16[4,128,768]{2,1,0} %add.3862), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.3938 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.3937), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.5702 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.5706 = bf16[512]{0} broadcast(bf16[] %constant.5702), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.329 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.333 = bf16[512]{0} broadcast(bf16[] %constant.329), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.324 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.328 = bf16[512]{0} broadcast(bf16[] %constant.324), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.3939 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.3938, bf16[512]{0} %broadcast.333, bf16[512]{0} %broadcast.328), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3941 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3939), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3942 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3939), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.3943 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3944 = bf16[512]{0} broadcast(bf16[] %constant.3943), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3945 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.3942, bf16[512]{0} %broadcast.3944), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.20 = bf16[512]{0} sqrt(bf16[512]{0} %add.3945), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.5720 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.20, bf16[512]{0} %sqrt.20), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.418 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.118 = bf16[512]{0} broadcast(bf16[] %constant.418), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.66 = bf16[512]{0} add(bf16[512]{0} %multiply.5720, bf16[512]{0} %broadcast.118), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p185.3948 = bf16[768]{0} parameter(185), frontend_attributes={neff_input_names="input185"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3954 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p185.3948), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.3940 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.3939), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.3947 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.3940), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p15.313 = bf16[768]{0} parameter(15), frontend_attributes={neff_input_names="input15"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.3950 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p15.313), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.3953 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.3947, bf16[4,128,768]{2,1,0} %broadcast.3950), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.3955 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.3954, bf16[4,128,768]{2,1,0} %multiply.3953), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4088 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p193.4086 = bf16[768,768]{1,0} parameter(193), frontend_attributes={neff_input_names="input193"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4087 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p193.4086), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4089 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4088, bf16[768,768]{0,1} %transpose.4087), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4090 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4089), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p192.4084 = bf16[768]{0} parameter(192), frontend_attributes={neff_input_names="input192"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4091 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p192.4084), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4092 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4090, bf16[4,128,768]{2,1,0} %broadcast.4091), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4095 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4092), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.4096 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4095), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4098 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4096), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4067 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p191.4065 = bf16[768,768]{1,0} parameter(191), frontend_attributes={neff_input_names="input191"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4066 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p191.4065), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4068 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4067, bf16[768,768]{0,1} %transpose.4066), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4069 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4068), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p190.4063 = bf16[768]{0} parameter(190), frontend_attributes={neff_input_names="input190"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4070 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p190.4063), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4071 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4069, bf16[4,128,768]{2,1,0} %broadcast.4070), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4074 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4071), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.4076 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4074), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4078 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.4076), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.4099 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.4098, bf16[48,64,128]{2,1,0} %reshape.4078), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1015 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.50 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.4099, bf16[48,128,128]{2,1,0} %broadcast.1015), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3054 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.50), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.4105 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.4106 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3054, bf16[4,12,128,128]{3,2,1,0} %broadcast.4105), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.4107 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.4112 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.4106, bf16[] %constant.4107), dimensions={3}, to_apply=%MaxComputation.4108, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.4113 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.4112), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.4114 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.4106, bf16[4,12,128,128]{3,2,1,0} %broadcast.4113), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.4115 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.4114), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.4116 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.4121 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.4115, bf16[] %constant.4116), dimensions={3}, to_apply=%AddComputation.4117, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.4122 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.4121), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.4123 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.4115, bf16[4,12,128,128]{3,2,1,0} %broadcast.4122), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.140 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.141 = s64[] multiply(s64[] %constant.140, s64[] %add.139), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.142 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.143 = s64[] add(s64[] %multiply.141, s64[] %constant.142), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4025 = u64[] convert(s64[] %add.143), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4029 = u64[1]{0} reshape(u64[] %convert.4025), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.421 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.4031 = u64[2]{0} concatenate(u64[1]{0} %reshape.4029, u64[1]{0} %constant.421), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.4032 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.4031), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.4033 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.4032), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.4035 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4036 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.4035), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.4037 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.4033, u32[4,12,128,128]{3,2,1,0} %broadcast.4036), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4038 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.4037), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.426 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.120 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.426), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4044 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.4038, f32[4,12,128,128]{3,2,1,0} %broadcast.120), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4047 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.4044), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4022 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.4048 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.4047, bf16[4,12,128,128]{3,2,1,0} %broadcast.4022), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.79 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.33 = bf16[] divide(bf16[] %constant.79, bf16[] %p7.14)
  %broadcast.121 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.33), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.107 = bf16[] constant(0)
  %broadcast.146 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.107), dimensions={}
  %select.11 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.4048, bf16[4,12,128,128]{3,2,1,0} %broadcast.121, bf16[4,12,128,128]{3,2,1,0} %broadcast.146), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4124 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.4123, bf16[4,12,128,128]{3,2,1,0} %select.11), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4126 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.4124), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4008 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p189.4006 = bf16[768,768]{1,0} parameter(189), frontend_attributes={neff_input_names="input189"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4007 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p189.4006), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4009 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4008, bf16[768,768]{0,1} %transpose.4007), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4010 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4009), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p188.4004 = bf16[768]{0} parameter(188), frontend_attributes={neff_input_names="input188"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4011 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p188.4004), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4012 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4010, bf16[4,128,768]{2,1,0} %broadcast.4011), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4015 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4012), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.4016 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4015), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4018 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4016), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.4127 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.4126, bf16[48,128,64]{2,1,0} %reshape.4018), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4128 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.4127), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4129 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.4128), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4131 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.4129), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p187.3997 = bf16[768,768]{1,0} parameter(187), frontend_attributes={neff_input_names="input187"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.3998 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p187.3997), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4132 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4131, bf16[768,768]{0,1} %transpose.3998), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4133 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4132), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p186.3995 = bf16[768]{0} parameter(186), frontend_attributes={neff_input_names="input186"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4134 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p186.3995), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4135 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4133, bf16[4,128,768]{2,1,0} %broadcast.4134), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.144 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.145 = s64[] multiply(s64[] %constant.144, s64[] %add.143), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.146 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.147 = s64[] add(s64[] %multiply.145, s64[] %constant.146), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3963 = u64[] convert(s64[] %add.147), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.3967 = u64[1]{0} reshape(u64[] %convert.3963), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.428 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.3969 = u64[2]{0} concatenate(u64[1]{0} %reshape.3967, u64[1]{0} %constant.428), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.3970 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.3969), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.3971 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.3970), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.3973 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3974 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.3973), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.3975 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.3971, u32[4,128,768]{2,1,0} %broadcast.3974), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3976 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.3975), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.433 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.122 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.433), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.3982 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.3976, f32[4,128,768]{2,1,0} %broadcast.122), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.3985 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.3982), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.3960 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.3986 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.3985, bf16[4,128,768]{2,1,0} %broadcast.3960), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.81 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.34 = bf16[] divide(bf16[] %constant.81, bf16[] %p7.14)
  %broadcast.123 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.34), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.105 = bf16[] constant(0)
  %broadcast.145 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.105), dimensions={}
  %select.10 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.3986, bf16[4,128,768]{2,1,0} %broadcast.123, bf16[4,128,768]{2,1,0} %broadcast.145), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4138 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4135, bf16[4,128,768]{2,1,0} %select.10), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.4139 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.4138, bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.4140 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4139), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.5414 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.5418 = bf16[512]{0} broadcast(bf16[] %constant.5414), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.302 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.306 = bf16[512]{0} broadcast(bf16[] %constant.302), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.297 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.301 = bf16[512]{0} broadcast(bf16[] %constant.297), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.4141 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.4140, bf16[512]{0} %broadcast.306, bf16[512]{0} %broadcast.301), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4143 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4141), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4144 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4141), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4145 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4146 = bf16[512]{0} broadcast(bf16[] %constant.4145), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4147 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.4144, bf16[512]{0} %broadcast.4146), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.21 = bf16[512]{0} sqrt(bf16[512]{0} %add.4147), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.5432 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.21, bf16[512]{0} %sqrt.21), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.434 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.124 = bf16[512]{0} broadcast(bf16[] %constant.434), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.70 = bf16[512]{0} add(bf16[512]{0} %multiply.5432, bf16[512]{0} %broadcast.124), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p194.4150 = bf16[768]{0} parameter(194), frontend_attributes={neff_input_names="input194"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4156 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p194.4150), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4142 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4141), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4149 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4142), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p14.286 = bf16[768]{0} parameter(14), frontend_attributes={neff_input_names="input14"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4152 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p14.286), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4155 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4149, bf16[4,128,768]{2,1,0} %broadcast.4152), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4157 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.4156, bf16[4,128,768]{2,1,0} %multiply.4155), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4210 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4157), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p198.4208 = bf16[3072,768]{1,0} parameter(198), frontend_attributes={neff_input_names="input198"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4209 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p198.4208), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4211 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.4210, bf16[768,3072]{0,1} %transpose.4209), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4212 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.4211), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p197.4206 = bf16[3072]{0} parameter(197), frontend_attributes={neff_input_names="input197"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4213 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p197.4206), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4214 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.4212, bf16[4,128,3072]{2,1,0} %broadcast.4213), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.43 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.4214), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.4224 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.43), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p196.4199 = bf16[768,3072]{1,0} parameter(196), frontend_attributes={neff_input_names="input196"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4200 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p196.4199), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4225 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.4224, bf16[3072,768]{0,1} %transpose.4200), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4226 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4225), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p195.4197 = bf16[768]{0} parameter(195), frontend_attributes={neff_input_names="input195"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4227 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p195.4197), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4228 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4226, bf16[4,128,768]{2,1,0} %broadcast.4227), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.148 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.149 = s64[] multiply(s64[] %constant.148, s64[] %add.147), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.150 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.151 = s64[] add(s64[] %multiply.149, s64[] %constant.150), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4165 = u64[] convert(s64[] %add.151), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4169 = u64[1]{0} reshape(u64[] %convert.4165), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.436 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.4171 = u64[2]{0} concatenate(u64[1]{0} %reshape.4169, u64[1]{0} %constant.436), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.4172 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.4171), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.4173 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.4172), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.4175 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4176 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.4175), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.4177 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.4173, u32[4,128,768]{2,1,0} %broadcast.4176), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4178 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.4177), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.441 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.125 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.441), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4184 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.4178, f32[4,128,768]{2,1,0} %broadcast.125), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4187 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.4184), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4162 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.4188 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.4187, bf16[4,128,768]{2,1,0} %broadcast.4162), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.83 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.35 = bf16[] divide(bf16[] %constant.83, bf16[] %p7.14)
  %broadcast.126 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.35), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.103 = bf16[] constant(0)
  %broadcast.144 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.103), dimensions={}
  %select.9 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.4188, bf16[4,128,768]{2,1,0} %broadcast.126, bf16[4,128,768]{2,1,0} %broadcast.144), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4231 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4228, bf16[4,128,768]{2,1,0} %select.9), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.4232 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.4231, bf16[4,128,768]{2,1,0} %add.4157), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.4233 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4232), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.5255 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.5259 = bf16[512]{0} broadcast(bf16[] %constant.5255), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.275 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.279 = bf16[512]{0} broadcast(bf16[] %constant.275), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.270 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.274 = bf16[512]{0} broadcast(bf16[] %constant.270), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.4234 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.4233, bf16[512]{0} %broadcast.279, bf16[512]{0} %broadcast.274), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4236 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4234), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4237 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4234), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4238 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4239 = bf16[512]{0} broadcast(bf16[] %constant.4238), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4240 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.4237, bf16[512]{0} %broadcast.4239), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.22 = bf16[512]{0} sqrt(bf16[512]{0} %add.4240), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.5273 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.22, bf16[512]{0} %sqrt.22), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.443 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.127 = bf16[512]{0} broadcast(bf16[] %constant.443), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.73 = bf16[512]{0} add(bf16[512]{0} %multiply.5273, bf16[512]{0} %broadcast.127), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p199.4243 = bf16[768]{0} parameter(199), frontend_attributes={neff_input_names="input199"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4249 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p199.4243), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4235 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4234), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4242 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4235), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p13.259 = bf16[768]{0} parameter(13), frontend_attributes={neff_input_names="input13"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4245 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p13.259), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4248 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4242, bf16[4,128,768]{2,1,0} %broadcast.4245), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4250 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.4249, bf16[4,128,768]{2,1,0} %multiply.4248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4383 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p207.4381 = bf16[768,768]{1,0} parameter(207), frontend_attributes={neff_input_names="input207"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4382 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p207.4381), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4384 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4383, bf16[768,768]{0,1} %transpose.4382), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4385 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4384), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p206.4379 = bf16[768]{0} parameter(206), frontend_attributes={neff_input_names="input206"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4386 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p206.4379), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4387 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4385, bf16[4,128,768]{2,1,0} %broadcast.4386), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4390 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4387), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.4391 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4390), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4393 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4391), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4362 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p205.4360 = bf16[768,768]{1,0} parameter(205), frontend_attributes={neff_input_names="input205"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4361 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p205.4360), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4363 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4362, bf16[768,768]{0,1} %transpose.4361), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4364 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4363), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p204.4358 = bf16[768]{0} parameter(204), frontend_attributes={neff_input_names="input204"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4365 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p204.4358), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4366 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4364, bf16[4,128,768]{2,1,0} %broadcast.4365), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4369 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4366), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %transpose.4371 = bf16[4,12,64,128]{2,1,3,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4369), dimensions={0,2,3,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %reshape.4373 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.4371), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %dot.4394 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.4393, bf16[48,64,128]{2,1,0} %reshape.4373), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=323}
  %broadcast.1018 = bf16[48,128,128]{2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %divide.51 = bf16[48,128,128]{2,1,0} divide(bf16[48,128,128]{2,1,0} %dot.4394, bf16[48,128,128]{2,1,0} %broadcast.1018), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %reshape.3057 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %divide.51), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=347}
  %broadcast.4400 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,128]{1,0} %multiply.282), dimensions={0,3}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %add.4401 = bf16[4,12,128,128]{3,2,1,0} add(bf16[4,12,128,128]{3,2,1,0} %reshape.3057, bf16[4,12,128,128]{3,2,1,0} %broadcast.4400), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=350}
  %constant.4402 = bf16[] constant(-inf), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.4407 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %add.4401, bf16[] %constant.4402), dimensions={3}, to_apply=%MaxComputation.4403, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.4408 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.4407), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %subtract.4409 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %add.4401, bf16[4,12,128,128]{3,2,1,0} %broadcast.4408), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %exponential.4410 = bf16[4,12,128,128]{3,2,1,0} exponential(bf16[4,12,128,128]{3,2,1,0} %subtract.4409), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.4411 = bf16[] constant(0), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %reduce.4416 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %exponential.4410, bf16[] %constant.4411), dimensions={3}, to_apply=%AddComputation.4412, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %broadcast.4417 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.4416), dimensions={0,1,2}, metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %divide.4418 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %exponential.4410, bf16[4,12,128,128]{3,2,1,0} %broadcast.4417), metadata={op_type="aten__softmax" op_name="aten__softmax" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1856}
  %constant.152 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.153 = s64[] multiply(s64[] %constant.152, s64[] %add.151), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.154 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.155 = s64[] add(s64[] %multiply.153, s64[] %constant.154), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4320 = u64[] convert(s64[] %add.155), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4324 = u64[1]{0} reshape(u64[] %convert.4320), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.445 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.4326 = u64[2]{0} concatenate(u64[1]{0} %reshape.4324, u64[1]{0} %constant.445), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.4327 = (u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.4326), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.4328 = u32[4,12,128,128]{3,2,1,0} get-tuple-element((u64[2]{0}, u32[4,12,128,128]{3,2,1,0}) %rng-bit-generator.4327), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.4330 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4331 = u32[4,12,128,128]{3,2,1,0} broadcast(u32[] %constant.4330), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.4332 = u32[4,12,128,128]{3,2,1,0} shift-right-logical(u32[4,12,128,128]{3,2,1,0} %get-tuple-element.4328, u32[4,12,128,128]{3,2,1,0} %broadcast.4331), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4333 = f32[4,12,128,128]{3,2,1,0} convert(u32[4,12,128,128]{3,2,1,0} %shift-right-logical.4332), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.451 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.129 = f32[4,12,128,128]{3,2,1,0} broadcast(f32[] %constant.451), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4339 = f32[4,12,128,128]{3,2,1,0} multiply(f32[4,12,128,128]{3,2,1,0} %convert.4333, f32[4,12,128,128]{3,2,1,0} %broadcast.129), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4342 = bf16[4,12,128,128]{3,2,1,0} convert(f32[4,12,128,128]{3,2,1,0} %multiply.4339), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4317 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.4343 = pred[4,12,128,128]{3,2,1,0} compare(bf16[4,12,128,128]{3,2,1,0} %convert.4342, bf16[4,12,128,128]{3,2,1,0} %broadcast.4317), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.85 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.36 = bf16[] divide(bf16[] %constant.85, bf16[] %p7.14)
  %broadcast.130 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %divide.36), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.101 = bf16[] constant(0)
  %broadcast.143 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %constant.101), dimensions={}
  %select.8 = bf16[4,12,128,128]{3,2,1,0} select(pred[4,12,128,128]{3,2,1,0} %compare.4343, bf16[4,12,128,128]{3,2,1,0} %broadcast.130, bf16[4,12,128,128]{3,2,1,0} %broadcast.143), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4419 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.4418, bf16[4,12,128,128]{3,2,1,0} %select.8), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4421 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.4419), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4303 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p203.4301 = bf16[768,768]{1,0} parameter(203), frontend_attributes={neff_input_names="input203"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4302 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p203.4301), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4304 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4303, bf16[768,768]{0,1} %transpose.4302), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4305 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4304), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p202.4299 = bf16[768]{0} parameter(202), frontend_attributes={neff_input_names="input202"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4306 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p202.4299), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4307 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4305, bf16[4,128,768]{2,1,0} %broadcast.4306), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4310 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4307), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %transpose.4311 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.4310), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4313 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4311), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %dot.4422 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.4421, bf16[48,128,64]{2,1,0} %reshape.4313), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=363}
  %reshape.4423 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.4422), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4424 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.4423), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4426 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.4424), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p201.4292 = bf16[768,768]{1,0} parameter(201), frontend_attributes={neff_input_names="input201"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4293 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p201.4292), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4427 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.4426, bf16[768,768]{0,1} %transpose.4293), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4428 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4427), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p200.4290 = bf16[768]{0} parameter(200), frontend_attributes={neff_input_names="input200"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4429 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p200.4290), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4430 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4428, bf16[4,128,768]{2,1,0} %broadcast.4429), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.156 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.157 = s64[] multiply(s64[] %constant.156, s64[] %add.155), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.158 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.159 = s64[] add(s64[] %multiply.157, s64[] %constant.158), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4258 = u64[] convert(s64[] %add.159), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4262 = u64[1]{0} reshape(u64[] %convert.4258), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.452 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.4264 = u64[2]{0} concatenate(u64[1]{0} %reshape.4262, u64[1]{0} %constant.452), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.4265 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.4264), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.4266 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.4265), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.4268 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4269 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.4268), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.4270 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.4266, u32[4,128,768]{2,1,0} %broadcast.4269), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4271 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.4270), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.457 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.131 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.457), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4277 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.4271, f32[4,128,768]{2,1,0} %broadcast.131), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4280 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.4277), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4255 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.4281 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.4280, bf16[4,128,768]{2,1,0} %broadcast.4255), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.87 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.37 = bf16[] divide(bf16[] %constant.87, bf16[] %p7.14)
  %broadcast.132 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.37), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.99 = bf16[] constant(0)
  %broadcast.142 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.99), dimensions={}
  %select.7 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.4281, bf16[4,128,768]{2,1,0} %broadcast.132, bf16[4,128,768]{2,1,0} %broadcast.142), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4433 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4430, bf16[4,128,768]{2,1,0} %select.7), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.4434 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.4433, bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=386}
  %reshape.4435 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4434), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4967 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.4971 = bf16[512]{0} broadcast(bf16[] %constant.4967), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.248 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.252 = bf16[512]{0} broadcast(bf16[] %constant.248), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.243 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.247 = bf16[512]{0} broadcast(bf16[] %constant.243), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.4436 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.4435, bf16[512]{0} %broadcast.252, bf16[512]{0} %broadcast.247), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4438 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4436), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4439 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4436), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4440 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4441 = bf16[512]{0} broadcast(bf16[] %constant.4440), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4442 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.4439, bf16[512]{0} %broadcast.4441), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.23 = bf16[512]{0} sqrt(bf16[512]{0} %add.4442), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4985 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.23, bf16[512]{0} %sqrt.23), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.458 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.133 = bf16[512]{0} broadcast(bf16[] %constant.458), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.77 = bf16[512]{0} add(bf16[512]{0} %multiply.4985, bf16[512]{0} %broadcast.133), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p208.4445 = bf16[768]{0} parameter(208), frontend_attributes={neff_input_names="input208"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4451 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p208.4445), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4437 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4436), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4444 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4437), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p12.232 = bf16[768]{0} parameter(12), frontend_attributes={neff_input_names="input12"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4447 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p12.232), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4450 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4444, bf16[4,128,768]{2,1,0} %broadcast.4447), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4452 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.4451, bf16[4,128,768]{2,1,0} %multiply.4450), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4505 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4452), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p212.4503 = bf16[3072,768]{1,0} parameter(212), frontend_attributes={neff_input_names="input212"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4504 = bf16[768,3072]{0,1} transpose(bf16[3072,768]{1,0} %p212.4503), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4506 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.4505, bf16[768,3072]{0,1} %transpose.4504), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4507 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.4506), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p211.4501 = bf16[3072]{0} parameter(211), frontend_attributes={neff_input_names="input211"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4508 = bf16[4,128,3072]{2,1,0} broadcast(bf16[3072]{0} %p211.4501), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4509 = bf16[4,128,3072]{2,1,0} add(bf16[4,128,3072]{2,1,0} %reshape.4507, bf16[4,128,3072]{2,1,0} %broadcast.4508), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %custom-call.44 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.4509), custom_call_target="AwsNeuronGelu", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_GeluForwardImpl" op_name="xla___op_GeluForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.4519 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.44), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p210.4494 = bf16[768,3072]{1,0} parameter(210), frontend_attributes={neff_input_names="input210"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.4495 = bf16[3072,768]{0,1} transpose(bf16[768,3072]{1,0} %p210.4494), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4520 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.4519, bf16[3072,768]{0,1} %transpose.4495), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4521 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4520), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p209.4492 = bf16[768]{0} parameter(209), frontend_attributes={neff_input_names="input209"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4522 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p209.4492), dimensions={2}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4523 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4521, bf16[4,128,768]{2,1,0} %broadcast.4522), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.160 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.161 = s64[] multiply(s64[] %constant.160, s64[] %add.159), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.162 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.163 = s64[] add(s64[] %multiply.161, s64[] %constant.162), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4460 = u64[] convert(s64[] %add.163), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.4464 = u64[1]{0} reshape(u64[] %convert.4460), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.461 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.4466 = u64[2]{0} concatenate(u64[1]{0} %reshape.4464, u64[1]{0} %constant.461), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.4467 = (u64[2]{0}, u32[4,128,768]{2,1,0}) rng-bit-generator(u64[2]{0} %concatenate.4466), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.4468 = u32[4,128,768]{2,1,0} get-tuple-element((u64[2]{0}, u32[4,128,768]{2,1,0}) %rng-bit-generator.4467), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.4470 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4471 = u32[4,128,768]{2,1,0} broadcast(u32[] %constant.4470), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.4472 = u32[4,128,768]{2,1,0} shift-right-logical(u32[4,128,768]{2,1,0} %get-tuple-element.4468, u32[4,128,768]{2,1,0} %broadcast.4471), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4473 = f32[4,128,768]{2,1,0} convert(u32[4,128,768]{2,1,0} %shift-right-logical.4472), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.466 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.134 = f32[4,128,768]{2,1,0} broadcast(f32[] %constant.466), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4479 = f32[4,128,768]{2,1,0} multiply(f32[4,128,768]{2,1,0} %convert.4473, f32[4,128,768]{2,1,0} %broadcast.134), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.4482 = bf16[4,128,768]{2,1,0} convert(f32[4,128,768]{2,1,0} %multiply.4479), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.4457 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.4483 = pred[4,128,768]{2,1,0} compare(bf16[4,128,768]{2,1,0} %convert.4482, bf16[4,128,768]{2,1,0} %broadcast.4457), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.89 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.38 = bf16[] divide(bf16[] %constant.89, bf16[] %p7.14)
  %broadcast.135 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %divide.38), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.97 = bf16[] constant(0)
  %broadcast.141 = bf16[4,128,768]{2,1,0} broadcast(bf16[] %constant.97), dimensions={}
  %select.6 = bf16[4,128,768]{2,1,0} select(pred[4,128,768]{2,1,0} %compare.4483, bf16[4,128,768]{2,1,0} %broadcast.135, bf16[4,128,768]{2,1,0} %broadcast.141), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4526 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4523, bf16[4,128,768]{2,1,0} %select.6), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.4527 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %multiply.4526, bf16[4,128,768]{2,1,0} %add.4452), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=464}
  %reshape.4528 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %add.4527), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4808 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.4812 = bf16[512]{0} broadcast(bf16[] %constant.4808), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.221 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.225 = bf16[512]{0} broadcast(bf16[] %constant.221), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.216 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.220 = bf16[512]{0} broadcast(bf16[] %constant.216), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %batch-norm-training.4529 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-training(bf16[1,512,768]{2,1,0} %reshape.4528, bf16[512]{0} %broadcast.225, bf16[512]{0} %broadcast.220), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4531 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4529), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4532 = bf16[512]{0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4529), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %constant.4533 = bf16[] constant(1.002e-12), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4534 = bf16[512]{0} broadcast(bf16[] %constant.4533), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4535 = bf16[512]{0} add(bf16[512]{0} %get-tuple-element.4532, bf16[512]{0} %broadcast.4534), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %sqrt.24 = bf16[512]{0} sqrt(bf16[512]{0} %add.4535), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4826 = bf16[512]{0} multiply(bf16[512]{0} %sqrt.24, bf16[512]{0} %sqrt.24), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.467 = bf16[] constant(-1.002e-12), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.136 = bf16[512]{0} broadcast(bf16[] %constant.467), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %add.80 = bf16[512]{0} add(bf16[512]{0} %multiply.4826, bf16[512]{0} %broadcast.136), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %p213.4538 = bf16[768]{0} parameter(213), frontend_attributes={neff_input_names="input213"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4544 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p213.4538), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %get-tuple-element.4530 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-training.4529), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %reshape.4537 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4530), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %p11.205 = bf16[768]{0} parameter(11), frontend_attributes={neff_input_names="input11"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %broadcast.4540 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p11.205), dimensions={2}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %multiply.4543 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4537, bf16[4,128,768]{2,1,0} %broadcast.4540), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %add.4545 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %broadcast.4544, bf16[4,128,768]{2,1,0} %multiply.4543), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=2543}
  %slice.4547 = bf16[4,1,768]{2,1,0} slice(bf16[4,128,768]{2,1,0} %add.4545), slice={[0:4], [0:1], [0:768]}, metadata={op_type="xla__generic_slice" op_name="xla__generic_slice" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %reshape.4548 = bf16[4,768]{1,0} reshape(bf16[4,1,768]{2,1,0} %slice.4547), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p10.202 = bf16[768,768]{1,0} parameter(10), frontend_attributes={neff_input_names="input10"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.203 = bf16[768,768]{0,1} transpose(bf16[768,768]{1,0} %p10.202), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4549 = bf16[4,768]{1,0} dot(bf16[4,768]{1,0} %reshape.4548, bf16[768,768]{0,1} %transpose.203), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p9.201 = bf16[768]{0} parameter(9), frontend_attributes={neff_input_names="input9"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4553 = bf16[4,768]{1,0} broadcast(bf16[768]{0} %p9.201), dimensions={1}, metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4554 = bf16[4,768]{1,0} add(bf16[4,768]{1,0} %dot.4549, bf16[4,768]{1,0} %broadcast.4553), metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %tanh.4555 = bf16[4,768]{1,0} tanh(bf16[4,768]{1,0} %add.4554), metadata={op_type="aten__tanh" op_name="aten__tanh" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/activation.py" source_line=356}
  %constant.164 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.165 = s64[] multiply(s64[] %constant.164, s64[] %add.163), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.166 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %add.167 = s64[] add(s64[] %multiply.165, s64[] %constant.166), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.174 = u64[] convert(s64[] %add.167), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %reshape.178 = u64[1]{0} reshape(u64[] %convert.174), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.470 = u64[1]{0} constant({0}), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %concatenate.180 = u64[2]{0} concatenate(u64[1]{0} %reshape.178, u64[1]{0} %constant.470), dimensions={0}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %rng-bit-generator.181 = (u64[2]{0}, u32[4,768]{1,0}) rng-bit-generator(u64[2]{0} %concatenate.180), algorithm=rng_default, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %get-tuple-element.182 = u32[4,768]{1,0} get-tuple-element((u64[2]{0}, u32[4,768]{1,0}) %rng-bit-generator.181), index=1, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.184 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.185 = u32[4,768]{1,0} broadcast(u32[] %constant.184), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %shift-right-logical.186 = u32[4,768]{1,0} shift-right-logical(u32[4,768]{1,0} %get-tuple-element.182, u32[4,768]{1,0} %broadcast.185), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.187 = f32[4,768]{1,0} convert(u32[4,768]{1,0} %shift-right-logical.186), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.475 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.137 = f32[4,768]{1,0} broadcast(f32[] %constant.475), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.193 = f32[4,768]{1,0} multiply(f32[4,768]{1,0} %convert.187, f32[4,768]{1,0} %broadcast.137), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %convert.196 = bf16[4,768]{1,0} convert(f32[4,768]{1,0} %multiply.193), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %broadcast.171 = bf16[4,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %compare.197 = pred[4,768]{1,0} compare(bf16[4,768]{1,0} %convert.196, bf16[4,768]{1,0} %broadcast.171), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.91 = bf16[] constant(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %divide.39 = bf16[] divide(bf16[] %constant.91, bf16[] %p7.14)
  %broadcast.138 = bf16[4,768]{1,0} broadcast(bf16[] %divide.39), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %constant.169 = bf16[] constant(0)
  %broadcast.538 = bf16[4,768]{1,0} broadcast(bf16[] %constant.169), dimensions={}
  %select.43 = bf16[4,768]{1,0} select(pred[4,768]{1,0} %compare.197, bf16[4,768]{1,0} %broadcast.138, bf16[4,768]{1,0} %broadcast.538), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %multiply.4556 = bf16[4,768]{1,0} multiply(bf16[4,768]{1,0} %tanh.4555, bf16[4,768]{1,0} %select.43), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/functional.py" source_line=1266}
  %p6.12 = bf16[2,768]{1,0} parameter(6), frontend_attributes={neff_input_names="input6"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %transpose.13 = bf16[768,2]{0,1} transpose(bf16[2,768]{1,0} %p6.12), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %dot.4557 = bf16[4,2]{1,0} dot(bf16[4,768]{1,0} %multiply.4556, bf16[768,2]{0,1} %transpose.13), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %p5.11 = bf16[2]{0} parameter(5), frontend_attributes={neff_input_names="input5"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %broadcast.4561 = bf16[4,2]{1,0} broadcast(bf16[2]{0} %p5.11), dimensions={1}, metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %add.4562 = bf16[4,2]{1,0} add(bf16[4,2]{1,0} %dot.4557, bf16[4,2]{1,0} %broadcast.4561), metadata={op_type="aten__addmm" op_name="aten__addmm" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py" source_line=114}
  %constant.0 = bf16[] constant(-inf)
  %reduce.0 = bf16[4]{0} reduce(bf16[4,2]{1,0} %add.4562, bf16[] %constant.0), dimensions={1}, to_apply=%SimpleCrossEntropyLossForwardMax.4563
  %broadcast.0 = bf16[4,2]{1,0} broadcast(bf16[4]{0} %reduce.0), dimensions={0}
  %subtract.0 = bf16[4,2]{1,0} subtract(bf16[4,2]{1,0} %add.4562, bf16[4,2]{1,0} %broadcast.0)
  %exponential.0 = bf16[4,2]{1,0} exponential(bf16[4,2]{1,0} %subtract.0)
  %constant.1 = bf16[] constant(0)
  %reduce.1 = bf16[4]{0} reduce(bf16[4,2]{1,0} %exponential.0, bf16[] %constant.1), dimensions={1}, to_apply=%SimpleCrossEntropyLossForwardAdd.4567
  %log.0 = bf16[4]{0} log(bf16[4]{0} %reduce.1)
  %broadcast.1 = bf16[4,2]{1,0} broadcast(bf16[4]{0} %log.0), dimensions={0}
  %subtract.1 = bf16[4,2]{1,0} subtract(bf16[4,2]{1,0} %subtract.0, bf16[4,2]{1,0} %broadcast.1), metadata={op_type="xla___op_SimpleCrossEntropyLossForwardImpl" op_name="xla___op_SimpleCrossEntropyLossForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %exponential.1 = bf16[4,2]{1,0} exponential(bf16[4,2]{1,0} %subtract.1)
  %p4.10 = s64[4]{0} parameter(4), frontend_attributes={neff_input_names="input4"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py" source_line=1597}
  %constant.5 = s64[] constant(-100)
  %broadcast.8 = s64[4]{0} broadcast(s64[] %constant.5), dimensions={}
  %compare.1 = pred[4]{0} compare(s64[4]{0} %p4.10, s64[4]{0} %broadcast.8), direction=NE
  %constant.4626 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/autograd/__init__.py" source_line=127}
  %convert.0 = bf16[4]{0} convert(pred[4]{0} %compare.1), metadata={op_type="xla___op_SimpleCrossEntropyLossForwardImpl" op_name="xla___op_SimpleCrossEntropyLossForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.8 = bf16[] constant(0)
  %reduce.5 = bf16[] reduce(bf16[4]{0} %convert.0, bf16[] %constant.8), dimensions={0}, to_apply=%SimpleCrossEntropyLossBackwardAdd.4628
  %divide.1 = bf16[] divide(bf16[] %constant.4626, bf16[] %reduce.5)
  %broadcast.9 = bf16[4]{0} broadcast(bf16[] %divide.1), dimensions={}
  %constant.95 = bf16[] constant(0)
  %broadcast.140 = bf16[4]{0} broadcast(bf16[] %constant.95), dimensions={}
  %select.5 = bf16[4]{0} select(pred[4]{0} %compare.1, bf16[4]{0} %broadcast.9, bf16[4]{0} %broadcast.140)
  %broadcast.10 = bf16[4,2]{1,0} broadcast(bf16[4]{0} %select.5), dimensions={0}
  %multiply.18 = bf16[4,2]{1,0} multiply(bf16[4,2]{1,0} %exponential.1, bf16[4,2]{1,0} %broadcast.10)
  %broadcast.12 = s64[4,2]{1,0} broadcast(s64[4]{0} %p4.10), dimensions={0}
  %iota.3 = s64[4,2]{1,0} iota(), iota_dimension=1
  %compare.2 = pred[4,2]{1,0} compare(s64[4,2]{1,0} %broadcast.12, s64[4,2]{1,0} %iota.3), direction=EQ
  %negate.1 = bf16[] negate(bf16[] %divide.1)
  %broadcast.15 = bf16[4,2]{1,0} broadcast(bf16[] %negate.1), dimensions={}
  %constant.9 = bf16[] constant(0)
  %broadcast.16 = bf16[4,2]{1,0} broadcast(bf16[] %constant.9), dimensions={}
  %select.1 = bf16[4,2]{1,0} select(pred[4,2]{1,0} %compare.2, bf16[4,2]{1,0} %broadcast.15, bf16[4,2]{1,0} %broadcast.16)
  %add.0 = bf16[4,2]{1,0} add(bf16[4,2]{1,0} %multiply.18, bf16[4,2]{1,0} %select.1), metadata={op_type="xla___op_SimpleCrossEntropyLossBackwardImpl" op_name="xla___op_SimpleCrossEntropyLossBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %dot.4711 = bf16[4,768]{1,0} dot(bf16[4,2]{1,0} %add.0, bf16[2,768]{1,0} %p6.12), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.4712 = bf16[4,768]{1,0} multiply(bf16[4,768]{1,0} %dot.4711, bf16[4,768]{1,0} %select.43), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4703 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.4707 = bf16[4,768]{1,0} broadcast(bf16[] %constant.4703), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.4702 = bf16[4,768]{1,0} multiply(bf16[4,768]{1,0} %tanh.4555, bf16[4,768]{1,0} %tanh.4555), metadata={op_type="aten__pow" op_name="aten__pow"}
  %subtract.4708 = bf16[4,768]{1,0} subtract(bf16[4,768]{1,0} %broadcast.4707, bf16[4,768]{1,0} %multiply.4702), metadata={op_type="aten__sub" op_name="aten__sub"}
  %multiply.4713 = bf16[4,768]{1,0} multiply(bf16[4,768]{1,0} %multiply.4712, bf16[4,768]{1,0} %subtract.4708), metadata={op_type="aten__mul" op_name="aten__mul"}
  %dot.4749 = bf16[4,768]{1,0} dot(bf16[4,768]{1,0} %multiply.4713, bf16[768,768]{1,0} %p10.202), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.4750 = bf16[4,1,768]{2,1,0} reshape(bf16[4,768]{1,0} %dot.4749), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.4751 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %pad = bf16[4,128,768]{2,1,0} pad(bf16[4,1,768]{2,1,0} %reshape.4750, bf16[] %constant.4751), padding=0_0x0_127x0_0, metadata={op_type="xla__update_slice" op_name="xla__update_slice"}
  %broadcast.4819 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p11.205), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.4820 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %pad, bf16[4,128,768]{2,1,0} %broadcast.4819), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.4821 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.4820), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.4829 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.4528, bf16[512]{0} %broadcast.4812, bf16[512]{0} %get-tuple-element.4531, bf16[512]{0} %add.80, bf16[1,512,768]{2,1,0} %reshape.4821), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.4830 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.4829), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.4833 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4830), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.4834 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4833, bf16[4,128,768]{2,1,0} %select.6), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.4853 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.4834), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.4869 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.4853, bf16[768,3072]{1,0} %p210.4494), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.4870 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.4869), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.45 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.4509), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.19 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.4870, bf16[4,128,3072]{2,1,0} %custom-call.45), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.4899 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.19), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.4920 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.4899, bf16[3072,768]{1,0} %p212.4503), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.4921 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.4920), metadata={op_type="aten__view" op_name="aten__view"}
  %add.4923 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4833, bf16[4,128,768]{2,1,0} %reshape.4921), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.4978 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p12.232), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.4979 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4923, bf16[4,128,768]{2,1,0} %broadcast.4978), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.4980 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.4979), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.4988 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.4435, bf16[512]{0} %broadcast.4971, bf16[512]{0} %get-tuple-element.4438, bf16[512]{0} %add.77, bf16[1,512,768]{2,1,0} %reshape.4980), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.4989 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.4988), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.4992 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.4989), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.5036 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.4419), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5037 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.5036), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.4993 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.4992, bf16[4,128,768]{2,1,0} %select.7), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5012 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.4993), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5031 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5012, bf16[768,768]{1,0} %p201.4292), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5033 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.5031), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5034 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.5033), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5035 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.5034), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5038 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.5037, bf16[48,128,64]{2,1,0} %reshape.5035), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5060 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5038), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5061 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5060), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5063 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5061), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5206 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5063, bf16[768,768]{1,0} %p203.4301), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5207 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5206), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5209 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.4992, bf16[4,128,768]{2,1,0} %reshape.5207), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.5095 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4391), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5096 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5095), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.5077 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4311), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5078 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5077), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.5079 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.5035, bf16[48,64,128]{1,2,0} %transpose.5078), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5080 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.5079), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.5081 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.5080, bf16[4,12,128,128]{3,2,1,0} %select.8), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5082 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.5081, bf16[4,12,128,128]{3,2,1,0} %divide.4418), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.5083 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.5088 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.5082, bf16[] %constant.5083), dimensions={3}, to_apply=%AddComputation.5084, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5089 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.5088), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.5090 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.5081, bf16[4,12,128,128]{3,2,1,0} %broadcast.5089), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.5091 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.4418, bf16[4,12,128,128]{3,2,1,0} %subtract.5090), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5092 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.5093 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.5091, bf16[4,12,128,128]{3,2,1,0} %broadcast.5092), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.5094 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.5093), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5097 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.5096, bf16[48,128,128]{2,1,0} %reshape.5094), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5120 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5097), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5122 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.5120), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5124 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.5122), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5196 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5124, bf16[768,768]{1,0} %p205.4360), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5197 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5196), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5210 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.5209, bf16[4,128,768]{2,1,0} %reshape.5197), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.5138 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.4371), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5139 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.5138), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.5140 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.5094, bf16[48,128,64]{1,2,0} %transpose.5139), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5162 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5140), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5163 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5162), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5165 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5163), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5186 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5165, bf16[768,768]{1,0} %p207.4381), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5187 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5186), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5211 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.5210, bf16[4,128,768]{2,1,0} %reshape.5187), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.5266 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p13.259), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5267 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5211, bf16[4,128,768]{2,1,0} %broadcast.5266), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5268 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5267), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.5276 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.4233, bf16[512]{0} %broadcast.5259, bf16[512]{0} %get-tuple-element.4236, bf16[512]{0} %add.73, bf16[1,512,768]{2,1,0} %reshape.5268), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.5277 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.5276), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.5280 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.5277), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.5281 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.5280, bf16[4,128,768]{2,1,0} %select.9), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5300 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5281), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5316 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.5300, bf16[768,3072]{1,0} %p196.4199), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5317 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.5316), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.46 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.4214), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.20 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.5317, bf16[4,128,3072]{2,1,0} %custom-call.46), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.5346 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.20), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5367 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.5346, bf16[3072,768]{1,0} %p198.4208), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5368 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5367), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5370 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.5280, bf16[4,128,768]{2,1,0} %reshape.5368), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.5425 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p14.286), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5426 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5370, bf16[4,128,768]{2,1,0} %broadcast.5425), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5427 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5426), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.5435 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.4140, bf16[512]{0} %broadcast.5418, bf16[512]{0} %get-tuple-element.4143, bf16[512]{0} %add.70, bf16[1,512,768]{2,1,0} %reshape.5427), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.5436 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.5435), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.5439 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.5436), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.5483 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.4124), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5484 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.5483), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.5440 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.5439, bf16[4,128,768]{2,1,0} %select.10), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5459 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5440), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5478 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5459, bf16[768,768]{1,0} %p187.3997), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5480 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.5478), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5481 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.5480), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5482 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.5481), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5485 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.5484, bf16[48,128,64]{2,1,0} %reshape.5482), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5507 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5485), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5508 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5507), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5510 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5508), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5653 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5510, bf16[768,768]{1,0} %p189.4006), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5654 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5653), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5656 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.5439, bf16[4,128,768]{2,1,0} %reshape.5654), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.5542 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4096), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5543 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5542), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.5524 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.4016), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5525 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5524), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.5526 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.5482, bf16[48,64,128]{1,2,0} %transpose.5525), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5527 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.5526), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.5528 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.5527, bf16[4,12,128,128]{3,2,1,0} %select.11), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5529 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.5528, bf16[4,12,128,128]{3,2,1,0} %divide.4123), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.5530 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.5535 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.5529, bf16[] %constant.5530), dimensions={3}, to_apply=%AddComputation.5531, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5536 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.5535), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.5537 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.5528, bf16[4,12,128,128]{3,2,1,0} %broadcast.5536), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.5538 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.4123, bf16[4,12,128,128]{3,2,1,0} %subtract.5537), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5539 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.5540 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.5538, bf16[4,12,128,128]{3,2,1,0} %broadcast.5539), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.5541 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.5540), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5544 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.5543, bf16[48,128,128]{2,1,0} %reshape.5541), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5567 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5544), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5569 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.5567), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5571 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.5569), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5643 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5571, bf16[768,768]{1,0} %p191.4065), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5644 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5643), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5657 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.5656, bf16[4,128,768]{2,1,0} %reshape.5644), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.5585 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.4076), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5586 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.5585), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.5587 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.5541, bf16[48,128,64]{1,2,0} %transpose.5586), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5609 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5587), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5610 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5609), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5612 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5610), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5633 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5612, bf16[768,768]{1,0} %p193.4086), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5634 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5633), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5658 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.5657, bf16[4,128,768]{2,1,0} %reshape.5634), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.5713 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p15.313), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5714 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5658, bf16[4,128,768]{2,1,0} %broadcast.5713), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5715 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5714), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.5723 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3938, bf16[512]{0} %broadcast.5706, bf16[512]{0} %get-tuple-element.3941, bf16[512]{0} %add.66, bf16[1,512,768]{2,1,0} %reshape.5715), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.5724 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.5723), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.5727 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.5724), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.5728 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.5727, bf16[4,128,768]{2,1,0} %select.12), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5747 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5728), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5763 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.5747, bf16[768,3072]{1,0} %p182.3904), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5764 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.5763), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.47 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3919), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.22 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.5764, bf16[4,128,3072]{2,1,0} %custom-call.47), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.5793 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.22), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5814 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.5793, bf16[3072,768]{1,0} %p184.3913), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5815 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.5814), metadata={op_type="aten__view" op_name="aten__view"}
  %add.5817 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.5727, bf16[4,128,768]{2,1,0} %reshape.5815), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.5872 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p16.340), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5873 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5817, bf16[4,128,768]{2,1,0} %broadcast.5872), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5874 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5873), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.5882 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3845, bf16[512]{0} %broadcast.5865, bf16[512]{0} %get-tuple-element.3848, bf16[512]{0} %add.64, bf16[1,512,768]{2,1,0} %reshape.5874), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.5883 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.5882), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.5886 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.5883), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.5930 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3829), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5931 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.5930), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.5887 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.5886, bf16[4,128,768]{2,1,0} %select.13), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.5906 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.5887), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5925 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5906, bf16[768,768]{1,0} %p173.3702), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.5927 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.5925), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5928 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.5927), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5929 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.5928), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5932 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.5931, bf16[48,128,64]{2,1,0} %reshape.5929), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5954 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5932), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5955 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5954), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5957 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5955), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6100 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.5957, bf16[768,768]{1,0} %p175.3711), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6101 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6100), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6103 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.5886, bf16[4,128,768]{2,1,0} %reshape.6101), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.5989 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3801), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5990 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5989), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.5971 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3721), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5972 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.5971), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.5973 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.5929, bf16[48,64,128]{1,2,0} %transpose.5972), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.5974 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.5973), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.5975 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.5974, bf16[4,12,128,128]{3,2,1,0} %select.14), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.5976 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.5975, bf16[4,12,128,128]{3,2,1,0} %divide.3828), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.5977 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.5982 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.5976, bf16[] %constant.5977), dimensions={3}, to_apply=%AddComputation.5978, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5983 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.5982), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.5984 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.5975, bf16[4,12,128,128]{3,2,1,0} %broadcast.5983), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.5985 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3828, bf16[4,12,128,128]{3,2,1,0} %subtract.5984), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.5986 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.5987 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.5985, bf16[4,12,128,128]{3,2,1,0} %broadcast.5986), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.5988 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.5987), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.5991 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.5990, bf16[48,128,128]{2,1,0} %reshape.5988), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6014 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5991), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6016 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.6014), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6018 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.6016), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6090 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6018, bf16[768,768]{1,0} %p177.3770), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6091 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6090), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6104 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6103, bf16[4,128,768]{2,1,0} %reshape.6091), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.6032 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3781), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6033 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.6032), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.6034 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.5988, bf16[48,128,64]{1,2,0} %transpose.6033), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6056 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6034), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6057 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6056), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6059 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6057), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6080 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6059, bf16[768,768]{1,0} %p179.3791), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6081 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6080), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6105 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6104, bf16[4,128,768]{2,1,0} %reshape.6081), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.6160 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p17.367), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6161 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6105, bf16[4,128,768]{2,1,0} %broadcast.6160), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6162 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6161), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.6170 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3643, bf16[512]{0} %broadcast.6153, bf16[512]{0} %get-tuple-element.3646, bf16[512]{0} %add.60, bf16[1,512,768]{2,1,0} %reshape.6162), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.6171 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.6170), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.6174 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.6171), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.6175 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.6174, bf16[4,128,768]{2,1,0} %select.15), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6194 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6175), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6210 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.6194, bf16[768,3072]{1,0} %p168.3609), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6211 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.6210), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.48 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3624), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.23 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.6211, bf16[4,128,3072]{2,1,0} %custom-call.48), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.6240 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.23), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6261 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.6240, bf16[3072,768]{1,0} %p170.3618), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6262 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6261), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6264 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.6174, bf16[4,128,768]{2,1,0} %reshape.6262), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.6319 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p18.394), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6320 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6264, bf16[4,128,768]{2,1,0} %broadcast.6319), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6321 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6320), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.6329 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3550, bf16[512]{0} %broadcast.6312, bf16[512]{0} %get-tuple-element.3553, bf16[512]{0} %add.57, bf16[1,512,768]{2,1,0} %reshape.6321), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.6330 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.6329), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.6333 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.6330), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.6377 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3534), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6378 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.6377), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.6334 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.6333, bf16[4,128,768]{2,1,0} %select.16), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6353 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6334), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6372 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6353, bf16[768,768]{1,0} %p159.3407), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6374 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.6372), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6375 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.6374), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6376 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.6375), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6379 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.6378, bf16[48,128,64]{2,1,0} %reshape.6376), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6401 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6379), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6402 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6401), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6404 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6402), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6547 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6404, bf16[768,768]{1,0} %p161.3416), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6548 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6547), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6550 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.6333, bf16[4,128,768]{2,1,0} %reshape.6548), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.6436 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3506), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6437 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.6436), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.6418 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3426), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6419 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.6418), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.6420 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.6376, bf16[48,64,128]{1,2,0} %transpose.6419), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6421 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.6420), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.6422 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.6421, bf16[4,12,128,128]{3,2,1,0} %select.17), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6423 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.6422, bf16[4,12,128,128]{3,2,1,0} %divide.3533), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.6424 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.6429 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.6423, bf16[] %constant.6424), dimensions={3}, to_apply=%AddComputation.6425, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.6430 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.6429), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.6431 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.6422, bf16[4,12,128,128]{3,2,1,0} %broadcast.6430), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.6432 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3533, bf16[4,12,128,128]{3,2,1,0} %subtract.6431), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.6433 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.6434 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.6432, bf16[4,12,128,128]{3,2,1,0} %broadcast.6433), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.6435 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.6434), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6438 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.6437, bf16[48,128,128]{2,1,0} %reshape.6435), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6461 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.6438), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6463 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.6461), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6465 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.6463), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6537 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6465, bf16[768,768]{1,0} %p163.3475), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6538 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6537), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6551 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6550, bf16[4,128,768]{2,1,0} %reshape.6538), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.6479 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3486), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6480 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.6479), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.6481 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.6435, bf16[48,128,64]{1,2,0} %transpose.6480), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6503 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6481), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6504 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6503), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6506 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6504), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6527 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6506, bf16[768,768]{1,0} %p165.3496), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6528 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6527), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6552 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6551, bf16[4,128,768]{2,1,0} %reshape.6528), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.6607 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p19.421), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6608 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6552, bf16[4,128,768]{2,1,0} %broadcast.6607), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6609 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6608), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.6617 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3348, bf16[512]{0} %broadcast.6600, bf16[512]{0} %get-tuple-element.3351, bf16[512]{0} %add.53, bf16[1,512,768]{2,1,0} %reshape.6609), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.6618 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.6617), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.6621 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.6618), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.6622 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.6621, bf16[4,128,768]{2,1,0} %select.18), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6641 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6622), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6657 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.6641, bf16[768,3072]{1,0} %p154.3314), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6658 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.6657), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.49 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3329), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.24 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.6658, bf16[4,128,3072]{2,1,0} %custom-call.49), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.6687 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.24), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6708 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.6687, bf16[3072,768]{1,0} %p156.3323), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6709 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6708), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6711 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.6621, bf16[4,128,768]{2,1,0} %reshape.6709), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.6766 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p20.448), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6767 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6711, bf16[4,128,768]{2,1,0} %broadcast.6766), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6768 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6767), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.6776 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3255, bf16[512]{0} %broadcast.6759, bf16[512]{0} %get-tuple-element.3258, bf16[512]{0} %add.50, bf16[1,512,768]{2,1,0} %reshape.6768), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.6777 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.6776), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.6780 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.6777), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.6824 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.3239), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6825 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.6824), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.6781 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.6780, bf16[4,128,768]{2,1,0} %select.19), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.6800 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.6781), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6819 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6800, bf16[768,768]{1,0} %p145.3112), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6821 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.6819), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6822 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.6821), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6823 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.6822), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6826 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.6825, bf16[48,128,64]{2,1,0} %reshape.6823), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6848 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6826), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6849 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6848), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6851 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6849), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6994 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6851, bf16[768,768]{1,0} %p147.3121), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6995 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6994), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6997 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.6780, bf16[4,128,768]{2,1,0} %reshape.6995), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.6883 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3211), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6884 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.6883), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.6865 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.3131), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6866 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.6865), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.6867 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.6823, bf16[48,64,128]{1,2,0} %transpose.6866), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6868 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.6867), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.6869 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.6868, bf16[4,12,128,128]{3,2,1,0} %select.20), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.6870 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.6869, bf16[4,12,128,128]{3,2,1,0} %divide.3238), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.6871 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.6876 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.6870, bf16[] %constant.6871), dimensions={3}, to_apply=%AddComputation.6872, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.6877 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.6876), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.6878 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.6869, bf16[4,12,128,128]{3,2,1,0} %broadcast.6877), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.6879 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.3238, bf16[4,12,128,128]{3,2,1,0} %subtract.6878), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.6880 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.6881 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.6879, bf16[4,12,128,128]{3,2,1,0} %broadcast.6880), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.6882 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.6881), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6885 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.6884, bf16[48,128,128]{2,1,0} %reshape.6882), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6908 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.6885), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6910 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.6908), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6912 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.6910), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6984 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6912, bf16[768,768]{1,0} %p149.3180), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6985 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6984), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6998 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6997, bf16[4,128,768]{2,1,0} %reshape.6985), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.6926 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.3191), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6927 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.6926), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.6928 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.6882, bf16[48,128,64]{1,2,0} %transpose.6927), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.6950 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6928), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6951 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6950), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6953 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6951), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.6974 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.6953, bf16[768,768]{1,0} %p151.3201), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.6975 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.6974), metadata={op_type="aten__view" op_name="aten__view"}
  %add.6999 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.6998, bf16[4,128,768]{2,1,0} %reshape.6975), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.7054 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p21.475), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7055 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6999, bf16[4,128,768]{2,1,0} %broadcast.7054), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7056 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7055), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.7064 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.3053, bf16[512]{0} %broadcast.7047, bf16[512]{0} %get-tuple-element.3056, bf16[512]{0} %add.46, bf16[1,512,768]{2,1,0} %reshape.7056), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.7065 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.7064), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.7068 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.7065), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.7069 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.7068, bf16[4,128,768]{2,1,0} %select.21), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7088 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7069), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7104 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.7088, bf16[768,3072]{1,0} %p140.3019), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7105 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.7104), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.50 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.3034), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.26 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.7105, bf16[4,128,3072]{2,1,0} %custom-call.50), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.7134 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.26), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7155 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.7134, bf16[3072,768]{1,0} %p142.3028), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7156 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7155), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7158 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.7068, bf16[4,128,768]{2,1,0} %reshape.7156), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.7213 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p22.502), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7214 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7158, bf16[4,128,768]{2,1,0} %broadcast.7213), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7215 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7214), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.7223 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2960, bf16[512]{0} %broadcast.7206, bf16[512]{0} %get-tuple-element.2963, bf16[512]{0} %add.44, bf16[1,512,768]{2,1,0} %reshape.7215), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.7224 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.7223), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.7227 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.7224), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.7271 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2944), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7272 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.7271), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.7228 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.7227, bf16[4,128,768]{2,1,0} %select.22), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7247 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7228), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7266 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7247, bf16[768,768]{1,0} %p131.2817), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7268 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.7266), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7269 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.7268), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7270 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.7269), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7273 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.7272, bf16[48,128,64]{2,1,0} %reshape.7270), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7295 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7273), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7296 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7295), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7298 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7296), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7441 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7298, bf16[768,768]{1,0} %p133.2826), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7442 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7441), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7444 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.7227, bf16[4,128,768]{2,1,0} %reshape.7442), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.7330 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2916), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7331 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.7330), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.7312 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2836), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7313 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.7312), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.7314 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.7270, bf16[48,64,128]{1,2,0} %transpose.7313), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7315 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.7314), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.7316 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.7315, bf16[4,12,128,128]{3,2,1,0} %select.23), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7317 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.7316, bf16[4,12,128,128]{3,2,1,0} %divide.2943), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.7318 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.7323 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.7317, bf16[] %constant.7318), dimensions={3}, to_apply=%AddComputation.7319, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.7324 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.7323), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.7325 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.7316, bf16[4,12,128,128]{3,2,1,0} %broadcast.7324), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.7326 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2943, bf16[4,12,128,128]{3,2,1,0} %subtract.7325), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.7327 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.7328 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.7326, bf16[4,12,128,128]{3,2,1,0} %broadcast.7327), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.7329 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.7328), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7332 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.7331, bf16[48,128,128]{2,1,0} %reshape.7329), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7355 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.7332), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7357 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.7355), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7359 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.7357), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7431 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7359, bf16[768,768]{1,0} %p135.2885), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7432 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7431), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7445 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.7444, bf16[4,128,768]{2,1,0} %reshape.7432), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.7373 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2896), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7374 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.7373), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.7375 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.7329, bf16[48,128,64]{1,2,0} %transpose.7374), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7397 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7375), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7398 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7397), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7400 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7398), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7421 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7400, bf16[768,768]{1,0} %p137.2906), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7422 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7421), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7446 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.7445, bf16[4,128,768]{2,1,0} %reshape.7422), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.7501 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p23.529), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7502 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7446, bf16[4,128,768]{2,1,0} %broadcast.7501), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7503 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7502), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.7511 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2758, bf16[512]{0} %broadcast.7494, bf16[512]{0} %get-tuple-element.2761, bf16[512]{0} %add.40, bf16[1,512,768]{2,1,0} %reshape.7503), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.7512 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.7511), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.7515 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.7512), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.7516 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.7515, bf16[4,128,768]{2,1,0} %select.24), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7535 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7516), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7551 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.7535, bf16[768,3072]{1,0} %p126.2724), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7552 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.7551), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.51 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2739), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.27 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.7552, bf16[4,128,3072]{2,1,0} %custom-call.51), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.7581 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.27), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7602 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.7581, bf16[3072,768]{1,0} %p128.2733), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7603 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7602), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7605 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.7515, bf16[4,128,768]{2,1,0} %reshape.7603), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.7660 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p24.556), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7661 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7605, bf16[4,128,768]{2,1,0} %broadcast.7660), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7662 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7661), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.7670 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2665, bf16[512]{0} %broadcast.7653, bf16[512]{0} %get-tuple-element.2668, bf16[512]{0} %add.37, bf16[1,512,768]{2,1,0} %reshape.7662), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.7671 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.7670), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.7674 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.7671), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.7718 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2649), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7719 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.7718), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.7675 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.7674, bf16[4,128,768]{2,1,0} %select.25), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7694 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7675), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7713 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7694, bf16[768,768]{1,0} %p117.2522), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7715 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.7713), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7716 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.7715), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7717 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.7716), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7720 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.7719, bf16[48,128,64]{2,1,0} %reshape.7717), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7742 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7720), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7743 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7742), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7745 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7743), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7888 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7745, bf16[768,768]{1,0} %p119.2531), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7889 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7888), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7891 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.7674, bf16[4,128,768]{2,1,0} %reshape.7889), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.7777 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2621), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7778 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.7777), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.7759 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2541), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7760 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.7759), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.7761 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.7717, bf16[48,64,128]{1,2,0} %transpose.7760), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7762 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.7761), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.7763 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.7762, bf16[4,12,128,128]{3,2,1,0} %select.26), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7764 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.7763, bf16[4,12,128,128]{3,2,1,0} %divide.2648), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.7765 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.7770 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.7764, bf16[] %constant.7765), dimensions={3}, to_apply=%AddComputation.7766, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.7771 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.7770), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.7772 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.7763, bf16[4,12,128,128]{3,2,1,0} %broadcast.7771), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.7773 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2648, bf16[4,12,128,128]{3,2,1,0} %subtract.7772), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.7774 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.7775 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.7773, bf16[4,12,128,128]{3,2,1,0} %broadcast.7774), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.7776 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.7775), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7779 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.7778, bf16[48,128,128]{2,1,0} %reshape.7776), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7802 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.7779), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7804 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.7802), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7806 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.7804), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7878 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7806, bf16[768,768]{1,0} %p121.2590), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7879 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7878), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7892 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.7891, bf16[4,128,768]{2,1,0} %reshape.7879), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.7820 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2601), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7821 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.7820), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.7822 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.7776, bf16[48,128,64]{1,2,0} %transpose.7821), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.7844 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7822), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7845 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7844), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7847 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7845), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7868 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.7847, bf16[768,768]{1,0} %p123.2611), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7869 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.7868), metadata={op_type="aten__view" op_name="aten__view"}
  %add.7893 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.7892, bf16[4,128,768]{2,1,0} %reshape.7869), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.7948 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p25.583), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.7949 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7893, bf16[4,128,768]{2,1,0} %broadcast.7948), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7950 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7949), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.7958 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2463, bf16[512]{0} %broadcast.7941, bf16[512]{0} %get-tuple-element.2466, bf16[512]{0} %add.33, bf16[1,512,768]{2,1,0} %reshape.7950), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.7959 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.7958), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.7962 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.7959), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.7963 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.7962, bf16[4,128,768]{2,1,0} %select.27), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.7982 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.7963), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.7998 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.7982, bf16[768,3072]{1,0} %p112.2429), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.7999 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.7998), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.52 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2444), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.28 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.7999, bf16[4,128,3072]{2,1,0} %custom-call.52), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.8028 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.28), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8049 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.8028, bf16[3072,768]{1,0} %p114.2438), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8050 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8049), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8052 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.7962, bf16[4,128,768]{2,1,0} %reshape.8050), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.8107 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p26.610), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8108 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8052, bf16[4,128,768]{2,1,0} %broadcast.8107), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8109 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8108), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.8117 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2370, bf16[512]{0} %broadcast.8100, bf16[512]{0} %get-tuple-element.2373, bf16[512]{0} %add.30, bf16[1,512,768]{2,1,0} %reshape.8109), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.8118 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.8117), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.8121 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.8118), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.8165 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2354), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8166 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.8165), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.8122 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.8121, bf16[4,128,768]{2,1,0} %select.28), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8141 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8122), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8160 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8141, bf16[768,768]{1,0} %p103.2227), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8162 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.8160), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8163 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.8162), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8164 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.8163), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8167 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.8166, bf16[48,128,64]{2,1,0} %reshape.8164), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8189 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8167), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8190 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8189), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8192 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8190), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8335 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8192, bf16[768,768]{1,0} %p105.2236), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8336 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8335), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8338 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.8121, bf16[4,128,768]{2,1,0} %reshape.8336), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.8224 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2326), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8225 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.8224), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.8206 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2246), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8207 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.8206), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.8208 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.8164, bf16[48,64,128]{1,2,0} %transpose.8207), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8209 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.8208), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.8210 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.8209, bf16[4,12,128,128]{3,2,1,0} %select.29), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8211 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.8210, bf16[4,12,128,128]{3,2,1,0} %divide.2353), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.8212 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.8217 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.8211, bf16[] %constant.8212), dimensions={3}, to_apply=%AddComputation.8213, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.8218 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.8217), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.8219 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.8210, bf16[4,12,128,128]{3,2,1,0} %broadcast.8218), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.8220 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2353, bf16[4,12,128,128]{3,2,1,0} %subtract.8219), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.8221 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.8222 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.8220, bf16[4,12,128,128]{3,2,1,0} %broadcast.8221), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.8223 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.8222), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8226 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.8225, bf16[48,128,128]{2,1,0} %reshape.8223), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8249 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.8226), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8251 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.8249), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8253 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.8251), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8325 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8253, bf16[768,768]{1,0} %p107.2295), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8326 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8325), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8339 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.8338, bf16[4,128,768]{2,1,0} %reshape.8326), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.8267 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2306), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8268 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.8267), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.8269 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.8223, bf16[48,128,64]{1,2,0} %transpose.8268), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8291 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8269), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8292 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8291), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8294 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8292), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8315 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8294, bf16[768,768]{1,0} %p109.2316), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8316 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8315), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8340 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.8339, bf16[4,128,768]{2,1,0} %reshape.8316), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.8395 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p27.637), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8396 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8340, bf16[4,128,768]{2,1,0} %broadcast.8395), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8397 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8396), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.8405 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2168, bf16[512]{0} %broadcast.8388, bf16[512]{0} %get-tuple-element.2171, bf16[512]{0} %add.26, bf16[1,512,768]{2,1,0} %reshape.8397), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.8406 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.8405), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.8409 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.8406), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.8410 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.8409, bf16[4,128,768]{2,1,0} %select.30), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8429 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8410), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8445 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.8429, bf16[768,3072]{1,0} %p98.2134), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8446 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.8445), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.53 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.2149), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.30 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.8446, bf16[4,128,3072]{2,1,0} %custom-call.53), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.8475 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.30), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8496 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.8475, bf16[3072,768]{1,0} %p100.2143), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8497 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8496), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8499 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.8409, bf16[4,128,768]{2,1,0} %reshape.8497), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.8554 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p28.664), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8555 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8499, bf16[4,128,768]{2,1,0} %broadcast.8554), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8556 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8555), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.8564 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.2075, bf16[512]{0} %broadcast.8547, bf16[512]{0} %get-tuple-element.2078, bf16[512]{0} %add.24, bf16[1,512,768]{2,1,0} %reshape.8556), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.8565 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.8564), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.8568 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.8565), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.8612 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.2059), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8613 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.8612), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.8569 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.8568, bf16[4,128,768]{2,1,0} %select.31), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8588 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8569), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8607 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8588, bf16[768,768]{1,0} %p89.1932), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8609 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.8607), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8610 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.8609), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8611 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.8610), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8614 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.8613, bf16[48,128,64]{2,1,0} %reshape.8611), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8636 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8614), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8637 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8636), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8639 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8637), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8782 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8639, bf16[768,768]{1,0} %p91.1941), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8783 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8782), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8785 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.8568, bf16[4,128,768]{2,1,0} %reshape.8783), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.8671 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.2031), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8672 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.8671), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.8653 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1951), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8654 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.8653), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.8655 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.8611, bf16[48,64,128]{1,2,0} %transpose.8654), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8656 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.8655), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.8657 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.8656, bf16[4,12,128,128]{3,2,1,0} %select.32), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8658 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.8657, bf16[4,12,128,128]{3,2,1,0} %divide.2058), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.8659 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.8664 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.8658, bf16[] %constant.8659), dimensions={3}, to_apply=%AddComputation.8660, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.8665 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.8664), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.8666 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.8657, bf16[4,12,128,128]{3,2,1,0} %broadcast.8665), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.8667 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.2058, bf16[4,12,128,128]{3,2,1,0} %subtract.8666), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.8668 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.8669 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.8667, bf16[4,12,128,128]{3,2,1,0} %broadcast.8668), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.8670 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.8669), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8673 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.8672, bf16[48,128,128]{2,1,0} %reshape.8670), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8696 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.8673), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8698 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.8696), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8700 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.8698), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8772 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8700, bf16[768,768]{1,0} %p93.2000), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8773 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8772), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8786 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.8785, bf16[4,128,768]{2,1,0} %reshape.8773), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.8714 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.2011), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8715 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.8714), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.8716 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.8670, bf16[48,128,64]{1,2,0} %transpose.8715), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.8738 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8716), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8739 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8738), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8741 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8739), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8762 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.8741, bf16[768,768]{1,0} %p95.2021), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8763 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8762), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8787 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.8786, bf16[4,128,768]{2,1,0} %reshape.8763), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.8842 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p29.691), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.8843 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8787, bf16[4,128,768]{2,1,0} %broadcast.8842), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8844 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8843), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.8852 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1873, bf16[512]{0} %broadcast.8835, bf16[512]{0} %get-tuple-element.1876, bf16[512]{0} %add.20, bf16[1,512,768]{2,1,0} %reshape.8844), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.8853 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.8852), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.8856 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.8853), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.8857 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.8856, bf16[4,128,768]{2,1,0} %select.33), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.8876 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.8857), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8892 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.8876, bf16[768,3072]{1,0} %p84.1839), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8893 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.8892), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.54 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1854), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.31 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.8893, bf16[4,128,3072]{2,1,0} %custom-call.54), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.8922 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.31), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.8943 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.8922, bf16[3072,768]{1,0} %p86.1848), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.8944 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.8943), metadata={op_type="aten__view" op_name="aten__view"}
  %add.8946 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.8856, bf16[4,128,768]{2,1,0} %reshape.8944), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.9001 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p30.718), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9002 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8946, bf16[4,128,768]{2,1,0} %broadcast.9001), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9003 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9002), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.9011 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1780, bf16[512]{0} %broadcast.8994, bf16[512]{0} %get-tuple-element.1783, bf16[512]{0} %add.17, bf16[1,512,768]{2,1,0} %reshape.9003), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.9012 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.9011), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.9015 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.9012), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.9059 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1764), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9060 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.9059), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.9016 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.9015, bf16[4,128,768]{2,1,0} %select.34), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9035 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9016), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9054 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9035, bf16[768,768]{1,0} %p75.1637), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9056 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.9054), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9057 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.9056), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9058 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.9057), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9061 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.9060, bf16[48,128,64]{2,1,0} %reshape.9058), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9083 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9061), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9084 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9083), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9086 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9084), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9229 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9086, bf16[768,768]{1,0} %p77.1646), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9230 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9229), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9232 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.9015, bf16[4,128,768]{2,1,0} %reshape.9230), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.9118 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1736), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9119 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.9118), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.9100 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1656), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9101 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.9100), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.9102 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.9058, bf16[48,64,128]{1,2,0} %transpose.9101), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9103 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.9102), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.9104 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.9103, bf16[4,12,128,128]{3,2,1,0} %select.35), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9105 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.9104, bf16[4,12,128,128]{3,2,1,0} %divide.1763), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.9106 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.9111 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.9105, bf16[] %constant.9106), dimensions={3}, to_apply=%AddComputation.9107, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.9112 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.9111), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.9113 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.9104, bf16[4,12,128,128]{3,2,1,0} %broadcast.9112), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.9114 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1763, bf16[4,12,128,128]{3,2,1,0} %subtract.9113), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.9115 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.9116 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.9114, bf16[4,12,128,128]{3,2,1,0} %broadcast.9115), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.9117 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.9116), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9120 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.9119, bf16[48,128,128]{2,1,0} %reshape.9117), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9143 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.9120), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9145 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.9143), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9147 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.9145), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9219 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9147, bf16[768,768]{1,0} %p79.1705), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9220 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9219), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9233 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.9232, bf16[4,128,768]{2,1,0} %reshape.9220), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.9161 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1716), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9162 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.9161), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.9163 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.9117, bf16[48,128,64]{1,2,0} %transpose.9162), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9185 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9163), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9186 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9185), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9188 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9186), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9209 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9188, bf16[768,768]{1,0} %p81.1726), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9210 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9209), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9234 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.9233, bf16[4,128,768]{2,1,0} %reshape.9210), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.9289 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p31.745), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9290 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9234, bf16[4,128,768]{2,1,0} %broadcast.9289), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9291 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9290), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.9299 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1578, bf16[512]{0} %broadcast.9282, bf16[512]{0} %get-tuple-element.1581, bf16[512]{0} %add.14, bf16[1,512,768]{2,1,0} %reshape.9291), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.9300 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.9299), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.9303 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.9300), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.9304 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.9303, bf16[4,128,768]{2,1,0} %select.36), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9323 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9304), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9339 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.9323, bf16[768,3072]{1,0} %p70.1544), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9340 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.9339), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.55 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1559), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.32 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.9340, bf16[4,128,3072]{2,1,0} %custom-call.55), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.9369 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.32), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9390 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.9369, bf16[3072,768]{1,0} %p72.1553), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9391 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9390), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9393 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.9303, bf16[4,128,768]{2,1,0} %reshape.9391), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.9448 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p32.772), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9449 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9393, bf16[4,128,768]{2,1,0} %broadcast.9448), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9450 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9449), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.9458 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1485, bf16[512]{0} %broadcast.9441, bf16[512]{0} %get-tuple-element.1488, bf16[512]{0} %add.12, bf16[1,512,768]{2,1,0} %reshape.9450), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.9459 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.9458), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.9462 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.9459), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.9506 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1469), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9507 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.9506), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.9463 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.9462, bf16[4,128,768]{2,1,0} %select.37), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9482 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9463), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9501 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9482, bf16[768,768]{1,0} %p61.1342), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9503 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.9501), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9504 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.9503), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9505 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.9504), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9508 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.9507, bf16[48,128,64]{2,1,0} %reshape.9505), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9530 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9508), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9531 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9530), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9533 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9531), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9676 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9533, bf16[768,768]{1,0} %p63.1351), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9677 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9676), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9679 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.9462, bf16[4,128,768]{2,1,0} %reshape.9677), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.9565 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1441), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9566 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.9565), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.9547 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1361), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9548 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.9547), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.9549 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.9505, bf16[48,64,128]{1,2,0} %transpose.9548), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9550 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.9549), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.9551 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.9550, bf16[4,12,128,128]{3,2,1,0} %select.38), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9552 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.9551, bf16[4,12,128,128]{3,2,1,0} %divide.1468), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.9553 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.9558 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.9552, bf16[] %constant.9553), dimensions={3}, to_apply=%AddComputation.9554, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.9559 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.9558), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.9560 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.9551, bf16[4,12,128,128]{3,2,1,0} %broadcast.9559), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.9561 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1468, bf16[4,12,128,128]{3,2,1,0} %subtract.9560), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.9562 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.9563 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.9561, bf16[4,12,128,128]{3,2,1,0} %broadcast.9562), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.9564 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.9563), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9567 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.9566, bf16[48,128,128]{2,1,0} %reshape.9564), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9590 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.9567), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9592 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.9590), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9594 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.9592), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9666 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9594, bf16[768,768]{1,0} %p65.1410), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9667 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9666), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9680 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.9679, bf16[4,128,768]{2,1,0} %reshape.9667), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.9608 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1421), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9609 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.9608), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.9610 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.9564, bf16[48,128,64]{1,2,0} %transpose.9609), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9632 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9610), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9633 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9632), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9635 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9633), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9656 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9635, bf16[768,768]{1,0} %p67.1431), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9657 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9656), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9681 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.9680, bf16[4,128,768]{2,1,0} %reshape.9657), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.9736 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p33.799), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9737 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9681, bf16[4,128,768]{2,1,0} %broadcast.9736), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9738 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9737), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.9746 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1283, bf16[512]{0} %broadcast.9729, bf16[512]{0} %get-tuple-element.1286, bf16[512]{0} %add.9, bf16[1,512,768]{2,1,0} %reshape.9738), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.9747 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.9746), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.9750 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.9747), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.9751 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.9750, bf16[4,128,768]{2,1,0} %select.39), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9770 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9751), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9786 = bf16[512,3072]{1,0} dot(bf16[512,768]{1,0} %reshape.9770, bf16[768,3072]{1,0} %p56.1249), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9787 = bf16[4,128,3072]{2,1,0} reshape(bf16[512,3072]{1,0} %dot.9786), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.56 = bf16[4,128,3072]{2,1,0} custom-call(bf16[4,128,3072]{2,1,0} %add.1264), custom_call_target="AwsNeuronGeluBackward", api_version=API_VERSION_UNSPECIFIED
  %multiply.34 = bf16[4,128,3072]{2,1,0} multiply(bf16[4,128,3072]{2,1,0} %reshape.9787, bf16[4,128,3072]{2,1,0} %custom-call.56), metadata={op_type="xla___op_GeluBackwardImpl" op_name="xla___op_GeluBackwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.9816 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %multiply.34), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9837 = bf16[512,768]{1,0} dot(bf16[512,3072]{1,0} %reshape.9816, bf16[3072,768]{1,0} %p58.1258), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9838 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.9837), metadata={op_type="aten__view" op_name="aten__view"}
  %add.9840 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.9750, bf16[4,128,768]{2,1,0} %reshape.9838), metadata={op_type="aten__add" op_name="aten__add"}
  %broadcast.9895 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p34.826), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9896 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9840, bf16[4,128,768]{2,1,0} %broadcast.9895), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9897 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9896), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.9905 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.1190, bf16[512]{0} %broadcast.9888, bf16[512]{0} %get-tuple-element.1193, bf16[512]{0} %add.7, bf16[1,512,768]{2,1,0} %reshape.9897), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.9906 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.9905), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.9909 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.9906), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.9953 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %multiply.1174), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9954 = bf16[48,128,128]{1,2,0} transpose(bf16[48,128,128]{2,1,0} %reshape.9953), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %multiply.9910 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %reshape.9909, bf16[4,128,768]{2,1,0} %select.40), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.9929 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.9910), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9948 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9929, bf16[768,768]{1,0} %p44.1025), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.9950 = bf16[4,128,12,64]{3,2,1,0} reshape(bf16[512,768]{1,0} %dot.9948), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9951 = bf16[4,12,128,64]{3,1,2,0} transpose(bf16[4,128,12,64]{3,2,1,0} %reshape.9950), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9952 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.9951), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.9955 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{1,2,0} %transpose.9954, bf16[48,128,64]{2,1,0} %reshape.9952), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9977 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9955), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9978 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9977), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9980 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9978), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.10123 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.9980, bf16[768,768]{1,0} %p46.1034), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.10124 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.10123), metadata={op_type="aten__view" op_name="aten__view"}
  %add.10126 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %reshape.9909, bf16[4,128,768]{2,1,0} %reshape.10124), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.10012 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1146), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10013 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.10012), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.9994 = bf16[48,128,64]{2,1,0} reshape(bf16[4,12,128,64]{3,1,2,0} %transpose.1044), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9995 = bf16[48,64,128]{1,2,0} transpose(bf16[48,128,64]{2,1,0} %reshape.9994), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.9996 = bf16[48,128,128]{2,1,0} dot(bf16[48,128,64]{2,1,0} %reshape.9952, bf16[48,64,128]{1,2,0} %transpose.9995), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.9997 = bf16[4,12,128,128]{3,2,1,0} reshape(bf16[48,128,128]{2,1,0} %dot.9996), metadata={op_type="aten__view" op_name="aten__view"}
  %multiply.9998 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %reshape.9997, bf16[4,12,128,128]{3,2,1,0} %select.41), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.9999 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %multiply.9998, bf16[4,12,128,128]{3,2,1,0} %divide.1173), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %constant.10000 = bf16[] constant(0), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %reduce.10005 = bf16[4,12,128]{2,1,0} reduce(bf16[4,12,128,128]{3,2,1,0} %multiply.9999, bf16[] %constant.10000), dimensions={3}, to_apply=%AddComputation.10001, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.10006 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[4,12,128]{2,1,0} %reduce.10005), dimensions={0,1,2}, metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %subtract.10007 = bf16[4,12,128,128]{3,2,1,0} subtract(bf16[4,12,128,128]{3,2,1,0} %multiply.9998, bf16[4,12,128,128]{3,2,1,0} %broadcast.10006), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %multiply.10008 = bf16[4,12,128,128]{3,2,1,0} multiply(bf16[4,12,128,128]{3,2,1,0} %divide.1173, bf16[4,12,128,128]{3,2,1,0} %subtract.10007), metadata={op_type="aten___softmax_backward_data" op_name="aten___softmax_backward_data"}
  %broadcast.10009 = bf16[4,12,128,128]{3,2,1,0} broadcast(bf16[] %p49.1107), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.10010 = bf16[4,12,128,128]{3,2,1,0} divide(bf16[4,12,128,128]{3,2,1,0} %multiply.10008, bf16[4,12,128,128]{3,2,1,0} %broadcast.10009), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.10011 = bf16[48,128,128]{2,1,0} reshape(bf16[4,12,128,128]{3,2,1,0} %divide.10010), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.10014 = bf16[48,64,128]{2,1,0} dot(bf16[48,64,128]{1,2,0} %transpose.10013, bf16[48,128,128]{2,1,0} %reshape.10011), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.10037 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.10014), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10039 = bf16[4,128,12,64]{1,3,2,0} transpose(bf16[4,12,64,128]{3,2,1,0} %reshape.10037), dimensions={0,3,1,2}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.10041 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{1,3,2,0} %transpose.10039), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.10113 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.10041, bf16[768,768]{1,0} %p51.1115), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.10114 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.10113), metadata={op_type="aten__view" op_name="aten__view"}
  %add.10127 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.10126, bf16[4,128,768]{2,1,0} %reshape.10114), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.10055 = bf16[48,64,128]{2,1,0} reshape(bf16[4,12,64,128]{2,1,3,0} %transpose.1126), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10056 = bf16[48,128,64]{1,2,0} transpose(bf16[48,64,128]{2,1,0} %reshape.10055), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %dot.10057 = bf16[48,128,64]{2,1,0} dot(bf16[48,128,128]{2,1,0} %reshape.10011, bf16[48,128,64]{1,2,0} %transpose.10056), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul"}
  %reshape.10079 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.10057), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10080 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.10079), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.10082 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.10080), metadata={op_type="aten__view" op_name="aten__view"}
  %dot.10103 = bf16[512,768]{1,0} dot(bf16[512,768]{1,0} %reshape.10082, bf16[768,768]{1,0} %p53.1136), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.10104 = bf16[4,128,768]{2,1,0} reshape(bf16[512,768]{1,0} %dot.10103), metadata={op_type="aten__view" op_name="aten__view"}
  %add.10128 = bf16[4,128,768]{2,1,0} add(bf16[4,128,768]{2,1,0} %add.10127, bf16[4,128,768]{2,1,0} %reshape.10104), metadata={op_type="aten__add" op_name="aten__add"}
  %multiply.10129 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.10128, bf16[4,128,768]{2,1,0} %select.42), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.10184 = bf16[4,128,768]{2,1,0} broadcast(bf16[768]{0} %p35.886), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.10185 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %multiply.10129, bf16[4,128,768]{2,1,0} %broadcast.10184), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.10186 = bf16[1,512,768]{2,1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.10185), metadata={op_type="aten__view" op_name="aten__view"}
  %batch-norm-grad.10194 = (bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) batch-norm-grad(bf16[1,512,768]{2,1,0} %reshape.965, bf16[512]{0} %broadcast.10177, bf16[512]{0} %get-tuple-element.968, bf16[512]{0} %add.3, bf16[1,512,768]{2,1,0} %reshape.10186), epsilon=1e-12, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.10195 = bf16[1,512,768]{2,1,0} get-tuple-element((bf16[1,512,768]{2,1,0}, bf16[512]{0}, bf16[512]{0}) %batch-norm-grad.10194), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %reshape.10291 = bf16[512,768]{1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.10195), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.10285 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.10289 = bf16[512,768]{1,0} broadcast(bf16[] %constant.10285), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %select.10301 = bf16[512,768]{1,0} select(pred[512,768]{1,0} %broadcast.10300, bf16[512,768]{1,0} %reshape.10291, bf16[512,768]{1,0} %broadcast.10289), metadata={op_type="aten__where" op_name="aten__where"}
  %scatter.10324 = bf16[28996,768]{1,0} scatter(bf16[28996,768]{1,0} %broadcast.10318, s64[512,1]{1,0} %reshape.10312, bf16[512,768]{1,0} %select.10301), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%ScatterCombiner.10320, metadata={op_type="aten__index_put" op_name="aten__index_put"}
  %custom-call.57 = bf16[28996,768]{1,0} custom-call(bf16[28996,768]{1,0} %scatter.10324), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.10554 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=49}
  %multiply.10333 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %custom-call.57, bf16[28996,768]{1,0} %custom-call.57), metadata={op_type="aten__mul" op_name="aten__norm.1/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10334 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10340 = bf16[] reduce(bf16[28996,768]{1,0} %multiply.10333, bf16[] %constant.10334), dimensions={0,1}, to_apply=%AddComputation.10336, metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10341 = bf16[] sqrt(bf16[] %reduce.10340), metadata={op_type="aten__sqrt" op_name="aten__norm.1/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10342 = bf16[1]{0} reshape(bf16[] %sqrt.10341), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.10 = bf16[] constant(0)
  %broadcast.17 = bf16[512,768]{1,0} broadcast(bf16[] %constant.10), dimensions={}
  %slice.10238 = s64[1,128]{1,0} slice(s64[1,512]{1,0} %p36.912), slice={[0:1], [0:128]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %reshape.6 = s64[128]{0} reshape(s64[1,128]{1,0} %slice.10238)
  %constant.11 = s64[] constant(0)
  %broadcast.18 = s64[128]{0} broadcast(s64[] %constant.11), dimensions={}
  %compare.3 = pred[128]{0} compare(s64[128]{0} %reshape.6, s64[128]{0} %broadcast.18), direction=GE
  %constant.12 = s64[] constant(512)
  %broadcast.19 = s64[128]{0} broadcast(s64[] %constant.12), dimensions={}
  %add.1 = s64[128]{0} add(s64[128]{0} %reshape.6, s64[128]{0} %broadcast.19)
  %select.2 = s64[128]{0} select(pred[128]{0} %compare.3, s64[128]{0} %reshape.6, s64[128]{0} %add.1)
  %reshape.7 = s64[128,1]{1,0} reshape(s64[128]{0} %select.2)
  %reshape.10198 = bf16[4,128,768]{2,1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.10195), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.10239 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.10245 = bf16[128,768]{1,0} reduce(bf16[4,128,768]{2,1,0} %reshape.10198, bf16[] %constant.10239), dimensions={0}, to_apply=%AddComputation.10241, metadata={op_type="aten__sum" op_name="aten__sum"}
  %scatter.0 = bf16[512,768]{1,0} scatter(bf16[512,768]{1,0} %broadcast.17, s64[128,1]{1,0} %reshape.7, bf16[128,768]{1,0} %reduce.10245), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%Int32PermissiveEmbeddingScatterCombiner.10247, metadata={op_type="xla___op_Int32PermissiveEmbeddingGradWeight" op_name="xla___op_Int32PermissiveEmbeddingGradWeight" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.58 = bf16[512,768]{1,0} custom-call(bf16[512,768]{1,0} %scatter.0), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %multiply.10276 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %custom-call.58, bf16[512,768]{1,0} %custom-call.58), metadata={op_type="aten__mul" op_name="aten__norm.2/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10277 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10283 = bf16[] reduce(bf16[512,768]{1,0} %multiply.10276, bf16[] %constant.10277), dimensions={0,1}, to_apply=%AddComputation.10279, metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10284 = bf16[] sqrt(bf16[] %reduce.10283), metadata={op_type="aten__sqrt" op_name="aten__norm.2/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10343 = bf16[1]{0} reshape(bf16[] %sqrt.10284), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.13 = bf16[] constant(0)
  %broadcast.20 = bf16[2,768]{1,0} broadcast(bf16[] %constant.13), dimensions={}
  %reshape.9 = s64[512]{0} reshape(s64[4,128]{1,0} %p38.933)
  %constant.14 = s64[] constant(0)
  %broadcast.21 = s64[512]{0} broadcast(s64[] %constant.14), dimensions={}
  %compare.4 = pred[512]{0} compare(s64[512]{0} %reshape.9, s64[512]{0} %broadcast.21), direction=GE
  %constant.15 = s64[] constant(2)
  %broadcast.22 = s64[512]{0} broadcast(s64[] %constant.15), dimensions={}
  %add.2 = s64[512]{0} add(s64[512]{0} %reshape.9, s64[512]{0} %broadcast.22)
  %select.3 = s64[512]{0} select(pred[512]{0} %compare.4, s64[512]{0} %reshape.9, s64[512]{0} %add.2)
  %reshape.10 = s64[512,1]{1,0} reshape(s64[512]{0} %select.3)
  %reshape.11 = bf16[512,768]{1,0} reshape(bf16[1,512,768]{2,1,0} %get-tuple-element.10195)
  %scatter.1 = bf16[2,768]{1,0} scatter(bf16[2,768]{1,0} %broadcast.20, s64[512,1]{1,0} %reshape.10, bf16[512,768]{1,0} %reshape.11), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%Int32PermissiveEmbeddingScatterCombiner.10199, metadata={op_type="xla___op_Int32PermissiveEmbeddingGradWeight" op_name="xla___op_Int32PermissiveEmbeddingGradWeight" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %custom-call.59 = bf16[2,768]{1,0} custom-call(bf16[2,768]{1,0} %scatter.1), custom_call_target="AwsNeuronTransferWithStaticRing", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_TransferWithStaticRingTransfer" op_name="xla___op_TransferWithStaticRingTransfer" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %multiply.10228 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %custom-call.59, bf16[2,768]{1,0} %custom-call.59), metadata={op_type="aten__mul" op_name="aten__norm.3/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10229 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10235 = bf16[] reduce(bf16[2,768]{1,0} %multiply.10228, bf16[] %constant.10229), dimensions={0,1}, to_apply=%AddComputation.10231, metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10236 = bf16[] sqrt(bf16[] %reduce.10235), metadata={op_type="aten__sqrt" op_name="aten__norm.3/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10344 = bf16[1]{0} reshape(bf16[] %sqrt.10236), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.10154 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %multiply.10129, bf16[4,128,768]{2,1,0} %reshape.974), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.10155 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.10161 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.10154, bf16[] %constant.10155), dimensions={0,1}, to_apply=%AddComputation.10157, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.10164 = bf16[768]{0} multiply(bf16[768]{0} %reduce.10161, bf16[768]{0} %reduce.10161), metadata={op_type="aten__mul" op_name="aten__norm.4/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10165 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10171 = bf16[] reduce(bf16[768]{0} %multiply.10164, bf16[] %constant.10165), dimensions={0}, to_apply=%AddComputation.10167, metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10172 = bf16[] sqrt(bf16[] %reduce.10171), metadata={op_type="aten__sqrt" op_name="aten__norm.4/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10345 = bf16[1]{0} reshape(bf16[] %sqrt.10172), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.10130 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.10136 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.10129, bf16[] %constant.10130), dimensions={0,1}, to_apply=%AddComputation.10132, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.10139 = bf16[768]{0} multiply(bf16[768]{0} %reduce.10136, bf16[768]{0} %reduce.10136), metadata={op_type="aten__mul" op_name="aten__norm.5/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10140 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10146 = bf16[] reduce(bf16[768]{0} %multiply.10139, bf16[] %constant.10140), dimensions={0}, to_apply=%AddComputation.10142, metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10147 = bf16[] sqrt(bf16[] %reduce.10146), metadata={op_type="aten__sqrt" op_name="aten__norm.5/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10346 = bf16[1]{0} reshape(bf16[] %sqrt.10147), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.10083 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10084 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.10083), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.10082, bf16[768,512]{0,1} %transpose.10084), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.10087 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot, bf16[768,768]{0,1} %dot), metadata={op_type="aten__mul" op_name="aten__norm.6/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10088 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10094 = bf16[] reduce(bf16[768,768]{0,1} %multiply.10087, bf16[] %constant.10088), dimensions={0,1}, to_apply=%AddComputation.10090, metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10095 = bf16[] sqrt(bf16[] %reduce.10094), metadata={op_type="aten__sqrt" op_name="aten__norm.6/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10347 = bf16[1]{0} reshape(bf16[] %sqrt.10095), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.10058 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.10057), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.10061 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.10058, bf16[] %constant.10061), dimensions={0,2}, to_apply=%AddComputation.10063, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.245 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.6, bf16[12,64]{1,0} %reduce.6), metadata={op_type="aten__mul" op_name="aten__norm.7/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10071 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10077 = bf16[] reduce(bf16[12,64]{1,0} %multiply.245, bf16[] %constant.10071), dimensions={0,1}, to_apply=%AddComputation.10073, metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10078 = bf16[] sqrt(bf16[] %reduce.10077), metadata={op_type="aten__sqrt" op_name="aten__norm.7/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10348 = bf16[1]{0} reshape(bf16[] %sqrt.10078), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.10042 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.10043 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.10042), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.1 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.10041, bf16[768,512]{0,1} %transpose.10043), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.10046 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.1, bf16[768,768]{0,1} %dot.1), metadata={op_type="aten__mul" op_name="aten__norm.8/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10047 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10053 = bf16[] reduce(bf16[768,768]{0,1} %multiply.10046, bf16[] %constant.10047), dimensions={0,1}, to_apply=%AddComputation.10049, metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10054 = bf16[] sqrt(bf16[] %reduce.10053), metadata={op_type="aten__sqrt" op_name="aten__norm.8/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10349 = bf16[1]{0} reshape(bf16[] %sqrt.10054), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.10015 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.10014), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.10019 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.10015, bf16[] %constant.10019), dimensions={0,3}, to_apply=%AddComputation.10021, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.244 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.7, bf16[12,64]{1,0} %reduce.7), metadata={op_type="aten__mul" op_name="aten__norm.9/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10029 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10035 = bf16[] reduce(bf16[12,64]{1,0} %multiply.244, bf16[] %constant.10029), dimensions={0,1}, to_apply=%AddComputation.10031, metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10036 = bf16[] sqrt(bf16[] %reduce.10035), metadata={op_type="aten__sqrt" op_name="aten__norm.9/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10350 = bf16[1]{0} reshape(bf16[] %sqrt.10036), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9981 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %multiply.984), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9982 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9981), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.2 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9980, bf16[768,512]{0,1} %transpose.9982), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9985 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.2, bf16[768,768]{0,1} %dot.2), metadata={op_type="aten__mul" op_name="aten__norm.10/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9986 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9992 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9985, bf16[] %constant.9986), dimensions={0,1}, to_apply=%AddComputation.9988, metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9993 = bf16[] sqrt(bf16[] %reduce.9992), metadata={op_type="aten__sqrt" op_name="aten__norm.10/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10351 = bf16[1]{0} reshape(bf16[] %sqrt.9993), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9956 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9955), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9959 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.9956, bf16[] %constant.9959), dimensions={0,2}, to_apply=%AddComputation.9961, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.243 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.8, bf16[12,64]{1,0} %reduce.8), metadata={op_type="aten__mul" op_name="aten__norm.11/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9969 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9975 = bf16[] reduce(bf16[12,64]{1,0} %multiply.243, bf16[] %constant.9969), dimensions={0,1}, to_apply=%AddComputation.9971, metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9976 = bf16[] sqrt(bf16[] %reduce.9975), metadata={op_type="aten__sqrt" op_name="aten__norm.11/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10352 = bf16[1]{0} reshape(bf16[] %sqrt.9976), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9930 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1177), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9931 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9930), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9933 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9931), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9934 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9933), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.3 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9929, bf16[768,512]{0,1} %transpose.9934), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9937 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.3, bf16[768,768]{0,1} %dot.3), metadata={op_type="aten__mul" op_name="aten__norm.12/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9938 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9944 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9937, bf16[] %constant.9938), dimensions={0,1}, to_apply=%AddComputation.9940, metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9945 = bf16[] sqrt(bf16[] %reduce.9944), metadata={op_type="aten__sqrt" op_name="aten__norm.12/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10353 = bf16[1]{0} reshape(bf16[] %sqrt.9945), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9911 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9917 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9910, bf16[] %constant.9911), dimensions={0,1}, to_apply=%AddComputation.9913, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9920 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9917, bf16[768]{0} %reduce.9917), metadata={op_type="aten__mul" op_name="aten__norm.13/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9921 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9927 = bf16[] reduce(bf16[768]{0} %multiply.9920, bf16[] %constant.9921), dimensions={0}, to_apply=%AddComputation.9923, metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9928 = bf16[] sqrt(bf16[] %reduce.9927), metadata={op_type="aten__sqrt" op_name="aten__norm.13/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10354 = bf16[1]{0} reshape(bf16[] %sqrt.9928), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.9865 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9840, bf16[4,128,768]{2,1,0} %reshape.1199), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.9866 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9872 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9865, bf16[] %constant.9866), dimensions={0,1}, to_apply=%AddComputation.9868, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9875 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9872, bf16[768]{0} %reduce.9872), metadata={op_type="aten__mul" op_name="aten__norm.14/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9876 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9882 = bf16[] reduce(bf16[768]{0} %multiply.9875, bf16[] %constant.9876), dimensions={0}, to_apply=%AddComputation.9878, metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9883 = bf16[] sqrt(bf16[] %reduce.9882), metadata={op_type="aten__sqrt" op_name="aten__norm.14/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10355 = bf16[1]{0} reshape(bf16[] %sqrt.9883), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9841 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9847 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.9840, bf16[] %constant.9841), dimensions={0,1}, to_apply=%AddComputation.9843, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9850 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9847, bf16[768]{0} %reduce.9847), metadata={op_type="aten__mul" op_name="aten__norm.15/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9851 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9857 = bf16[] reduce(bf16[768]{0} %multiply.9850, bf16[] %constant.9851), dimensions={0}, to_apply=%AddComputation.9853, metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9858 = bf16[] sqrt(bf16[] %reduce.9857), metadata={op_type="aten__sqrt" op_name="aten__norm.15/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10356 = bf16[1]{0} reshape(bf16[] %sqrt.9858), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9817 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1207), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9818 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9817), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.4 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.9816, bf16[768,512]{0,1} %transpose.9818), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9821 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.4, bf16[3072,768]{0,1} %dot.4), metadata={op_type="aten__mul" op_name="aten__norm.16/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9822 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9828 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.9821, bf16[] %constant.9822), dimensions={0,1}, to_apply=%AddComputation.9824, metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9829 = bf16[] sqrt(bf16[] %reduce.9828), metadata={op_type="aten__sqrt" op_name="aten__norm.16/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10357 = bf16[1]{0} reshape(bf16[] %sqrt.9829), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9798 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9804 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.34, bf16[] %constant.9798), dimensions={0,1}, to_apply=%AddComputation.9800, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9807 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.9804, bf16[3072]{0} %reduce.9804), metadata={op_type="aten__mul" op_name="aten__norm.17/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9808 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9814 = bf16[] reduce(bf16[3072]{0} %multiply.9807, bf16[] %constant.9808), dimensions={0}, to_apply=%AddComputation.9810, metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9815 = bf16[] sqrt(bf16[] %reduce.9814), metadata={op_type="aten__sqrt" op_name="aten__norm.17/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10358 = bf16[1]{0} reshape(bf16[] %sqrt.9815), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9771 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.33), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9772 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.9771), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.5 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.9770, bf16[3072,512]{0,1} %transpose.9772), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9775 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.5, bf16[768,3072]{0,1} %dot.5), metadata={op_type="aten__mul" op_name="aten__norm.18/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9776 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9782 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.9775, bf16[] %constant.9776), dimensions={0,1}, to_apply=%AddComputation.9778, metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9783 = bf16[] sqrt(bf16[] %reduce.9782), metadata={op_type="aten__sqrt" op_name="aten__norm.18/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10359 = bf16[1]{0} reshape(bf16[] %sqrt.9783), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9752 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9758 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9751, bf16[] %constant.9752), dimensions={0,1}, to_apply=%AddComputation.9754, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9761 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9758, bf16[768]{0} %reduce.9758), metadata={op_type="aten__mul" op_name="aten__norm.19/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9762 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9768 = bf16[] reduce(bf16[768]{0} %multiply.9761, bf16[] %constant.9762), dimensions={0}, to_apply=%AddComputation.9764, metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9769 = bf16[] sqrt(bf16[] %reduce.9768), metadata={op_type="aten__sqrt" op_name="aten__norm.19/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10360 = bf16[1]{0} reshape(bf16[] %sqrt.9769), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.9706 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9681, bf16[4,128,768]{2,1,0} %reshape.1292), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.9707 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9713 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9706, bf16[] %constant.9707), dimensions={0,1}, to_apply=%AddComputation.9709, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9716 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9713, bf16[768]{0} %reduce.9713), metadata={op_type="aten__mul" op_name="aten__norm.20/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9717 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9723 = bf16[] reduce(bf16[768]{0} %multiply.9716, bf16[] %constant.9717), dimensions={0}, to_apply=%AddComputation.9719, metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9724 = bf16[] sqrt(bf16[] %reduce.9723), metadata={op_type="aten__sqrt" op_name="aten__norm.20/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10361 = bf16[1]{0} reshape(bf16[] %sqrt.9724), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9682 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9688 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.9681, bf16[] %constant.9682), dimensions={0,1}, to_apply=%AddComputation.9684, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9691 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9688, bf16[768]{0} %reduce.9688), metadata={op_type="aten__mul" op_name="aten__norm.21/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9692 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9698 = bf16[] reduce(bf16[768]{0} %multiply.9691, bf16[] %constant.9692), dimensions={0}, to_apply=%AddComputation.9694, metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9699 = bf16[] sqrt(bf16[] %reduce.9698), metadata={op_type="aten__sqrt" op_name="aten__norm.21/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10362 = bf16[1]{0} reshape(bf16[] %sqrt.9699), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9636 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9637 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9636), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.6 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9635, bf16[768,512]{0,1} %transpose.9637), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9640 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.6, bf16[768,768]{0,1} %dot.6), metadata={op_type="aten__mul" op_name="aten__norm.22/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9641 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9647 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9640, bf16[] %constant.9641), dimensions={0,1}, to_apply=%AddComputation.9643, metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9648 = bf16[] sqrt(bf16[] %reduce.9647), metadata={op_type="aten__sqrt" op_name="aten__norm.22/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10363 = bf16[1]{0} reshape(bf16[] %sqrt.9648), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9611 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9610), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9614 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.9611, bf16[] %constant.9614), dimensions={0,2}, to_apply=%AddComputation.9616, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.242 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.9, bf16[12,64]{1,0} %reduce.9), metadata={op_type="aten__mul" op_name="aten__norm.23/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9624 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9630 = bf16[] reduce(bf16[12,64]{1,0} %multiply.242, bf16[] %constant.9624), dimensions={0,1}, to_apply=%AddComputation.9626, metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9631 = bf16[] sqrt(bf16[] %reduce.9630), metadata={op_type="aten__sqrt" op_name="aten__norm.23/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10364 = bf16[1]{0} reshape(bf16[] %sqrt.9631), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9595 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9596 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9595), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.7 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9594, bf16[768,512]{0,1} %transpose.9596), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9599 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.7, bf16[768,768]{0,1} %dot.7), metadata={op_type="aten__mul" op_name="aten__norm.24/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9600 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9606 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9599, bf16[] %constant.9600), dimensions={0,1}, to_apply=%AddComputation.9602, metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9607 = bf16[] sqrt(bf16[] %reduce.9606), metadata={op_type="aten__sqrt" op_name="aten__norm.24/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10365 = bf16[1]{0} reshape(bf16[] %sqrt.9607), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9568 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.9567), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9572 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.10 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.9568, bf16[] %constant.9572), dimensions={0,3}, to_apply=%AddComputation.9574, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.241 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.10, bf16[12,64]{1,0} %reduce.10), metadata={op_type="aten__mul" op_name="aten__norm.25/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9582 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9588 = bf16[] reduce(bf16[12,64]{1,0} %multiply.241, bf16[] %constant.9582), dimensions={0,1}, to_apply=%AddComputation.9584, metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9589 = bf16[] sqrt(bf16[] %reduce.9588), metadata={op_type="aten__sqrt" op_name="aten__norm.25/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10366 = bf16[1]{0} reshape(bf16[] %sqrt.9589), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9534 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1300), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9535 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9534), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.8 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9533, bf16[768,512]{0,1} %transpose.9535), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9538 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.8, bf16[768,768]{0,1} %dot.8), metadata={op_type="aten__mul" op_name="aten__norm.26/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9539 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9545 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9538, bf16[] %constant.9539), dimensions={0,1}, to_apply=%AddComputation.9541, metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9546 = bf16[] sqrt(bf16[] %reduce.9545), metadata={op_type="aten__sqrt" op_name="aten__norm.26/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10367 = bf16[1]{0} reshape(bf16[] %sqrt.9546), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9509 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9508), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9512 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.11 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.9509, bf16[] %constant.9512), dimensions={0,2}, to_apply=%AddComputation.9514, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.240 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.11, bf16[12,64]{1,0} %reduce.11), metadata={op_type="aten__mul" op_name="aten__norm.27/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9522 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9528 = bf16[] reduce(bf16[12,64]{1,0} %multiply.240, bf16[] %constant.9522), dimensions={0,1}, to_apply=%AddComputation.9524, metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9529 = bf16[] sqrt(bf16[] %reduce.9528), metadata={op_type="aten__sqrt" op_name="aten__norm.27/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10368 = bf16[1]{0} reshape(bf16[] %sqrt.9529), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9483 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1472), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9484 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9483), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9486 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9484), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9487 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9486), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.9 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9482, bf16[768,512]{0,1} %transpose.9487), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9490 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.9, bf16[768,768]{0,1} %dot.9), metadata={op_type="aten__mul" op_name="aten__norm.28/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9491 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9497 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9490, bf16[] %constant.9491), dimensions={0,1}, to_apply=%AddComputation.9493, metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9498 = bf16[] sqrt(bf16[] %reduce.9497), metadata={op_type="aten__sqrt" op_name="aten__norm.28/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10369 = bf16[1]{0} reshape(bf16[] %sqrt.9498), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9464 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9470 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9463, bf16[] %constant.9464), dimensions={0,1}, to_apply=%AddComputation.9466, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9473 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9470, bf16[768]{0} %reduce.9470), metadata={op_type="aten__mul" op_name="aten__norm.29/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9474 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9480 = bf16[] reduce(bf16[768]{0} %multiply.9473, bf16[] %constant.9474), dimensions={0}, to_apply=%AddComputation.9476, metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9481 = bf16[] sqrt(bf16[] %reduce.9480), metadata={op_type="aten__sqrt" op_name="aten__norm.29/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10370 = bf16[1]{0} reshape(bf16[] %sqrt.9481), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.9418 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9393, bf16[4,128,768]{2,1,0} %reshape.1494), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.9419 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9425 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9418, bf16[] %constant.9419), dimensions={0,1}, to_apply=%AddComputation.9421, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9428 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9425, bf16[768]{0} %reduce.9425), metadata={op_type="aten__mul" op_name="aten__norm.30/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9429 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9435 = bf16[] reduce(bf16[768]{0} %multiply.9428, bf16[] %constant.9429), dimensions={0}, to_apply=%AddComputation.9431, metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9436 = bf16[] sqrt(bf16[] %reduce.9435), metadata={op_type="aten__sqrt" op_name="aten__norm.30/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10371 = bf16[1]{0} reshape(bf16[] %sqrt.9436), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9394 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9400 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.9393, bf16[] %constant.9394), dimensions={0,1}, to_apply=%AddComputation.9396, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9403 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9400, bf16[768]{0} %reduce.9400), metadata={op_type="aten__mul" op_name="aten__norm.31/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9404 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9410 = bf16[] reduce(bf16[768]{0} %multiply.9403, bf16[] %constant.9404), dimensions={0}, to_apply=%AddComputation.9406, metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9411 = bf16[] sqrt(bf16[] %reduce.9410), metadata={op_type="aten__sqrt" op_name="aten__norm.31/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10372 = bf16[1]{0} reshape(bf16[] %sqrt.9411), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9370 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1502), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9371 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9370), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.10 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.9369, bf16[768,512]{0,1} %transpose.9371), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9374 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.10, bf16[3072,768]{0,1} %dot.10), metadata={op_type="aten__mul" op_name="aten__norm.32/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9375 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.32/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9381 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.9374, bf16[] %constant.9375), dimensions={0,1}, to_apply=%AddComputation.9377, metadata={op_type="aten__sum" op_name="aten__norm.32/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9382 = bf16[] sqrt(bf16[] %reduce.9381), metadata={op_type="aten__sqrt" op_name="aten__norm.32/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10373 = bf16[1]{0} reshape(bf16[] %sqrt.9382), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9351 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9357 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.32, bf16[] %constant.9351), dimensions={0,1}, to_apply=%AddComputation.9353, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9360 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.9357, bf16[3072]{0} %reduce.9357), metadata={op_type="aten__mul" op_name="aten__norm.33/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9361 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.33/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9367 = bf16[] reduce(bf16[3072]{0} %multiply.9360, bf16[] %constant.9361), dimensions={0}, to_apply=%AddComputation.9363, metadata={op_type="aten__sum" op_name="aten__norm.33/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9368 = bf16[] sqrt(bf16[] %reduce.9367), metadata={op_type="aten__sqrt" op_name="aten__norm.33/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10374 = bf16[1]{0} reshape(bf16[] %sqrt.9368), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9324 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.34), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9325 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.9324), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.11 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.9323, bf16[3072,512]{0,1} %transpose.9325), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9328 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.11, bf16[768,3072]{0,1} %dot.11), metadata={op_type="aten__mul" op_name="aten__norm.34/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9329 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.34/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9335 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.9328, bf16[] %constant.9329), dimensions={0,1}, to_apply=%AddComputation.9331, metadata={op_type="aten__sum" op_name="aten__norm.34/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9336 = bf16[] sqrt(bf16[] %reduce.9335), metadata={op_type="aten__sqrt" op_name="aten__norm.34/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10375 = bf16[1]{0} reshape(bf16[] %sqrt.9336), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9305 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9311 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9304, bf16[] %constant.9305), dimensions={0,1}, to_apply=%AddComputation.9307, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9314 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9311, bf16[768]{0} %reduce.9311), metadata={op_type="aten__mul" op_name="aten__norm.35/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9315 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.35/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9321 = bf16[] reduce(bf16[768]{0} %multiply.9314, bf16[] %constant.9315), dimensions={0}, to_apply=%AddComputation.9317, metadata={op_type="aten__sum" op_name="aten__norm.35/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9322 = bf16[] sqrt(bf16[] %reduce.9321), metadata={op_type="aten__sqrt" op_name="aten__norm.35/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10376 = bf16[1]{0} reshape(bf16[] %sqrt.9322), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.9259 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.9234, bf16[4,128,768]{2,1,0} %reshape.1587), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.9260 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9266 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9259, bf16[] %constant.9260), dimensions={0,1}, to_apply=%AddComputation.9262, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9269 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9266, bf16[768]{0} %reduce.9266), metadata={op_type="aten__mul" op_name="aten__norm.36/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9270 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.36/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9276 = bf16[] reduce(bf16[768]{0} %multiply.9269, bf16[] %constant.9270), dimensions={0}, to_apply=%AddComputation.9272, metadata={op_type="aten__sum" op_name="aten__norm.36/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9277 = bf16[] sqrt(bf16[] %reduce.9276), metadata={op_type="aten__sqrt" op_name="aten__norm.36/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10377 = bf16[1]{0} reshape(bf16[] %sqrt.9277), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9235 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9241 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.9234, bf16[] %constant.9235), dimensions={0,1}, to_apply=%AddComputation.9237, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9244 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9241, bf16[768]{0} %reduce.9241), metadata={op_type="aten__mul" op_name="aten__norm.37/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9245 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.37/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9251 = bf16[] reduce(bf16[768]{0} %multiply.9244, bf16[] %constant.9245), dimensions={0}, to_apply=%AddComputation.9247, metadata={op_type="aten__sum" op_name="aten__norm.37/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9252 = bf16[] sqrt(bf16[] %reduce.9251), metadata={op_type="aten__sqrt" op_name="aten__norm.37/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10378 = bf16[1]{0} reshape(bf16[] %sqrt.9252), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9189 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9190 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9189), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.12 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9188, bf16[768,512]{0,1} %transpose.9190), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9193 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.12, bf16[768,768]{0,1} %dot.12), metadata={op_type="aten__mul" op_name="aten__norm.38/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9194 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.38/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9200 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9193, bf16[] %constant.9194), dimensions={0,1}, to_apply=%AddComputation.9196, metadata={op_type="aten__sum" op_name="aten__norm.38/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9201 = bf16[] sqrt(bf16[] %reduce.9200), metadata={op_type="aten__sqrt" op_name="aten__norm.38/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10379 = bf16[1]{0} reshape(bf16[] %sqrt.9201), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9164 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9163), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9167 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.12 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.9164, bf16[] %constant.9167), dimensions={0,2}, to_apply=%AddComputation.9169, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.239 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.12, bf16[12,64]{1,0} %reduce.12), metadata={op_type="aten__mul" op_name="aten__norm.39/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9177 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.39/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9183 = bf16[] reduce(bf16[12,64]{1,0} %multiply.239, bf16[] %constant.9177), dimensions={0,1}, to_apply=%AddComputation.9179, metadata={op_type="aten__sum" op_name="aten__norm.39/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9184 = bf16[] sqrt(bf16[] %reduce.9183), metadata={op_type="aten__sqrt" op_name="aten__norm.39/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10380 = bf16[1]{0} reshape(bf16[] %sqrt.9184), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9148 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9149 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9148), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.13 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9147, bf16[768,512]{0,1} %transpose.9149), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9152 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.13, bf16[768,768]{0,1} %dot.13), metadata={op_type="aten__mul" op_name="aten__norm.40/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9153 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.40/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9159 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9152, bf16[] %constant.9153), dimensions={0,1}, to_apply=%AddComputation.9155, metadata={op_type="aten__sum" op_name="aten__norm.40/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9160 = bf16[] sqrt(bf16[] %reduce.9159), metadata={op_type="aten__sqrt" op_name="aten__norm.40/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10381 = bf16[1]{0} reshape(bf16[] %sqrt.9160), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9121 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.9120), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9125 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.13 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.9121, bf16[] %constant.9125), dimensions={0,3}, to_apply=%AddComputation.9127, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.238 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.13, bf16[12,64]{1,0} %reduce.13), metadata={op_type="aten__mul" op_name="aten__norm.41/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9135 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.41/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9141 = bf16[] reduce(bf16[12,64]{1,0} %multiply.238, bf16[] %constant.9135), dimensions={0,1}, to_apply=%AddComputation.9137, metadata={op_type="aten__sum" op_name="aten__norm.41/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9142 = bf16[] sqrt(bf16[] %reduce.9141), metadata={op_type="aten__sqrt" op_name="aten__norm.41/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10382 = bf16[1]{0} reshape(bf16[] %sqrt.9142), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9087 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1595), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9088 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9087), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.14 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9086, bf16[768,512]{0,1} %transpose.9088), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9091 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.14, bf16[768,768]{0,1} %dot.14), metadata={op_type="aten__mul" op_name="aten__norm.42/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9092 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.42/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9098 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9091, bf16[] %constant.9092), dimensions={0,1}, to_apply=%AddComputation.9094, metadata={op_type="aten__sum" op_name="aten__norm.42/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9099 = bf16[] sqrt(bf16[] %reduce.9098), metadata={op_type="aten__sqrt" op_name="aten__norm.42/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10383 = bf16[1]{0} reshape(bf16[] %sqrt.9099), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9062 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.9061), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.9065 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.14 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.9062, bf16[] %constant.9065), dimensions={0,2}, to_apply=%AddComputation.9067, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.237 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.14, bf16[12,64]{1,0} %reduce.14), metadata={op_type="aten__mul" op_name="aten__norm.43/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9075 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.43/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9081 = bf16[] reduce(bf16[12,64]{1,0} %multiply.237, bf16[] %constant.9075), dimensions={0,1}, to_apply=%AddComputation.9077, metadata={op_type="aten__sum" op_name="aten__norm.43/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9082 = bf16[] sqrt(bf16[] %reduce.9081), metadata={op_type="aten__sqrt" op_name="aten__norm.43/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10384 = bf16[1]{0} reshape(bf16[] %sqrt.9082), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.9036 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.1767), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9037 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.9036), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.9039 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.9037), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.9040 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.9039), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.15 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.9035, bf16[768,512]{0,1} %transpose.9040), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.9043 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.15, bf16[768,768]{0,1} %dot.15), metadata={op_type="aten__mul" op_name="aten__norm.44/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9044 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.44/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9050 = bf16[] reduce(bf16[768,768]{0,1} %multiply.9043, bf16[] %constant.9044), dimensions={0,1}, to_apply=%AddComputation.9046, metadata={op_type="aten__sum" op_name="aten__norm.44/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9051 = bf16[] sqrt(bf16[] %reduce.9050), metadata={op_type="aten__sqrt" op_name="aten__norm.44/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10385 = bf16[1]{0} reshape(bf16[] %sqrt.9051), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.9017 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.9023 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.9016, bf16[] %constant.9017), dimensions={0,1}, to_apply=%AddComputation.9019, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.9026 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9023, bf16[768]{0} %reduce.9023), metadata={op_type="aten__mul" op_name="aten__norm.45/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.9027 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.45/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.9033 = bf16[] reduce(bf16[768]{0} %multiply.9026, bf16[] %constant.9027), dimensions={0}, to_apply=%AddComputation.9029, metadata={op_type="aten__sum" op_name="aten__norm.45/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.9034 = bf16[] sqrt(bf16[] %reduce.9033), metadata={op_type="aten__sqrt" op_name="aten__norm.45/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10386 = bf16[1]{0} reshape(bf16[] %sqrt.9034), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.8971 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8946, bf16[4,128,768]{2,1,0} %reshape.1789), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.8972 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8978 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8971, bf16[] %constant.8972), dimensions={0,1}, to_apply=%AddComputation.8974, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8981 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8978, bf16[768]{0} %reduce.8978), metadata={op_type="aten__mul" op_name="aten__norm.46/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8982 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.46/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8988 = bf16[] reduce(bf16[768]{0} %multiply.8981, bf16[] %constant.8982), dimensions={0}, to_apply=%AddComputation.8984, metadata={op_type="aten__sum" op_name="aten__norm.46/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8989 = bf16[] sqrt(bf16[] %reduce.8988), metadata={op_type="aten__sqrt" op_name="aten__norm.46/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10387 = bf16[1]{0} reshape(bf16[] %sqrt.8989), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8947 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8953 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.8946, bf16[] %constant.8947), dimensions={0,1}, to_apply=%AddComputation.8949, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8956 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8953, bf16[768]{0} %reduce.8953), metadata={op_type="aten__mul" op_name="aten__norm.47/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8957 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.47/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8963 = bf16[] reduce(bf16[768]{0} %multiply.8956, bf16[] %constant.8957), dimensions={0}, to_apply=%AddComputation.8959, metadata={op_type="aten__sum" op_name="aten__norm.47/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8964 = bf16[] sqrt(bf16[] %reduce.8963), metadata={op_type="aten__sqrt" op_name="aten__norm.47/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10388 = bf16[1]{0} reshape(bf16[] %sqrt.8964), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8923 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1797), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8924 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8923), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.16 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.8922, bf16[768,512]{0,1} %transpose.8924), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8927 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.16, bf16[3072,768]{0,1} %dot.16), metadata={op_type="aten__mul" op_name="aten__norm.48/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8928 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.48/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8934 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.8927, bf16[] %constant.8928), dimensions={0,1}, to_apply=%AddComputation.8930, metadata={op_type="aten__sum" op_name="aten__norm.48/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8935 = bf16[] sqrt(bf16[] %reduce.8934), metadata={op_type="aten__sqrt" op_name="aten__norm.48/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10389 = bf16[1]{0} reshape(bf16[] %sqrt.8935), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8904 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8910 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.31, bf16[] %constant.8904), dimensions={0,1}, to_apply=%AddComputation.8906, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8913 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8910, bf16[3072]{0} %reduce.8910), metadata={op_type="aten__mul" op_name="aten__norm.49/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8914 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.49/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8920 = bf16[] reduce(bf16[3072]{0} %multiply.8913, bf16[] %constant.8914), dimensions={0}, to_apply=%AddComputation.8916, metadata={op_type="aten__sum" op_name="aten__norm.49/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8921 = bf16[] sqrt(bf16[] %reduce.8920), metadata={op_type="aten__sqrt" op_name="aten__norm.49/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10390 = bf16[1]{0} reshape(bf16[] %sqrt.8921), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8877 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.35), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8878 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.8877), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.17 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.8876, bf16[3072,512]{0,1} %transpose.8878), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8881 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.17, bf16[768,3072]{0,1} %dot.17), metadata={op_type="aten__mul" op_name="aten__norm.50/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8882 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.50/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8888 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.8881, bf16[] %constant.8882), dimensions={0,1}, to_apply=%AddComputation.8884, metadata={op_type="aten__sum" op_name="aten__norm.50/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8889 = bf16[] sqrt(bf16[] %reduce.8888), metadata={op_type="aten__sqrt" op_name="aten__norm.50/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10391 = bf16[1]{0} reshape(bf16[] %sqrt.8889), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8858 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8864 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8857, bf16[] %constant.8858), dimensions={0,1}, to_apply=%AddComputation.8860, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8867 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8864, bf16[768]{0} %reduce.8864), metadata={op_type="aten__mul" op_name="aten__norm.51/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8868 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.51/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8874 = bf16[] reduce(bf16[768]{0} %multiply.8867, bf16[] %constant.8868), dimensions={0}, to_apply=%AddComputation.8870, metadata={op_type="aten__sum" op_name="aten__norm.51/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8875 = bf16[] sqrt(bf16[] %reduce.8874), metadata={op_type="aten__sqrt" op_name="aten__norm.51/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10392 = bf16[1]{0} reshape(bf16[] %sqrt.8875), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.8812 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8787, bf16[4,128,768]{2,1,0} %reshape.1882), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.8813 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8819 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8812, bf16[] %constant.8813), dimensions={0,1}, to_apply=%AddComputation.8815, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8822 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8819, bf16[768]{0} %reduce.8819), metadata={op_type="aten__mul" op_name="aten__norm.52/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8823 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.52/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8829 = bf16[] reduce(bf16[768]{0} %multiply.8822, bf16[] %constant.8823), dimensions={0}, to_apply=%AddComputation.8825, metadata={op_type="aten__sum" op_name="aten__norm.52/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8830 = bf16[] sqrt(bf16[] %reduce.8829), metadata={op_type="aten__sqrt" op_name="aten__norm.52/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10393 = bf16[1]{0} reshape(bf16[] %sqrt.8830), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8788 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8794 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.8787, bf16[] %constant.8788), dimensions={0,1}, to_apply=%AddComputation.8790, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8797 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8794, bf16[768]{0} %reduce.8794), metadata={op_type="aten__mul" op_name="aten__norm.53/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8798 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.53/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8804 = bf16[] reduce(bf16[768]{0} %multiply.8797, bf16[] %constant.8798), dimensions={0}, to_apply=%AddComputation.8800, metadata={op_type="aten__sum" op_name="aten__norm.53/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8805 = bf16[] sqrt(bf16[] %reduce.8804), metadata={op_type="aten__sqrt" op_name="aten__norm.53/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10394 = bf16[1]{0} reshape(bf16[] %sqrt.8805), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8742 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8743 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8742), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.18 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8741, bf16[768,512]{0,1} %transpose.8743), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8746 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.18, bf16[768,768]{0,1} %dot.18), metadata={op_type="aten__mul" op_name="aten__norm.54/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8747 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.54/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8753 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8746, bf16[] %constant.8747), dimensions={0,1}, to_apply=%AddComputation.8749, metadata={op_type="aten__sum" op_name="aten__norm.54/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8754 = bf16[] sqrt(bf16[] %reduce.8753), metadata={op_type="aten__sqrt" op_name="aten__norm.54/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10395 = bf16[1]{0} reshape(bf16[] %sqrt.8754), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8717 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8716), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8720 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.15 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.8717, bf16[] %constant.8720), dimensions={0,2}, to_apply=%AddComputation.8722, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.236 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.15, bf16[12,64]{1,0} %reduce.15), metadata={op_type="aten__mul" op_name="aten__norm.55/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8730 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.55/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8736 = bf16[] reduce(bf16[12,64]{1,0} %multiply.236, bf16[] %constant.8730), dimensions={0,1}, to_apply=%AddComputation.8732, metadata={op_type="aten__sum" op_name="aten__norm.55/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8737 = bf16[] sqrt(bf16[] %reduce.8736), metadata={op_type="aten__sqrt" op_name="aten__norm.55/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10396 = bf16[1]{0} reshape(bf16[] %sqrt.8737), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8701 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8702 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8701), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.19 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8700, bf16[768,512]{0,1} %transpose.8702), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8705 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.19, bf16[768,768]{0,1} %dot.19), metadata={op_type="aten__mul" op_name="aten__norm.56/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8706 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.56/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8712 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8705, bf16[] %constant.8706), dimensions={0,1}, to_apply=%AddComputation.8708, metadata={op_type="aten__sum" op_name="aten__norm.56/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8713 = bf16[] sqrt(bf16[] %reduce.8712), metadata={op_type="aten__sqrt" op_name="aten__norm.56/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10397 = bf16[1]{0} reshape(bf16[] %sqrt.8713), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8674 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.8673), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8678 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.16 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.8674, bf16[] %constant.8678), dimensions={0,3}, to_apply=%AddComputation.8680, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.235 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.16, bf16[12,64]{1,0} %reduce.16), metadata={op_type="aten__mul" op_name="aten__norm.57/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8688 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.57/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8694 = bf16[] reduce(bf16[12,64]{1,0} %multiply.235, bf16[] %constant.8688), dimensions={0,1}, to_apply=%AddComputation.8690, metadata={op_type="aten__sum" op_name="aten__norm.57/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8695 = bf16[] sqrt(bf16[] %reduce.8694), metadata={op_type="aten__sqrt" op_name="aten__norm.57/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10398 = bf16[1]{0} reshape(bf16[] %sqrt.8695), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8640 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.1890), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8641 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8640), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.20 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8639, bf16[768,512]{0,1} %transpose.8641), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8644 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.20, bf16[768,768]{0,1} %dot.20), metadata={op_type="aten__mul" op_name="aten__norm.58/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8645 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.58/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8651 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8644, bf16[] %constant.8645), dimensions={0,1}, to_apply=%AddComputation.8647, metadata={op_type="aten__sum" op_name="aten__norm.58/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8652 = bf16[] sqrt(bf16[] %reduce.8651), metadata={op_type="aten__sqrt" op_name="aten__norm.58/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10399 = bf16[1]{0} reshape(bf16[] %sqrt.8652), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8615 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8614), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8618 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.17 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.8615, bf16[] %constant.8618), dimensions={0,2}, to_apply=%AddComputation.8620, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.234 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.17, bf16[12,64]{1,0} %reduce.17), metadata={op_type="aten__mul" op_name="aten__norm.59/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8628 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.59/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8634 = bf16[] reduce(bf16[12,64]{1,0} %multiply.234, bf16[] %constant.8628), dimensions={0,1}, to_apply=%AddComputation.8630, metadata={op_type="aten__sum" op_name="aten__norm.59/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8635 = bf16[] sqrt(bf16[] %reduce.8634), metadata={op_type="aten__sqrt" op_name="aten__norm.59/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10400 = bf16[1]{0} reshape(bf16[] %sqrt.8635), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8589 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2062), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8590 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8589), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8592 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8590), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8593 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8592), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.21 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8588, bf16[768,512]{0,1} %transpose.8593), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8596 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.21, bf16[768,768]{0,1} %dot.21), metadata={op_type="aten__mul" op_name="aten__norm.60/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8597 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.60/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8603 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8596, bf16[] %constant.8597), dimensions={0,1}, to_apply=%AddComputation.8599, metadata={op_type="aten__sum" op_name="aten__norm.60/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8604 = bf16[] sqrt(bf16[] %reduce.8603), metadata={op_type="aten__sqrt" op_name="aten__norm.60/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10401 = bf16[1]{0} reshape(bf16[] %sqrt.8604), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8570 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8576 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8569, bf16[] %constant.8570), dimensions={0,1}, to_apply=%AddComputation.8572, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8579 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8576, bf16[768]{0} %reduce.8576), metadata={op_type="aten__mul" op_name="aten__norm.61/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8580 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.61/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8586 = bf16[] reduce(bf16[768]{0} %multiply.8579, bf16[] %constant.8580), dimensions={0}, to_apply=%AddComputation.8582, metadata={op_type="aten__sum" op_name="aten__norm.61/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8587 = bf16[] sqrt(bf16[] %reduce.8586), metadata={op_type="aten__sqrt" op_name="aten__norm.61/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10402 = bf16[1]{0} reshape(bf16[] %sqrt.8587), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.8524 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8499, bf16[4,128,768]{2,1,0} %reshape.2084), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.8525 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8531 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8524, bf16[] %constant.8525), dimensions={0,1}, to_apply=%AddComputation.8527, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8534 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8531, bf16[768]{0} %reduce.8531), metadata={op_type="aten__mul" op_name="aten__norm.62/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8535 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.62/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8541 = bf16[] reduce(bf16[768]{0} %multiply.8534, bf16[] %constant.8535), dimensions={0}, to_apply=%AddComputation.8537, metadata={op_type="aten__sum" op_name="aten__norm.62/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8542 = bf16[] sqrt(bf16[] %reduce.8541), metadata={op_type="aten__sqrt" op_name="aten__norm.62/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10403 = bf16[1]{0} reshape(bf16[] %sqrt.8542), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8500 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8506 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.8499, bf16[] %constant.8500), dimensions={0,1}, to_apply=%AddComputation.8502, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8509 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8506, bf16[768]{0} %reduce.8506), metadata={op_type="aten__mul" op_name="aten__norm.63/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8510 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.63/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8516 = bf16[] reduce(bf16[768]{0} %multiply.8509, bf16[] %constant.8510), dimensions={0}, to_apply=%AddComputation.8512, metadata={op_type="aten__sum" op_name="aten__norm.63/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8517 = bf16[] sqrt(bf16[] %reduce.8516), metadata={op_type="aten__sqrt" op_name="aten__norm.63/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10404 = bf16[1]{0} reshape(bf16[] %sqrt.8517), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8476 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2092), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8477 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8476), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.22 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.8475, bf16[768,512]{0,1} %transpose.8477), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8480 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.22, bf16[3072,768]{0,1} %dot.22), metadata={op_type="aten__mul" op_name="aten__norm.64/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8481 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.64/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8487 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.8480, bf16[] %constant.8481), dimensions={0,1}, to_apply=%AddComputation.8483, metadata={op_type="aten__sum" op_name="aten__norm.64/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8488 = bf16[] sqrt(bf16[] %reduce.8487), metadata={op_type="aten__sqrt" op_name="aten__norm.64/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10405 = bf16[1]{0} reshape(bf16[] %sqrt.8488), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8457 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8463 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.30, bf16[] %constant.8457), dimensions={0,1}, to_apply=%AddComputation.8459, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8466 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8463, bf16[3072]{0} %reduce.8463), metadata={op_type="aten__mul" op_name="aten__norm.65/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8467 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.65/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8473 = bf16[] reduce(bf16[3072]{0} %multiply.8466, bf16[] %constant.8467), dimensions={0}, to_apply=%AddComputation.8469, metadata={op_type="aten__sum" op_name="aten__norm.65/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8474 = bf16[] sqrt(bf16[] %reduce.8473), metadata={op_type="aten__sqrt" op_name="aten__norm.65/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10406 = bf16[1]{0} reshape(bf16[] %sqrt.8474), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8430 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.36), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8431 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.8430), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.23 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.8429, bf16[3072,512]{0,1} %transpose.8431), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8434 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.23, bf16[768,3072]{0,1} %dot.23), metadata={op_type="aten__mul" op_name="aten__norm.66/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8435 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.66/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8441 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.8434, bf16[] %constant.8435), dimensions={0,1}, to_apply=%AddComputation.8437, metadata={op_type="aten__sum" op_name="aten__norm.66/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8442 = bf16[] sqrt(bf16[] %reduce.8441), metadata={op_type="aten__sqrt" op_name="aten__norm.66/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10407 = bf16[1]{0} reshape(bf16[] %sqrt.8442), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8411 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8417 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8410, bf16[] %constant.8411), dimensions={0,1}, to_apply=%AddComputation.8413, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8420 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8417, bf16[768]{0} %reduce.8417), metadata={op_type="aten__mul" op_name="aten__norm.67/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8421 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.67/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8427 = bf16[] reduce(bf16[768]{0} %multiply.8420, bf16[] %constant.8421), dimensions={0}, to_apply=%AddComputation.8423, metadata={op_type="aten__sum" op_name="aten__norm.67/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8428 = bf16[] sqrt(bf16[] %reduce.8427), metadata={op_type="aten__sqrt" op_name="aten__norm.67/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10408 = bf16[1]{0} reshape(bf16[] %sqrt.8428), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.8365 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8340, bf16[4,128,768]{2,1,0} %reshape.2177), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.8366 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8372 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8365, bf16[] %constant.8366), dimensions={0,1}, to_apply=%AddComputation.8368, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8375 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8372, bf16[768]{0} %reduce.8372), metadata={op_type="aten__mul" op_name="aten__norm.68/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8376 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.68/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8382 = bf16[] reduce(bf16[768]{0} %multiply.8375, bf16[] %constant.8376), dimensions={0}, to_apply=%AddComputation.8378, metadata={op_type="aten__sum" op_name="aten__norm.68/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8383 = bf16[] sqrt(bf16[] %reduce.8382), metadata={op_type="aten__sqrt" op_name="aten__norm.68/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10409 = bf16[1]{0} reshape(bf16[] %sqrt.8383), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8341 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8347 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.8340, bf16[] %constant.8341), dimensions={0,1}, to_apply=%AddComputation.8343, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8350 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8347, bf16[768]{0} %reduce.8347), metadata={op_type="aten__mul" op_name="aten__norm.69/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8351 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.69/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8357 = bf16[] reduce(bf16[768]{0} %multiply.8350, bf16[] %constant.8351), dimensions={0}, to_apply=%AddComputation.8353, metadata={op_type="aten__sum" op_name="aten__norm.69/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8358 = bf16[] sqrt(bf16[] %reduce.8357), metadata={op_type="aten__sqrt" op_name="aten__norm.69/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10410 = bf16[1]{0} reshape(bf16[] %sqrt.8358), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8295 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8296 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8295), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.24 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8294, bf16[768,512]{0,1} %transpose.8296), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8299 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.24, bf16[768,768]{0,1} %dot.24), metadata={op_type="aten__mul" op_name="aten__norm.70/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8300 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.70/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8306 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8299, bf16[] %constant.8300), dimensions={0,1}, to_apply=%AddComputation.8302, metadata={op_type="aten__sum" op_name="aten__norm.70/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8307 = bf16[] sqrt(bf16[] %reduce.8306), metadata={op_type="aten__sqrt" op_name="aten__norm.70/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10411 = bf16[1]{0} reshape(bf16[] %sqrt.8307), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8270 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8269), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8273 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.18 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.8270, bf16[] %constant.8273), dimensions={0,2}, to_apply=%AddComputation.8275, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.233 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.18, bf16[12,64]{1,0} %reduce.18), metadata={op_type="aten__mul" op_name="aten__norm.71/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8283 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.71/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8289 = bf16[] reduce(bf16[12,64]{1,0} %multiply.233, bf16[] %constant.8283), dimensions={0,1}, to_apply=%AddComputation.8285, metadata={op_type="aten__sum" op_name="aten__norm.71/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8290 = bf16[] sqrt(bf16[] %reduce.8289), metadata={op_type="aten__sqrt" op_name="aten__norm.71/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10412 = bf16[1]{0} reshape(bf16[] %sqrt.8290), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8254 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8255 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8254), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.25 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8253, bf16[768,512]{0,1} %transpose.8255), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8258 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.25, bf16[768,768]{0,1} %dot.25), metadata={op_type="aten__mul" op_name="aten__norm.72/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8259 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.72/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8265 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8258, bf16[] %constant.8259), dimensions={0,1}, to_apply=%AddComputation.8261, metadata={op_type="aten__sum" op_name="aten__norm.72/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8266 = bf16[] sqrt(bf16[] %reduce.8265), metadata={op_type="aten__sqrt" op_name="aten__norm.72/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10413 = bf16[1]{0} reshape(bf16[] %sqrt.8266), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8227 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.8226), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8231 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.19 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.8227, bf16[] %constant.8231), dimensions={0,3}, to_apply=%AddComputation.8233, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.232 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.19, bf16[12,64]{1,0} %reduce.19), metadata={op_type="aten__mul" op_name="aten__norm.73/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8241 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.73/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8247 = bf16[] reduce(bf16[12,64]{1,0} %multiply.232, bf16[] %constant.8241), dimensions={0,1}, to_apply=%AddComputation.8243, metadata={op_type="aten__sum" op_name="aten__norm.73/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8248 = bf16[] sqrt(bf16[] %reduce.8247), metadata={op_type="aten__sqrt" op_name="aten__norm.73/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10414 = bf16[1]{0} reshape(bf16[] %sqrt.8248), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8193 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2185), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8194 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8193), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.26 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8192, bf16[768,512]{0,1} %transpose.8194), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8197 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.26, bf16[768,768]{0,1} %dot.26), metadata={op_type="aten__mul" op_name="aten__norm.74/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8198 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.74/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8204 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8197, bf16[] %constant.8198), dimensions={0,1}, to_apply=%AddComputation.8200, metadata={op_type="aten__sum" op_name="aten__norm.74/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8205 = bf16[] sqrt(bf16[] %reduce.8204), metadata={op_type="aten__sqrt" op_name="aten__norm.74/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10415 = bf16[1]{0} reshape(bf16[] %sqrt.8205), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8168 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.8167), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.8171 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.20 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.8168, bf16[] %constant.8171), dimensions={0,2}, to_apply=%AddComputation.8173, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.231 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.20, bf16[12,64]{1,0} %reduce.20), metadata={op_type="aten__mul" op_name="aten__norm.75/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8181 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.75/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8187 = bf16[] reduce(bf16[12,64]{1,0} %multiply.231, bf16[] %constant.8181), dimensions={0,1}, to_apply=%AddComputation.8183, metadata={op_type="aten__sum" op_name="aten__norm.75/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8188 = bf16[] sqrt(bf16[] %reduce.8187), metadata={op_type="aten__sqrt" op_name="aten__norm.75/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10416 = bf16[1]{0} reshape(bf16[] %sqrt.8188), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8142 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2357), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8143 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.8142), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.8145 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.8143), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8146 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8145), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.27 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.8141, bf16[768,512]{0,1} %transpose.8146), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8149 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.27, bf16[768,768]{0,1} %dot.27), metadata={op_type="aten__mul" op_name="aten__norm.76/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8150 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.76/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8156 = bf16[] reduce(bf16[768,768]{0,1} %multiply.8149, bf16[] %constant.8150), dimensions={0,1}, to_apply=%AddComputation.8152, metadata={op_type="aten__sum" op_name="aten__norm.76/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8157 = bf16[] sqrt(bf16[] %reduce.8156), metadata={op_type="aten__sqrt" op_name="aten__norm.76/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10417 = bf16[1]{0} reshape(bf16[] %sqrt.8157), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8123 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8129 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8122, bf16[] %constant.8123), dimensions={0,1}, to_apply=%AddComputation.8125, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8132 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8129, bf16[768]{0} %reduce.8129), metadata={op_type="aten__mul" op_name="aten__norm.77/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8133 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.77/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8139 = bf16[] reduce(bf16[768]{0} %multiply.8132, bf16[] %constant.8133), dimensions={0}, to_apply=%AddComputation.8135, metadata={op_type="aten__sum" op_name="aten__norm.77/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8140 = bf16[] sqrt(bf16[] %reduce.8139), metadata={op_type="aten__sqrt" op_name="aten__norm.77/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10418 = bf16[1]{0} reshape(bf16[] %sqrt.8140), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.8077 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.8052, bf16[4,128,768]{2,1,0} %reshape.2379), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.8078 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8084 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.8077, bf16[] %constant.8078), dimensions={0,1}, to_apply=%AddComputation.8080, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8087 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8084, bf16[768]{0} %reduce.8084), metadata={op_type="aten__mul" op_name="aten__norm.78/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8088 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.78/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8094 = bf16[] reduce(bf16[768]{0} %multiply.8087, bf16[] %constant.8088), dimensions={0}, to_apply=%AddComputation.8090, metadata={op_type="aten__sum" op_name="aten__norm.78/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8095 = bf16[] sqrt(bf16[] %reduce.8094), metadata={op_type="aten__sqrt" op_name="aten__norm.78/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10419 = bf16[1]{0} reshape(bf16[] %sqrt.8095), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8053 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8059 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.8052, bf16[] %constant.8053), dimensions={0,1}, to_apply=%AddComputation.8055, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8062 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8059, bf16[768]{0} %reduce.8059), metadata={op_type="aten__mul" op_name="aten__norm.79/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8063 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.79/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8069 = bf16[] reduce(bf16[768]{0} %multiply.8062, bf16[] %constant.8063), dimensions={0}, to_apply=%AddComputation.8065, metadata={op_type="aten__sum" op_name="aten__norm.79/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8070 = bf16[] sqrt(bf16[] %reduce.8069), metadata={op_type="aten__sqrt" op_name="aten__norm.79/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10420 = bf16[1]{0} reshape(bf16[] %sqrt.8070), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.8029 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2387), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.8030 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.8029), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.28 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.8028, bf16[768,512]{0,1} %transpose.8030), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.8033 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.28, bf16[3072,768]{0,1} %dot.28), metadata={op_type="aten__mul" op_name="aten__norm.80/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8034 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.80/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8040 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.8033, bf16[] %constant.8034), dimensions={0,1}, to_apply=%AddComputation.8036, metadata={op_type="aten__sum" op_name="aten__norm.80/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8041 = bf16[] sqrt(bf16[] %reduce.8040), metadata={op_type="aten__sqrt" op_name="aten__norm.80/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10421 = bf16[1]{0} reshape(bf16[] %sqrt.8041), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.8010 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.8016 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.28, bf16[] %constant.8010), dimensions={0,1}, to_apply=%AddComputation.8012, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.8019 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8016, bf16[3072]{0} %reduce.8016), metadata={op_type="aten__mul" op_name="aten__norm.81/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.8020 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.81/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.8026 = bf16[] reduce(bf16[3072]{0} %multiply.8019, bf16[] %constant.8020), dimensions={0}, to_apply=%AddComputation.8022, metadata={op_type="aten__sum" op_name="aten__norm.81/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.8027 = bf16[] sqrt(bf16[] %reduce.8026), metadata={op_type="aten__sqrt" op_name="aten__norm.81/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10422 = bf16[1]{0} reshape(bf16[] %sqrt.8027), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7983 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.37), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7984 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.7983), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.29 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.7982, bf16[3072,512]{0,1} %transpose.7984), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7987 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.29, bf16[768,3072]{0,1} %dot.29), metadata={op_type="aten__mul" op_name="aten__norm.82/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7988 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.82/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7994 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.7987, bf16[] %constant.7988), dimensions={0,1}, to_apply=%AddComputation.7990, metadata={op_type="aten__sum" op_name="aten__norm.82/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7995 = bf16[] sqrt(bf16[] %reduce.7994), metadata={op_type="aten__sqrt" op_name="aten__norm.82/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10423 = bf16[1]{0} reshape(bf16[] %sqrt.7995), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7964 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7970 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7963, bf16[] %constant.7964), dimensions={0,1}, to_apply=%AddComputation.7966, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7973 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7970, bf16[768]{0} %reduce.7970), metadata={op_type="aten__mul" op_name="aten__norm.83/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7974 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.83/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7980 = bf16[] reduce(bf16[768]{0} %multiply.7973, bf16[] %constant.7974), dimensions={0}, to_apply=%AddComputation.7976, metadata={op_type="aten__sum" op_name="aten__norm.83/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7981 = bf16[] sqrt(bf16[] %reduce.7980), metadata={op_type="aten__sqrt" op_name="aten__norm.83/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10424 = bf16[1]{0} reshape(bf16[] %sqrt.7981), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.7918 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7893, bf16[4,128,768]{2,1,0} %reshape.2472), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.7919 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7925 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7918, bf16[] %constant.7919), dimensions={0,1}, to_apply=%AddComputation.7921, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7928 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7925, bf16[768]{0} %reduce.7925), metadata={op_type="aten__mul" op_name="aten__norm.84/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7929 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.84/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7935 = bf16[] reduce(bf16[768]{0} %multiply.7928, bf16[] %constant.7929), dimensions={0}, to_apply=%AddComputation.7931, metadata={op_type="aten__sum" op_name="aten__norm.84/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7936 = bf16[] sqrt(bf16[] %reduce.7935), metadata={op_type="aten__sqrt" op_name="aten__norm.84/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10425 = bf16[1]{0} reshape(bf16[] %sqrt.7936), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7894 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7900 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.7893, bf16[] %constant.7894), dimensions={0,1}, to_apply=%AddComputation.7896, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7903 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7900, bf16[768]{0} %reduce.7900), metadata={op_type="aten__mul" op_name="aten__norm.85/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7904 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.85/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7910 = bf16[] reduce(bf16[768]{0} %multiply.7903, bf16[] %constant.7904), dimensions={0}, to_apply=%AddComputation.7906, metadata={op_type="aten__sum" op_name="aten__norm.85/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7911 = bf16[] sqrt(bf16[] %reduce.7910), metadata={op_type="aten__sqrt" op_name="aten__norm.85/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10426 = bf16[1]{0} reshape(bf16[] %sqrt.7911), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7848 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7849 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7848), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.30 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7847, bf16[768,512]{0,1} %transpose.7849), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7852 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.30, bf16[768,768]{0,1} %dot.30), metadata={op_type="aten__mul" op_name="aten__norm.86/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7853 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.86/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7859 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7852, bf16[] %constant.7853), dimensions={0,1}, to_apply=%AddComputation.7855, metadata={op_type="aten__sum" op_name="aten__norm.86/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7860 = bf16[] sqrt(bf16[] %reduce.7859), metadata={op_type="aten__sqrt" op_name="aten__norm.86/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10427 = bf16[1]{0} reshape(bf16[] %sqrt.7860), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7823 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7822), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7826 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.21 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.7823, bf16[] %constant.7826), dimensions={0,2}, to_apply=%AddComputation.7828, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.230 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.21, bf16[12,64]{1,0} %reduce.21), metadata={op_type="aten__mul" op_name="aten__norm.87/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7836 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.87/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7842 = bf16[] reduce(bf16[12,64]{1,0} %multiply.230, bf16[] %constant.7836), dimensions={0,1}, to_apply=%AddComputation.7838, metadata={op_type="aten__sum" op_name="aten__norm.87/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7843 = bf16[] sqrt(bf16[] %reduce.7842), metadata={op_type="aten__sqrt" op_name="aten__norm.87/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10428 = bf16[1]{0} reshape(bf16[] %sqrt.7843), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7807 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7808 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7807), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.31 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7806, bf16[768,512]{0,1} %transpose.7808), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7811 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.31, bf16[768,768]{0,1} %dot.31), metadata={op_type="aten__mul" op_name="aten__norm.88/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7812 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.88/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7818 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7811, bf16[] %constant.7812), dimensions={0,1}, to_apply=%AddComputation.7814, metadata={op_type="aten__sum" op_name="aten__norm.88/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7819 = bf16[] sqrt(bf16[] %reduce.7818), metadata={op_type="aten__sqrt" op_name="aten__norm.88/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10429 = bf16[1]{0} reshape(bf16[] %sqrt.7819), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7780 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.7779), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7784 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.22 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.7780, bf16[] %constant.7784), dimensions={0,3}, to_apply=%AddComputation.7786, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.229 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.22, bf16[12,64]{1,0} %reduce.22), metadata={op_type="aten__mul" op_name="aten__norm.89/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7794 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.89/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7800 = bf16[] reduce(bf16[12,64]{1,0} %multiply.229, bf16[] %constant.7794), dimensions={0,1}, to_apply=%AddComputation.7796, metadata={op_type="aten__sum" op_name="aten__norm.89/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7801 = bf16[] sqrt(bf16[] %reduce.7800), metadata={op_type="aten__sqrt" op_name="aten__norm.89/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10430 = bf16[1]{0} reshape(bf16[] %sqrt.7801), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7746 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2480), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7747 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7746), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.32 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7745, bf16[768,512]{0,1} %transpose.7747), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7750 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.32, bf16[768,768]{0,1} %dot.32), metadata={op_type="aten__mul" op_name="aten__norm.90/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7751 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.90/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7757 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7750, bf16[] %constant.7751), dimensions={0,1}, to_apply=%AddComputation.7753, metadata={op_type="aten__sum" op_name="aten__norm.90/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7758 = bf16[] sqrt(bf16[] %reduce.7757), metadata={op_type="aten__sqrt" op_name="aten__norm.90/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10431 = bf16[1]{0} reshape(bf16[] %sqrt.7758), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7721 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7720), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7724 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.23 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.7721, bf16[] %constant.7724), dimensions={0,2}, to_apply=%AddComputation.7726, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.228 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.23, bf16[12,64]{1,0} %reduce.23), metadata={op_type="aten__mul" op_name="aten__norm.91/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7734 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.91/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7740 = bf16[] reduce(bf16[12,64]{1,0} %multiply.228, bf16[] %constant.7734), dimensions={0,1}, to_apply=%AddComputation.7736, metadata={op_type="aten__sum" op_name="aten__norm.91/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7741 = bf16[] sqrt(bf16[] %reduce.7740), metadata={op_type="aten__sqrt" op_name="aten__norm.91/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10432 = bf16[1]{0} reshape(bf16[] %sqrt.7741), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7695 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2652), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7696 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7695), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7698 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7696), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7699 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7698), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.33 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7694, bf16[768,512]{0,1} %transpose.7699), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7702 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.33, bf16[768,768]{0,1} %dot.33), metadata={op_type="aten__mul" op_name="aten__norm.92/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7703 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.92/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7709 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7702, bf16[] %constant.7703), dimensions={0,1}, to_apply=%AddComputation.7705, metadata={op_type="aten__sum" op_name="aten__norm.92/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7710 = bf16[] sqrt(bf16[] %reduce.7709), metadata={op_type="aten__sqrt" op_name="aten__norm.92/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10433 = bf16[1]{0} reshape(bf16[] %sqrt.7710), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7676 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7682 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7675, bf16[] %constant.7676), dimensions={0,1}, to_apply=%AddComputation.7678, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7685 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7682, bf16[768]{0} %reduce.7682), metadata={op_type="aten__mul" op_name="aten__norm.93/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7686 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.93/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7692 = bf16[] reduce(bf16[768]{0} %multiply.7685, bf16[] %constant.7686), dimensions={0}, to_apply=%AddComputation.7688, metadata={op_type="aten__sum" op_name="aten__norm.93/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7693 = bf16[] sqrt(bf16[] %reduce.7692), metadata={op_type="aten__sqrt" op_name="aten__norm.93/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10434 = bf16[1]{0} reshape(bf16[] %sqrt.7693), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.7630 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7605, bf16[4,128,768]{2,1,0} %reshape.2674), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.7631 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7637 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7630, bf16[] %constant.7631), dimensions={0,1}, to_apply=%AddComputation.7633, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7640 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7637, bf16[768]{0} %reduce.7637), metadata={op_type="aten__mul" op_name="aten__norm.94/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7641 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.94/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7647 = bf16[] reduce(bf16[768]{0} %multiply.7640, bf16[] %constant.7641), dimensions={0}, to_apply=%AddComputation.7643, metadata={op_type="aten__sum" op_name="aten__norm.94/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7648 = bf16[] sqrt(bf16[] %reduce.7647), metadata={op_type="aten__sqrt" op_name="aten__norm.94/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10435 = bf16[1]{0} reshape(bf16[] %sqrt.7648), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7606 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7612 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.7605, bf16[] %constant.7606), dimensions={0,1}, to_apply=%AddComputation.7608, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7615 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7612, bf16[768]{0} %reduce.7612), metadata={op_type="aten__mul" op_name="aten__norm.95/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7616 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.95/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7622 = bf16[] reduce(bf16[768]{0} %multiply.7615, bf16[] %constant.7616), dimensions={0}, to_apply=%AddComputation.7618, metadata={op_type="aten__sum" op_name="aten__norm.95/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7623 = bf16[] sqrt(bf16[] %reduce.7622), metadata={op_type="aten__sqrt" op_name="aten__norm.95/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10436 = bf16[1]{0} reshape(bf16[] %sqrt.7623), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7582 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2682), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7583 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7582), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.34 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.7581, bf16[768,512]{0,1} %transpose.7583), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7586 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.34, bf16[3072,768]{0,1} %dot.34), metadata={op_type="aten__mul" op_name="aten__norm.96/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7587 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.96/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7593 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.7586, bf16[] %constant.7587), dimensions={0,1}, to_apply=%AddComputation.7589, metadata={op_type="aten__sum" op_name="aten__norm.96/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7594 = bf16[] sqrt(bf16[] %reduce.7593), metadata={op_type="aten__sqrt" op_name="aten__norm.96/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10437 = bf16[1]{0} reshape(bf16[] %sqrt.7594), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7563 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7569 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.27, bf16[] %constant.7563), dimensions={0,1}, to_apply=%AddComputation.7565, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7572 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.7569, bf16[3072]{0} %reduce.7569), metadata={op_type="aten__mul" op_name="aten__norm.97/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7573 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.97/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7579 = bf16[] reduce(bf16[3072]{0} %multiply.7572, bf16[] %constant.7573), dimensions={0}, to_apply=%AddComputation.7575, metadata={op_type="aten__sum" op_name="aten__norm.97/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7580 = bf16[] sqrt(bf16[] %reduce.7579), metadata={op_type="aten__sqrt" op_name="aten__norm.97/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10438 = bf16[1]{0} reshape(bf16[] %sqrt.7580), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7536 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.38), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7537 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.7536), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.35 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.7535, bf16[3072,512]{0,1} %transpose.7537), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7540 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.35, bf16[768,3072]{0,1} %dot.35), metadata={op_type="aten__mul" op_name="aten__norm.98/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7541 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.98/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7547 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.7540, bf16[] %constant.7541), dimensions={0,1}, to_apply=%AddComputation.7543, metadata={op_type="aten__sum" op_name="aten__norm.98/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7548 = bf16[] sqrt(bf16[] %reduce.7547), metadata={op_type="aten__sqrt" op_name="aten__norm.98/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10439 = bf16[1]{0} reshape(bf16[] %sqrt.7548), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7517 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7523 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7516, bf16[] %constant.7517), dimensions={0,1}, to_apply=%AddComputation.7519, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7526 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7523, bf16[768]{0} %reduce.7523), metadata={op_type="aten__mul" op_name="aten__norm.99/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7527 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.99/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7533 = bf16[] reduce(bf16[768]{0} %multiply.7526, bf16[] %constant.7527), dimensions={0}, to_apply=%AddComputation.7529, metadata={op_type="aten__sum" op_name="aten__norm.99/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7534 = bf16[] sqrt(bf16[] %reduce.7533), metadata={op_type="aten__sqrt" op_name="aten__norm.99/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10440 = bf16[1]{0} reshape(bf16[] %sqrt.7534), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.7471 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7446, bf16[4,128,768]{2,1,0} %reshape.2767), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.7472 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7478 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7471, bf16[] %constant.7472), dimensions={0,1}, to_apply=%AddComputation.7474, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7481 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7478, bf16[768]{0} %reduce.7478), metadata={op_type="aten__mul" op_name="aten__norm.100/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7482 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.100/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7488 = bf16[] reduce(bf16[768]{0} %multiply.7481, bf16[] %constant.7482), dimensions={0}, to_apply=%AddComputation.7484, metadata={op_type="aten__sum" op_name="aten__norm.100/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7489 = bf16[] sqrt(bf16[] %reduce.7488), metadata={op_type="aten__sqrt" op_name="aten__norm.100/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10441 = bf16[1]{0} reshape(bf16[] %sqrt.7489), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7447 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7453 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.7446, bf16[] %constant.7447), dimensions={0,1}, to_apply=%AddComputation.7449, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7456 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7453, bf16[768]{0} %reduce.7453), metadata={op_type="aten__mul" op_name="aten__norm.101/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7457 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.101/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7463 = bf16[] reduce(bf16[768]{0} %multiply.7456, bf16[] %constant.7457), dimensions={0}, to_apply=%AddComputation.7459, metadata={op_type="aten__sum" op_name="aten__norm.101/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7464 = bf16[] sqrt(bf16[] %reduce.7463), metadata={op_type="aten__sqrt" op_name="aten__norm.101/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10442 = bf16[1]{0} reshape(bf16[] %sqrt.7464), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7401 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7402 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7401), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.36 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7400, bf16[768,512]{0,1} %transpose.7402), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7405 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.36, bf16[768,768]{0,1} %dot.36), metadata={op_type="aten__mul" op_name="aten__norm.102/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7406 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.102/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7412 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7405, bf16[] %constant.7406), dimensions={0,1}, to_apply=%AddComputation.7408, metadata={op_type="aten__sum" op_name="aten__norm.102/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7413 = bf16[] sqrt(bf16[] %reduce.7412), metadata={op_type="aten__sqrt" op_name="aten__norm.102/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10443 = bf16[1]{0} reshape(bf16[] %sqrt.7413), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7376 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7375), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7379 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.24 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.7376, bf16[] %constant.7379), dimensions={0,2}, to_apply=%AddComputation.7381, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.227 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.24, bf16[12,64]{1,0} %reduce.24), metadata={op_type="aten__mul" op_name="aten__norm.103/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7389 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.103/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7395 = bf16[] reduce(bf16[12,64]{1,0} %multiply.227, bf16[] %constant.7389), dimensions={0,1}, to_apply=%AddComputation.7391, metadata={op_type="aten__sum" op_name="aten__norm.103/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7396 = bf16[] sqrt(bf16[] %reduce.7395), metadata={op_type="aten__sqrt" op_name="aten__norm.103/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10444 = bf16[1]{0} reshape(bf16[] %sqrt.7396), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7360 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7361 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7360), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.37 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7359, bf16[768,512]{0,1} %transpose.7361), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7364 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.37, bf16[768,768]{0,1} %dot.37), metadata={op_type="aten__mul" op_name="aten__norm.104/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7365 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.104/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7371 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7364, bf16[] %constant.7365), dimensions={0,1}, to_apply=%AddComputation.7367, metadata={op_type="aten__sum" op_name="aten__norm.104/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7372 = bf16[] sqrt(bf16[] %reduce.7371), metadata={op_type="aten__sqrt" op_name="aten__norm.104/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10445 = bf16[1]{0} reshape(bf16[] %sqrt.7372), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7333 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.7332), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7337 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.25 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.7333, bf16[] %constant.7337), dimensions={0,3}, to_apply=%AddComputation.7339, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.226 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.25, bf16[12,64]{1,0} %reduce.25), metadata={op_type="aten__mul" op_name="aten__norm.105/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7347 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.105/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7353 = bf16[] reduce(bf16[12,64]{1,0} %multiply.226, bf16[] %constant.7347), dimensions={0,1}, to_apply=%AddComputation.7349, metadata={op_type="aten__sum" op_name="aten__norm.105/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7354 = bf16[] sqrt(bf16[] %reduce.7353), metadata={op_type="aten__sqrt" op_name="aten__norm.105/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10446 = bf16[1]{0} reshape(bf16[] %sqrt.7354), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7299 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2775), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7300 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7299), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.38 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7298, bf16[768,512]{0,1} %transpose.7300), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7303 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.38, bf16[768,768]{0,1} %dot.38), metadata={op_type="aten__mul" op_name="aten__norm.106/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7304 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.106/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7310 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7303, bf16[] %constant.7304), dimensions={0,1}, to_apply=%AddComputation.7306, metadata={op_type="aten__sum" op_name="aten__norm.106/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7311 = bf16[] sqrt(bf16[] %reduce.7310), metadata={op_type="aten__sqrt" op_name="aten__norm.106/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10447 = bf16[1]{0} reshape(bf16[] %sqrt.7311), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7274 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.7273), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.7277 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.26 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.7274, bf16[] %constant.7277), dimensions={0,2}, to_apply=%AddComputation.7279, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.225 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.26, bf16[12,64]{1,0} %reduce.26), metadata={op_type="aten__mul" op_name="aten__norm.107/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7287 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.107/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7293 = bf16[] reduce(bf16[12,64]{1,0} %multiply.225, bf16[] %constant.7287), dimensions={0,1}, to_apply=%AddComputation.7289, metadata={op_type="aten__sum" op_name="aten__norm.107/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7294 = bf16[] sqrt(bf16[] %reduce.7293), metadata={op_type="aten__sqrt" op_name="aten__norm.107/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10448 = bf16[1]{0} reshape(bf16[] %sqrt.7294), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7248 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.2947), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7249 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.7248), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.7251 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.7249), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7252 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7251), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.39 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.7247, bf16[768,512]{0,1} %transpose.7252), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7255 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.39, bf16[768,768]{0,1} %dot.39), metadata={op_type="aten__mul" op_name="aten__norm.108/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7256 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.108/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7262 = bf16[] reduce(bf16[768,768]{0,1} %multiply.7255, bf16[] %constant.7256), dimensions={0,1}, to_apply=%AddComputation.7258, metadata={op_type="aten__sum" op_name="aten__norm.108/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7263 = bf16[] sqrt(bf16[] %reduce.7262), metadata={op_type="aten__sqrt" op_name="aten__norm.108/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10449 = bf16[1]{0} reshape(bf16[] %sqrt.7263), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7229 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7235 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7228, bf16[] %constant.7229), dimensions={0,1}, to_apply=%AddComputation.7231, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7238 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7235, bf16[768]{0} %reduce.7235), metadata={op_type="aten__mul" op_name="aten__norm.109/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7239 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.109/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7245 = bf16[] reduce(bf16[768]{0} %multiply.7238, bf16[] %constant.7239), dimensions={0}, to_apply=%AddComputation.7241, metadata={op_type="aten__sum" op_name="aten__norm.109/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7246 = bf16[] sqrt(bf16[] %reduce.7245), metadata={op_type="aten__sqrt" op_name="aten__norm.109/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10450 = bf16[1]{0} reshape(bf16[] %sqrt.7246), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.7183 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.7158, bf16[4,128,768]{2,1,0} %reshape.2969), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.7184 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7190 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7183, bf16[] %constant.7184), dimensions={0,1}, to_apply=%AddComputation.7186, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7193 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7190, bf16[768]{0} %reduce.7190), metadata={op_type="aten__mul" op_name="aten__norm.110/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7194 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.110/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7200 = bf16[] reduce(bf16[768]{0} %multiply.7193, bf16[] %constant.7194), dimensions={0}, to_apply=%AddComputation.7196, metadata={op_type="aten__sum" op_name="aten__norm.110/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7201 = bf16[] sqrt(bf16[] %reduce.7200), metadata={op_type="aten__sqrt" op_name="aten__norm.110/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10451 = bf16[1]{0} reshape(bf16[] %sqrt.7201), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7159 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7165 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.7158, bf16[] %constant.7159), dimensions={0,1}, to_apply=%AddComputation.7161, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7168 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7165, bf16[768]{0} %reduce.7165), metadata={op_type="aten__mul" op_name="aten__norm.111/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7169 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.111/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7175 = bf16[] reduce(bf16[768]{0} %multiply.7168, bf16[] %constant.7169), dimensions={0}, to_apply=%AddComputation.7171, metadata={op_type="aten__sum" op_name="aten__norm.111/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7176 = bf16[] sqrt(bf16[] %reduce.7175), metadata={op_type="aten__sqrt" op_name="aten__norm.111/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10452 = bf16[1]{0} reshape(bf16[] %sqrt.7176), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7135 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.2977), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7136 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.7135), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.40 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.7134, bf16[768,512]{0,1} %transpose.7136), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7139 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.40, bf16[3072,768]{0,1} %dot.40), metadata={op_type="aten__mul" op_name="aten__norm.112/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7140 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.112/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7146 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.7139, bf16[] %constant.7140), dimensions={0,1}, to_apply=%AddComputation.7142, metadata={op_type="aten__sum" op_name="aten__norm.112/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7147 = bf16[] sqrt(bf16[] %reduce.7146), metadata={op_type="aten__sqrt" op_name="aten__norm.112/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10453 = bf16[1]{0} reshape(bf16[] %sqrt.7147), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7116 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7122 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.26, bf16[] %constant.7116), dimensions={0,1}, to_apply=%AddComputation.7118, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7125 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.7122, bf16[3072]{0} %reduce.7122), metadata={op_type="aten__mul" op_name="aten__norm.113/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7126 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.113/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7132 = bf16[] reduce(bf16[3072]{0} %multiply.7125, bf16[] %constant.7126), dimensions={0}, to_apply=%AddComputation.7128, metadata={op_type="aten__sum" op_name="aten__norm.113/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7133 = bf16[] sqrt(bf16[] %reduce.7132), metadata={op_type="aten__sqrt" op_name="aten__norm.113/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10454 = bf16[1]{0} reshape(bf16[] %sqrt.7133), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.7089 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.39), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.7090 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.7089), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.41 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.7088, bf16[3072,512]{0,1} %transpose.7090), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.7093 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.41, bf16[768,3072]{0,1} %dot.41), metadata={op_type="aten__mul" op_name="aten__norm.114/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7094 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.114/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7100 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.7093, bf16[] %constant.7094), dimensions={0,1}, to_apply=%AddComputation.7096, metadata={op_type="aten__sum" op_name="aten__norm.114/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7101 = bf16[] sqrt(bf16[] %reduce.7100), metadata={op_type="aten__sqrt" op_name="aten__norm.114/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10455 = bf16[1]{0} reshape(bf16[] %sqrt.7101), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7070 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7076 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7069, bf16[] %constant.7070), dimensions={0,1}, to_apply=%AddComputation.7072, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7079 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7076, bf16[768]{0} %reduce.7076), metadata={op_type="aten__mul" op_name="aten__norm.115/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7080 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.115/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7086 = bf16[] reduce(bf16[768]{0} %multiply.7079, bf16[] %constant.7080), dimensions={0}, to_apply=%AddComputation.7082, metadata={op_type="aten__sum" op_name="aten__norm.115/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7087 = bf16[] sqrt(bf16[] %reduce.7086), metadata={op_type="aten__sqrt" op_name="aten__norm.115/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10456 = bf16[1]{0} reshape(bf16[] %sqrt.7087), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.7024 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6999, bf16[4,128,768]{2,1,0} %reshape.3062), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.7025 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7031 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.7024, bf16[] %constant.7025), dimensions={0,1}, to_apply=%AddComputation.7027, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7034 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7031, bf16[768]{0} %reduce.7031), metadata={op_type="aten__mul" op_name="aten__norm.116/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7035 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.116/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7041 = bf16[] reduce(bf16[768]{0} %multiply.7034, bf16[] %constant.7035), dimensions={0}, to_apply=%AddComputation.7037, metadata={op_type="aten__sum" op_name="aten__norm.116/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7042 = bf16[] sqrt(bf16[] %reduce.7041), metadata={op_type="aten__sqrt" op_name="aten__norm.116/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10457 = bf16[1]{0} reshape(bf16[] %sqrt.7042), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.7000 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.7006 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.6999, bf16[] %constant.7000), dimensions={0,1}, to_apply=%AddComputation.7002, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.7009 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7006, bf16[768]{0} %reduce.7006), metadata={op_type="aten__mul" op_name="aten__norm.117/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.7010 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.117/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.7016 = bf16[] reduce(bf16[768]{0} %multiply.7009, bf16[] %constant.7010), dimensions={0}, to_apply=%AddComputation.7012, metadata={op_type="aten__sum" op_name="aten__norm.117/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.7017 = bf16[] sqrt(bf16[] %reduce.7016), metadata={op_type="aten__sqrt" op_name="aten__norm.117/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10458 = bf16[1]{0} reshape(bf16[] %sqrt.7017), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6954 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6955 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6954), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.42 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6953, bf16[768,512]{0,1} %transpose.6955), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6958 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.42, bf16[768,768]{0,1} %dot.42), metadata={op_type="aten__mul" op_name="aten__norm.118/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6959 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.118/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6965 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6958, bf16[] %constant.6959), dimensions={0,1}, to_apply=%AddComputation.6961, metadata={op_type="aten__sum" op_name="aten__norm.118/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6966 = bf16[] sqrt(bf16[] %reduce.6965), metadata={op_type="aten__sqrt" op_name="aten__norm.118/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10459 = bf16[1]{0} reshape(bf16[] %sqrt.6966), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6929 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6928), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6932 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.27 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.6929, bf16[] %constant.6932), dimensions={0,2}, to_apply=%AddComputation.6934, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.224 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.27, bf16[12,64]{1,0} %reduce.27), metadata={op_type="aten__mul" op_name="aten__norm.119/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6942 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.119/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6948 = bf16[] reduce(bf16[12,64]{1,0} %multiply.224, bf16[] %constant.6942), dimensions={0,1}, to_apply=%AddComputation.6944, metadata={op_type="aten__sum" op_name="aten__norm.119/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6949 = bf16[] sqrt(bf16[] %reduce.6948), metadata={op_type="aten__sqrt" op_name="aten__norm.119/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10460 = bf16[1]{0} reshape(bf16[] %sqrt.6949), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6913 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6914 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6913), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.43 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6912, bf16[768,512]{0,1} %transpose.6914), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6917 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.43, bf16[768,768]{0,1} %dot.43), metadata={op_type="aten__mul" op_name="aten__norm.120/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6918 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.120/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6924 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6917, bf16[] %constant.6918), dimensions={0,1}, to_apply=%AddComputation.6920, metadata={op_type="aten__sum" op_name="aten__norm.120/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6925 = bf16[] sqrt(bf16[] %reduce.6924), metadata={op_type="aten__sqrt" op_name="aten__norm.120/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10461 = bf16[1]{0} reshape(bf16[] %sqrt.6925), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6886 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.6885), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6890 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.28 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.6886, bf16[] %constant.6890), dimensions={0,3}, to_apply=%AddComputation.6892, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.223 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.28, bf16[12,64]{1,0} %reduce.28), metadata={op_type="aten__mul" op_name="aten__norm.121/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6900 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.121/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6906 = bf16[] reduce(bf16[12,64]{1,0} %multiply.223, bf16[] %constant.6900), dimensions={0,1}, to_apply=%AddComputation.6902, metadata={op_type="aten__sum" op_name="aten__norm.121/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6907 = bf16[] sqrt(bf16[] %reduce.6906), metadata={op_type="aten__sqrt" op_name="aten__norm.121/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10462 = bf16[1]{0} reshape(bf16[] %sqrt.6907), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6852 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3070), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6853 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6852), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.44 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6851, bf16[768,512]{0,1} %transpose.6853), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6856 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.44, bf16[768,768]{0,1} %dot.44), metadata={op_type="aten__mul" op_name="aten__norm.122/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6857 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.122/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6863 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6856, bf16[] %constant.6857), dimensions={0,1}, to_apply=%AddComputation.6859, metadata={op_type="aten__sum" op_name="aten__norm.122/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6864 = bf16[] sqrt(bf16[] %reduce.6863), metadata={op_type="aten__sqrt" op_name="aten__norm.122/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10463 = bf16[1]{0} reshape(bf16[] %sqrt.6864), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6827 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6826), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6830 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.29 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.6827, bf16[] %constant.6830), dimensions={0,2}, to_apply=%AddComputation.6832, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.222 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.29, bf16[12,64]{1,0} %reduce.29), metadata={op_type="aten__mul" op_name="aten__norm.123/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6840 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.123/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6846 = bf16[] reduce(bf16[12,64]{1,0} %multiply.222, bf16[] %constant.6840), dimensions={0,1}, to_apply=%AddComputation.6842, metadata={op_type="aten__sum" op_name="aten__norm.123/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6847 = bf16[] sqrt(bf16[] %reduce.6846), metadata={op_type="aten__sqrt" op_name="aten__norm.123/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10464 = bf16[1]{0} reshape(bf16[] %sqrt.6847), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6801 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3242), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6802 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6801), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6804 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6802), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6805 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6804), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.45 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6800, bf16[768,512]{0,1} %transpose.6805), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6808 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.45, bf16[768,768]{0,1} %dot.45), metadata={op_type="aten__mul" op_name="aten__norm.124/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6809 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.124/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6815 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6808, bf16[] %constant.6809), dimensions={0,1}, to_apply=%AddComputation.6811, metadata={op_type="aten__sum" op_name="aten__norm.124/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6816 = bf16[] sqrt(bf16[] %reduce.6815), metadata={op_type="aten__sqrt" op_name="aten__norm.124/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10465 = bf16[1]{0} reshape(bf16[] %sqrt.6816), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6782 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6788 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6781, bf16[] %constant.6782), dimensions={0,1}, to_apply=%AddComputation.6784, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6791 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6788, bf16[768]{0} %reduce.6788), metadata={op_type="aten__mul" op_name="aten__norm.125/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6792 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.125/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6798 = bf16[] reduce(bf16[768]{0} %multiply.6791, bf16[] %constant.6792), dimensions={0}, to_apply=%AddComputation.6794, metadata={op_type="aten__sum" op_name="aten__norm.125/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6799 = bf16[] sqrt(bf16[] %reduce.6798), metadata={op_type="aten__sqrt" op_name="aten__norm.125/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10466 = bf16[1]{0} reshape(bf16[] %sqrt.6799), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.6736 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6711, bf16[4,128,768]{2,1,0} %reshape.3264), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.6737 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6743 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6736, bf16[] %constant.6737), dimensions={0,1}, to_apply=%AddComputation.6739, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6746 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6743, bf16[768]{0} %reduce.6743), metadata={op_type="aten__mul" op_name="aten__norm.126/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6747 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.126/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6753 = bf16[] reduce(bf16[768]{0} %multiply.6746, bf16[] %constant.6747), dimensions={0}, to_apply=%AddComputation.6749, metadata={op_type="aten__sum" op_name="aten__norm.126/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6754 = bf16[] sqrt(bf16[] %reduce.6753), metadata={op_type="aten__sqrt" op_name="aten__norm.126/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10467 = bf16[1]{0} reshape(bf16[] %sqrt.6754), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6712 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6718 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.6711, bf16[] %constant.6712), dimensions={0,1}, to_apply=%AddComputation.6714, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6721 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6718, bf16[768]{0} %reduce.6718), metadata={op_type="aten__mul" op_name="aten__norm.127/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6722 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.127/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6728 = bf16[] reduce(bf16[768]{0} %multiply.6721, bf16[] %constant.6722), dimensions={0}, to_apply=%AddComputation.6724, metadata={op_type="aten__sum" op_name="aten__norm.127/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6729 = bf16[] sqrt(bf16[] %reduce.6728), metadata={op_type="aten__sqrt" op_name="aten__norm.127/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10468 = bf16[1]{0} reshape(bf16[] %sqrt.6729), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6688 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3272), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6689 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6688), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.46 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.6687, bf16[768,512]{0,1} %transpose.6689), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6692 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.46, bf16[3072,768]{0,1} %dot.46), metadata={op_type="aten__mul" op_name="aten__norm.128/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6693 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.128/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6699 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.6692, bf16[] %constant.6693), dimensions={0,1}, to_apply=%AddComputation.6695, metadata={op_type="aten__sum" op_name="aten__norm.128/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6700 = bf16[] sqrt(bf16[] %reduce.6699), metadata={op_type="aten__sqrt" op_name="aten__norm.128/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10469 = bf16[1]{0} reshape(bf16[] %sqrt.6700), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6669 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6675 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.24, bf16[] %constant.6669), dimensions={0,1}, to_apply=%AddComputation.6671, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6678 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.6675, bf16[3072]{0} %reduce.6675), metadata={op_type="aten__mul" op_name="aten__norm.129/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6679 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.129/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6685 = bf16[] reduce(bf16[3072]{0} %multiply.6678, bf16[] %constant.6679), dimensions={0}, to_apply=%AddComputation.6681, metadata={op_type="aten__sum" op_name="aten__norm.129/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6686 = bf16[] sqrt(bf16[] %reduce.6685), metadata={op_type="aten__sqrt" op_name="aten__norm.129/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10470 = bf16[1]{0} reshape(bf16[] %sqrt.6686), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6642 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.40), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6643 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.6642), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.47 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.6641, bf16[3072,512]{0,1} %transpose.6643), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6646 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.47, bf16[768,3072]{0,1} %dot.47), metadata={op_type="aten__mul" op_name="aten__norm.130/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6647 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.130/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6653 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.6646, bf16[] %constant.6647), dimensions={0,1}, to_apply=%AddComputation.6649, metadata={op_type="aten__sum" op_name="aten__norm.130/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6654 = bf16[] sqrt(bf16[] %reduce.6653), metadata={op_type="aten__sqrt" op_name="aten__norm.130/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10471 = bf16[1]{0} reshape(bf16[] %sqrt.6654), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6623 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6629 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6622, bf16[] %constant.6623), dimensions={0,1}, to_apply=%AddComputation.6625, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6632 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6629, bf16[768]{0} %reduce.6629), metadata={op_type="aten__mul" op_name="aten__norm.131/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6633 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.131/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6639 = bf16[] reduce(bf16[768]{0} %multiply.6632, bf16[] %constant.6633), dimensions={0}, to_apply=%AddComputation.6635, metadata={op_type="aten__sum" op_name="aten__norm.131/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6640 = bf16[] sqrt(bf16[] %reduce.6639), metadata={op_type="aten__sqrt" op_name="aten__norm.131/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10472 = bf16[1]{0} reshape(bf16[] %sqrt.6640), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.6577 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6552, bf16[4,128,768]{2,1,0} %reshape.3357), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.6578 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6584 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6577, bf16[] %constant.6578), dimensions={0,1}, to_apply=%AddComputation.6580, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6587 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6584, bf16[768]{0} %reduce.6584), metadata={op_type="aten__mul" op_name="aten__norm.132/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6588 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.132/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6594 = bf16[] reduce(bf16[768]{0} %multiply.6587, bf16[] %constant.6588), dimensions={0}, to_apply=%AddComputation.6590, metadata={op_type="aten__sum" op_name="aten__norm.132/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6595 = bf16[] sqrt(bf16[] %reduce.6594), metadata={op_type="aten__sqrt" op_name="aten__norm.132/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10473 = bf16[1]{0} reshape(bf16[] %sqrt.6595), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6553 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6559 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.6552, bf16[] %constant.6553), dimensions={0,1}, to_apply=%AddComputation.6555, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6562 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6559, bf16[768]{0} %reduce.6559), metadata={op_type="aten__mul" op_name="aten__norm.133/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6563 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.133/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6569 = bf16[] reduce(bf16[768]{0} %multiply.6562, bf16[] %constant.6563), dimensions={0}, to_apply=%AddComputation.6565, metadata={op_type="aten__sum" op_name="aten__norm.133/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6570 = bf16[] sqrt(bf16[] %reduce.6569), metadata={op_type="aten__sqrt" op_name="aten__norm.133/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10474 = bf16[1]{0} reshape(bf16[] %sqrt.6570), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6507 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6508 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6507), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.48 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6506, bf16[768,512]{0,1} %transpose.6508), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6511 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.48, bf16[768,768]{0,1} %dot.48), metadata={op_type="aten__mul" op_name="aten__norm.134/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6512 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.134/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6518 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6511, bf16[] %constant.6512), dimensions={0,1}, to_apply=%AddComputation.6514, metadata={op_type="aten__sum" op_name="aten__norm.134/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6519 = bf16[] sqrt(bf16[] %reduce.6518), metadata={op_type="aten__sqrt" op_name="aten__norm.134/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10475 = bf16[1]{0} reshape(bf16[] %sqrt.6519), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6482 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6481), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6485 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.30 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.6482, bf16[] %constant.6485), dimensions={0,2}, to_apply=%AddComputation.6487, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.221 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.30, bf16[12,64]{1,0} %reduce.30), metadata={op_type="aten__mul" op_name="aten__norm.135/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6495 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.135/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6501 = bf16[] reduce(bf16[12,64]{1,0} %multiply.221, bf16[] %constant.6495), dimensions={0,1}, to_apply=%AddComputation.6497, metadata={op_type="aten__sum" op_name="aten__norm.135/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6502 = bf16[] sqrt(bf16[] %reduce.6501), metadata={op_type="aten__sqrt" op_name="aten__norm.135/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10476 = bf16[1]{0} reshape(bf16[] %sqrt.6502), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6466 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6467 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6466), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.49 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6465, bf16[768,512]{0,1} %transpose.6467), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6470 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.49, bf16[768,768]{0,1} %dot.49), metadata={op_type="aten__mul" op_name="aten__norm.136/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6471 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.136/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6477 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6470, bf16[] %constant.6471), dimensions={0,1}, to_apply=%AddComputation.6473, metadata={op_type="aten__sum" op_name="aten__norm.136/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6478 = bf16[] sqrt(bf16[] %reduce.6477), metadata={op_type="aten__sqrt" op_name="aten__norm.136/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10477 = bf16[1]{0} reshape(bf16[] %sqrt.6478), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6439 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.6438), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6443 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.31 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.6439, bf16[] %constant.6443), dimensions={0,3}, to_apply=%AddComputation.6445, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.220 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.31, bf16[12,64]{1,0} %reduce.31), metadata={op_type="aten__mul" op_name="aten__norm.137/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6453 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.137/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6459 = bf16[] reduce(bf16[12,64]{1,0} %multiply.220, bf16[] %constant.6453), dimensions={0,1}, to_apply=%AddComputation.6455, metadata={op_type="aten__sum" op_name="aten__norm.137/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6460 = bf16[] sqrt(bf16[] %reduce.6459), metadata={op_type="aten__sqrt" op_name="aten__norm.137/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10478 = bf16[1]{0} reshape(bf16[] %sqrt.6460), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6405 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3365), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6406 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6405), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.50 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6404, bf16[768,512]{0,1} %transpose.6406), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6409 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.50, bf16[768,768]{0,1} %dot.50), metadata={op_type="aten__mul" op_name="aten__norm.138/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6410 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.138/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6416 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6409, bf16[] %constant.6410), dimensions={0,1}, to_apply=%AddComputation.6412, metadata={op_type="aten__sum" op_name="aten__norm.138/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6417 = bf16[] sqrt(bf16[] %reduce.6416), metadata={op_type="aten__sqrt" op_name="aten__norm.138/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10479 = bf16[1]{0} reshape(bf16[] %sqrt.6417), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6380 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6379), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6383 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.32 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.6380, bf16[] %constant.6383), dimensions={0,2}, to_apply=%AddComputation.6385, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.219 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.32, bf16[12,64]{1,0} %reduce.32), metadata={op_type="aten__mul" op_name="aten__norm.139/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6393 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.139/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6399 = bf16[] reduce(bf16[12,64]{1,0} %multiply.219, bf16[] %constant.6393), dimensions={0,1}, to_apply=%AddComputation.6395, metadata={op_type="aten__sum" op_name="aten__norm.139/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6400 = bf16[] sqrt(bf16[] %reduce.6399), metadata={op_type="aten__sqrt" op_name="aten__norm.139/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10480 = bf16[1]{0} reshape(bf16[] %sqrt.6400), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6354 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3537), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6355 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.6354), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.6357 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.6355), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6358 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6357), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.51 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6353, bf16[768,512]{0,1} %transpose.6358), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6361 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.51, bf16[768,768]{0,1} %dot.51), metadata={op_type="aten__mul" op_name="aten__norm.140/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6362 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.140/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6368 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6361, bf16[] %constant.6362), dimensions={0,1}, to_apply=%AddComputation.6364, metadata={op_type="aten__sum" op_name="aten__norm.140/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6369 = bf16[] sqrt(bf16[] %reduce.6368), metadata={op_type="aten__sqrt" op_name="aten__norm.140/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10481 = bf16[1]{0} reshape(bf16[] %sqrt.6369), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6335 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6341 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6334, bf16[] %constant.6335), dimensions={0,1}, to_apply=%AddComputation.6337, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6344 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6341, bf16[768]{0} %reduce.6341), metadata={op_type="aten__mul" op_name="aten__norm.141/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6345 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.141/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6351 = bf16[] reduce(bf16[768]{0} %multiply.6344, bf16[] %constant.6345), dimensions={0}, to_apply=%AddComputation.6347, metadata={op_type="aten__sum" op_name="aten__norm.141/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6352 = bf16[] sqrt(bf16[] %reduce.6351), metadata={op_type="aten__sqrt" op_name="aten__norm.141/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10482 = bf16[1]{0} reshape(bf16[] %sqrt.6352), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.6289 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6264, bf16[4,128,768]{2,1,0} %reshape.3559), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.6290 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6296 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6289, bf16[] %constant.6290), dimensions={0,1}, to_apply=%AddComputation.6292, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6299 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6296, bf16[768]{0} %reduce.6296), metadata={op_type="aten__mul" op_name="aten__norm.142/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6300 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.142/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6306 = bf16[] reduce(bf16[768]{0} %multiply.6299, bf16[] %constant.6300), dimensions={0}, to_apply=%AddComputation.6302, metadata={op_type="aten__sum" op_name="aten__norm.142/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6307 = bf16[] sqrt(bf16[] %reduce.6306), metadata={op_type="aten__sqrt" op_name="aten__norm.142/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10483 = bf16[1]{0} reshape(bf16[] %sqrt.6307), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6265 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6271 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.6264, bf16[] %constant.6265), dimensions={0,1}, to_apply=%AddComputation.6267, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6274 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6271, bf16[768]{0} %reduce.6271), metadata={op_type="aten__mul" op_name="aten__norm.143/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6275 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.143/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6281 = bf16[] reduce(bf16[768]{0} %multiply.6274, bf16[] %constant.6275), dimensions={0}, to_apply=%AddComputation.6277, metadata={op_type="aten__sum" op_name="aten__norm.143/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6282 = bf16[] sqrt(bf16[] %reduce.6281), metadata={op_type="aten__sqrt" op_name="aten__norm.143/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10484 = bf16[1]{0} reshape(bf16[] %sqrt.6282), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6241 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3567), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6242 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6241), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.52 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.6240, bf16[768,512]{0,1} %transpose.6242), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6245 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.52, bf16[3072,768]{0,1} %dot.52), metadata={op_type="aten__mul" op_name="aten__norm.144/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6246 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.144/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6252 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.6245, bf16[] %constant.6246), dimensions={0,1}, to_apply=%AddComputation.6248, metadata={op_type="aten__sum" op_name="aten__norm.144/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6253 = bf16[] sqrt(bf16[] %reduce.6252), metadata={op_type="aten__sqrt" op_name="aten__norm.144/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10485 = bf16[1]{0} reshape(bf16[] %sqrt.6253), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6222 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6228 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.23, bf16[] %constant.6222), dimensions={0,1}, to_apply=%AddComputation.6224, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6231 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.6228, bf16[3072]{0} %reduce.6228), metadata={op_type="aten__mul" op_name="aten__norm.145/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6232 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.145/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6238 = bf16[] reduce(bf16[3072]{0} %multiply.6231, bf16[] %constant.6232), dimensions={0}, to_apply=%AddComputation.6234, metadata={op_type="aten__sum" op_name="aten__norm.145/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6239 = bf16[] sqrt(bf16[] %reduce.6238), metadata={op_type="aten__sqrt" op_name="aten__norm.145/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10486 = bf16[1]{0} reshape(bf16[] %sqrt.6239), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6195 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.41), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6196 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.6195), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.53 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.6194, bf16[3072,512]{0,1} %transpose.6196), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6199 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.53, bf16[768,3072]{0,1} %dot.53), metadata={op_type="aten__mul" op_name="aten__norm.146/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6200 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.146/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6206 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.6199, bf16[] %constant.6200), dimensions={0,1}, to_apply=%AddComputation.6202, metadata={op_type="aten__sum" op_name="aten__norm.146/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6207 = bf16[] sqrt(bf16[] %reduce.6206), metadata={op_type="aten__sqrt" op_name="aten__norm.146/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10487 = bf16[1]{0} reshape(bf16[] %sqrt.6207), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6176 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6182 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6175, bf16[] %constant.6176), dimensions={0,1}, to_apply=%AddComputation.6178, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6185 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6182, bf16[768]{0} %reduce.6182), metadata={op_type="aten__mul" op_name="aten__norm.147/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6186 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.147/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6192 = bf16[] reduce(bf16[768]{0} %multiply.6185, bf16[] %constant.6186), dimensions={0}, to_apply=%AddComputation.6188, metadata={op_type="aten__sum" op_name="aten__norm.147/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6193 = bf16[] sqrt(bf16[] %reduce.6192), metadata={op_type="aten__sqrt" op_name="aten__norm.147/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10488 = bf16[1]{0} reshape(bf16[] %sqrt.6193), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.6130 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.6105, bf16[4,128,768]{2,1,0} %reshape.3652), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.6131 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6137 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.6130, bf16[] %constant.6131), dimensions={0,1}, to_apply=%AddComputation.6133, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6140 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6137, bf16[768]{0} %reduce.6137), metadata={op_type="aten__mul" op_name="aten__norm.148/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6141 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.148/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6147 = bf16[] reduce(bf16[768]{0} %multiply.6140, bf16[] %constant.6141), dimensions={0}, to_apply=%AddComputation.6143, metadata={op_type="aten__sum" op_name="aten__norm.148/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6148 = bf16[] sqrt(bf16[] %reduce.6147), metadata={op_type="aten__sqrt" op_name="aten__norm.148/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10489 = bf16[1]{0} reshape(bf16[] %sqrt.6148), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.6106 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.6112 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.6105, bf16[] %constant.6106), dimensions={0,1}, to_apply=%AddComputation.6108, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.6115 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6112, bf16[768]{0} %reduce.6112), metadata={op_type="aten__mul" op_name="aten__norm.149/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6116 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.149/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6122 = bf16[] reduce(bf16[768]{0} %multiply.6115, bf16[] %constant.6116), dimensions={0}, to_apply=%AddComputation.6118, metadata={op_type="aten__sum" op_name="aten__norm.149/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6123 = bf16[] sqrt(bf16[] %reduce.6122), metadata={op_type="aten__sqrt" op_name="aten__norm.149/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10490 = bf16[1]{0} reshape(bf16[] %sqrt.6123), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6060 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6061 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6060), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.54 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6059, bf16[768,512]{0,1} %transpose.6061), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6064 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.54, bf16[768,768]{0,1} %dot.54), metadata={op_type="aten__mul" op_name="aten__norm.150/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6065 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.150/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6071 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6064, bf16[] %constant.6065), dimensions={0,1}, to_apply=%AddComputation.6067, metadata={op_type="aten__sum" op_name="aten__norm.150/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6072 = bf16[] sqrt(bf16[] %reduce.6071), metadata={op_type="aten__sqrt" op_name="aten__norm.150/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10491 = bf16[1]{0} reshape(bf16[] %sqrt.6072), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6035 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.6034), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.6038 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.33 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.6035, bf16[] %constant.6038), dimensions={0,2}, to_apply=%AddComputation.6040, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.218 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.33, bf16[12,64]{1,0} %reduce.33), metadata={op_type="aten__mul" op_name="aten__norm.151/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6048 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.151/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6054 = bf16[] reduce(bf16[12,64]{1,0} %multiply.218, bf16[] %constant.6048), dimensions={0,1}, to_apply=%AddComputation.6050, metadata={op_type="aten__sum" op_name="aten__norm.151/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6055 = bf16[] sqrt(bf16[] %reduce.6054), metadata={op_type="aten__sqrt" op_name="aten__norm.151/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10492 = bf16[1]{0} reshape(bf16[] %sqrt.6055), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.6019 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.6020 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.6019), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.55 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.6018, bf16[768,512]{0,1} %transpose.6020), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.6023 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.55, bf16[768,768]{0,1} %dot.55), metadata={op_type="aten__mul" op_name="aten__norm.152/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6024 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.152/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6030 = bf16[] reduce(bf16[768,768]{0,1} %multiply.6023, bf16[] %constant.6024), dimensions={0,1}, to_apply=%AddComputation.6026, metadata={op_type="aten__sum" op_name="aten__norm.152/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6031 = bf16[] sqrt(bf16[] %reduce.6030), metadata={op_type="aten__sqrt" op_name="aten__norm.152/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10493 = bf16[1]{0} reshape(bf16[] %sqrt.6031), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5992 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5991), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5996 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.34 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.5992, bf16[] %constant.5996), dimensions={0,3}, to_apply=%AddComputation.5998, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.217 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.34, bf16[12,64]{1,0} %reduce.34), metadata={op_type="aten__mul" op_name="aten__norm.153/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.6006 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.153/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.6012 = bf16[] reduce(bf16[12,64]{1,0} %multiply.217, bf16[] %constant.6006), dimensions={0,1}, to_apply=%AddComputation.6008, metadata={op_type="aten__sum" op_name="aten__norm.153/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.6013 = bf16[] sqrt(bf16[] %reduce.6012), metadata={op_type="aten__sqrt" op_name="aten__norm.153/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10494 = bf16[1]{0} reshape(bf16[] %sqrt.6013), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5958 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3660), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5959 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5958), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.56 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5957, bf16[768,512]{0,1} %transpose.5959), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5962 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.56, bf16[768,768]{0,1} %dot.56), metadata={op_type="aten__mul" op_name="aten__norm.154/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5963 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.154/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5969 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5962, bf16[] %constant.5963), dimensions={0,1}, to_apply=%AddComputation.5965, metadata={op_type="aten__sum" op_name="aten__norm.154/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5970 = bf16[] sqrt(bf16[] %reduce.5969), metadata={op_type="aten__sqrt" op_name="aten__norm.154/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10495 = bf16[1]{0} reshape(bf16[] %sqrt.5970), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5933 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5932), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5936 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.35 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.5933, bf16[] %constant.5936), dimensions={0,2}, to_apply=%AddComputation.5938, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.216 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.35, bf16[12,64]{1,0} %reduce.35), metadata={op_type="aten__mul" op_name="aten__norm.155/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5946 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.155/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5952 = bf16[] reduce(bf16[12,64]{1,0} %multiply.216, bf16[] %constant.5946), dimensions={0,1}, to_apply=%AddComputation.5948, metadata={op_type="aten__sum" op_name="aten__norm.155/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5953 = bf16[] sqrt(bf16[] %reduce.5952), metadata={op_type="aten__sqrt" op_name="aten__norm.155/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10496 = bf16[1]{0} reshape(bf16[] %sqrt.5953), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5907 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.3832), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5908 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5907), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5910 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5908), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5911 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5910), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.57 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5906, bf16[768,512]{0,1} %transpose.5911), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5914 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.57, bf16[768,768]{0,1} %dot.57), metadata={op_type="aten__mul" op_name="aten__norm.156/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5915 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.156/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5921 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5914, bf16[] %constant.5915), dimensions={0,1}, to_apply=%AddComputation.5917, metadata={op_type="aten__sum" op_name="aten__norm.156/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5922 = bf16[] sqrt(bf16[] %reduce.5921), metadata={op_type="aten__sqrt" op_name="aten__norm.156/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10497 = bf16[1]{0} reshape(bf16[] %sqrt.5922), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5888 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5894 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5887, bf16[] %constant.5888), dimensions={0,1}, to_apply=%AddComputation.5890, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5897 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5894, bf16[768]{0} %reduce.5894), metadata={op_type="aten__mul" op_name="aten__norm.157/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5898 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.157/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5904 = bf16[] reduce(bf16[768]{0} %multiply.5897, bf16[] %constant.5898), dimensions={0}, to_apply=%AddComputation.5900, metadata={op_type="aten__sum" op_name="aten__norm.157/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5905 = bf16[] sqrt(bf16[] %reduce.5904), metadata={op_type="aten__sqrt" op_name="aten__norm.157/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10498 = bf16[1]{0} reshape(bf16[] %sqrt.5905), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.5842 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5817, bf16[4,128,768]{2,1,0} %reshape.3854), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.5843 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5849 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5842, bf16[] %constant.5843), dimensions={0,1}, to_apply=%AddComputation.5845, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5852 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5849, bf16[768]{0} %reduce.5849), metadata={op_type="aten__mul" op_name="aten__norm.158/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5853 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.158/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5859 = bf16[] reduce(bf16[768]{0} %multiply.5852, bf16[] %constant.5853), dimensions={0}, to_apply=%AddComputation.5855, metadata={op_type="aten__sum" op_name="aten__norm.158/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5860 = bf16[] sqrt(bf16[] %reduce.5859), metadata={op_type="aten__sqrt" op_name="aten__norm.158/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10499 = bf16[1]{0} reshape(bf16[] %sqrt.5860), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5818 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5824 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.5817, bf16[] %constant.5818), dimensions={0,1}, to_apply=%AddComputation.5820, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5827 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5824, bf16[768]{0} %reduce.5824), metadata={op_type="aten__mul" op_name="aten__norm.159/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5828 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.159/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5834 = bf16[] reduce(bf16[768]{0} %multiply.5827, bf16[] %constant.5828), dimensions={0}, to_apply=%AddComputation.5830, metadata={op_type="aten__sum" op_name="aten__norm.159/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5835 = bf16[] sqrt(bf16[] %reduce.5834), metadata={op_type="aten__sqrt" op_name="aten__norm.159/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10500 = bf16[1]{0} reshape(bf16[] %sqrt.5835), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5794 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3862), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5795 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5794), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.58 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.5793, bf16[768,512]{0,1} %transpose.5795), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5798 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.58, bf16[3072,768]{0,1} %dot.58), metadata={op_type="aten__mul" op_name="aten__norm.160/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5799 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.160/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5805 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.5798, bf16[] %constant.5799), dimensions={0,1}, to_apply=%AddComputation.5801, metadata={op_type="aten__sum" op_name="aten__norm.160/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5806 = bf16[] sqrt(bf16[] %reduce.5805), metadata={op_type="aten__sqrt" op_name="aten__norm.160/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10501 = bf16[1]{0} reshape(bf16[] %sqrt.5806), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5775 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5781 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.22, bf16[] %constant.5775), dimensions={0,1}, to_apply=%AddComputation.5777, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5784 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.5781, bf16[3072]{0} %reduce.5781), metadata={op_type="aten__mul" op_name="aten__norm.161/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5785 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.161/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5791 = bf16[] reduce(bf16[3072]{0} %multiply.5784, bf16[] %constant.5785), dimensions={0}, to_apply=%AddComputation.5787, metadata={op_type="aten__sum" op_name="aten__norm.161/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5792 = bf16[] sqrt(bf16[] %reduce.5791), metadata={op_type="aten__sqrt" op_name="aten__norm.161/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10502 = bf16[1]{0} reshape(bf16[] %sqrt.5792), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5748 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.42), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5749 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.5748), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.59 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.5747, bf16[3072,512]{0,1} %transpose.5749), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5752 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.59, bf16[768,3072]{0,1} %dot.59), metadata={op_type="aten__mul" op_name="aten__norm.162/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5753 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.162/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5759 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.5752, bf16[] %constant.5753), dimensions={0,1}, to_apply=%AddComputation.5755, metadata={op_type="aten__sum" op_name="aten__norm.162/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5760 = bf16[] sqrt(bf16[] %reduce.5759), metadata={op_type="aten__sqrt" op_name="aten__norm.162/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10503 = bf16[1]{0} reshape(bf16[] %sqrt.5760), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5729 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5735 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5728, bf16[] %constant.5729), dimensions={0,1}, to_apply=%AddComputation.5731, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5738 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5735, bf16[768]{0} %reduce.5735), metadata={op_type="aten__mul" op_name="aten__norm.163/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5739 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.163/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5745 = bf16[] reduce(bf16[768]{0} %multiply.5738, bf16[] %constant.5739), dimensions={0}, to_apply=%AddComputation.5741, metadata={op_type="aten__sum" op_name="aten__norm.163/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5746 = bf16[] sqrt(bf16[] %reduce.5745), metadata={op_type="aten__sqrt" op_name="aten__norm.163/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10504 = bf16[1]{0} reshape(bf16[] %sqrt.5746), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.5683 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5658, bf16[4,128,768]{2,1,0} %reshape.3947), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.5684 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5690 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5683, bf16[] %constant.5684), dimensions={0,1}, to_apply=%AddComputation.5686, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5693 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5690, bf16[768]{0} %reduce.5690), metadata={op_type="aten__mul" op_name="aten__norm.164/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5694 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.164/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5700 = bf16[] reduce(bf16[768]{0} %multiply.5693, bf16[] %constant.5694), dimensions={0}, to_apply=%AddComputation.5696, metadata={op_type="aten__sum" op_name="aten__norm.164/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5701 = bf16[] sqrt(bf16[] %reduce.5700), metadata={op_type="aten__sqrt" op_name="aten__norm.164/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10505 = bf16[1]{0} reshape(bf16[] %sqrt.5701), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5659 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5665 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.5658, bf16[] %constant.5659), dimensions={0,1}, to_apply=%AddComputation.5661, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5668 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5665, bf16[768]{0} %reduce.5665), metadata={op_type="aten__mul" op_name="aten__norm.165/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5669 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.165/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5675 = bf16[] reduce(bf16[768]{0} %multiply.5668, bf16[] %constant.5669), dimensions={0}, to_apply=%AddComputation.5671, metadata={op_type="aten__sum" op_name="aten__norm.165/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5676 = bf16[] sqrt(bf16[] %reduce.5675), metadata={op_type="aten__sqrt" op_name="aten__norm.165/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10506 = bf16[1]{0} reshape(bf16[] %sqrt.5676), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5613 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5614 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5613), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.60 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5612, bf16[768,512]{0,1} %transpose.5614), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5617 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.60, bf16[768,768]{0,1} %dot.60), metadata={op_type="aten__mul" op_name="aten__norm.166/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5618 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.166/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5624 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5617, bf16[] %constant.5618), dimensions={0,1}, to_apply=%AddComputation.5620, metadata={op_type="aten__sum" op_name="aten__norm.166/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5625 = bf16[] sqrt(bf16[] %reduce.5624), metadata={op_type="aten__sqrt" op_name="aten__norm.166/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10507 = bf16[1]{0} reshape(bf16[] %sqrt.5625), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5588 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5587), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5591 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.36 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.5588, bf16[] %constant.5591), dimensions={0,2}, to_apply=%AddComputation.5593, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.215 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.36, bf16[12,64]{1,0} %reduce.36), metadata={op_type="aten__mul" op_name="aten__norm.167/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5601 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.167/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5607 = bf16[] reduce(bf16[12,64]{1,0} %multiply.215, bf16[] %constant.5601), dimensions={0,1}, to_apply=%AddComputation.5603, metadata={op_type="aten__sum" op_name="aten__norm.167/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5608 = bf16[] sqrt(bf16[] %reduce.5607), metadata={op_type="aten__sqrt" op_name="aten__norm.167/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10508 = bf16[1]{0} reshape(bf16[] %sqrt.5608), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5572 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5573 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5572), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.61 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5571, bf16[768,512]{0,1} %transpose.5573), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5576 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.61, bf16[768,768]{0,1} %dot.61), metadata={op_type="aten__mul" op_name="aten__norm.168/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5577 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.168/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5583 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5576, bf16[] %constant.5577), dimensions={0,1}, to_apply=%AddComputation.5579, metadata={op_type="aten__sum" op_name="aten__norm.168/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5584 = bf16[] sqrt(bf16[] %reduce.5583), metadata={op_type="aten__sqrt" op_name="aten__norm.168/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10509 = bf16[1]{0} reshape(bf16[] %sqrt.5584), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5545 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5544), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5549 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.37 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.5545, bf16[] %constant.5549), dimensions={0,3}, to_apply=%AddComputation.5551, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.214 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.37, bf16[12,64]{1,0} %reduce.37), metadata={op_type="aten__mul" op_name="aten__norm.169/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5559 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.169/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5565 = bf16[] reduce(bf16[12,64]{1,0} %multiply.214, bf16[] %constant.5559), dimensions={0,1}, to_apply=%AddComputation.5561, metadata={op_type="aten__sum" op_name="aten__norm.169/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5566 = bf16[] sqrt(bf16[] %reduce.5565), metadata={op_type="aten__sqrt" op_name="aten__norm.169/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10510 = bf16[1]{0} reshape(bf16[] %sqrt.5566), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5511 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.3955), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5512 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5511), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.62 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5510, bf16[768,512]{0,1} %transpose.5512), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5515 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.62, bf16[768,768]{0,1} %dot.62), metadata={op_type="aten__mul" op_name="aten__norm.170/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5516 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.170/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5522 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5515, bf16[] %constant.5516), dimensions={0,1}, to_apply=%AddComputation.5518, metadata={op_type="aten__sum" op_name="aten__norm.170/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5523 = bf16[] sqrt(bf16[] %reduce.5522), metadata={op_type="aten__sqrt" op_name="aten__norm.170/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10511 = bf16[1]{0} reshape(bf16[] %sqrt.5523), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5486 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5485), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5489 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.38 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.5486, bf16[] %constant.5489), dimensions={0,2}, to_apply=%AddComputation.5491, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.213 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.38, bf16[12,64]{1,0} %reduce.38), metadata={op_type="aten__mul" op_name="aten__norm.171/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5499 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.171/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5505 = bf16[] reduce(bf16[12,64]{1,0} %multiply.213, bf16[] %constant.5499), dimensions={0,1}, to_apply=%AddComputation.5501, metadata={op_type="aten__sum" op_name="aten__norm.171/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5506 = bf16[] sqrt(bf16[] %reduce.5505), metadata={op_type="aten__sqrt" op_name="aten__norm.171/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10512 = bf16[1]{0} reshape(bf16[] %sqrt.5506), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5460 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.4127), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5461 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5460), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5463 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5461), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5464 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5463), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.63 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5459, bf16[768,512]{0,1} %transpose.5464), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5467 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.63, bf16[768,768]{0,1} %dot.63), metadata={op_type="aten__mul" op_name="aten__norm.172/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5468 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.172/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5474 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5467, bf16[] %constant.5468), dimensions={0,1}, to_apply=%AddComputation.5470, metadata={op_type="aten__sum" op_name="aten__norm.172/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5475 = bf16[] sqrt(bf16[] %reduce.5474), metadata={op_type="aten__sqrt" op_name="aten__norm.172/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10513 = bf16[1]{0} reshape(bf16[] %sqrt.5475), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5441 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5447 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5440, bf16[] %constant.5441), dimensions={0,1}, to_apply=%AddComputation.5443, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5450 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5447, bf16[768]{0} %reduce.5447), metadata={op_type="aten__mul" op_name="aten__norm.173/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5451 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.173/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5457 = bf16[] reduce(bf16[768]{0} %multiply.5450, bf16[] %constant.5451), dimensions={0}, to_apply=%AddComputation.5453, metadata={op_type="aten__sum" op_name="aten__norm.173/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5458 = bf16[] sqrt(bf16[] %reduce.5457), metadata={op_type="aten__sqrt" op_name="aten__norm.173/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10514 = bf16[1]{0} reshape(bf16[] %sqrt.5458), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.5395 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5370, bf16[4,128,768]{2,1,0} %reshape.4149), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.5396 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5402 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5395, bf16[] %constant.5396), dimensions={0,1}, to_apply=%AddComputation.5398, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5405 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5402, bf16[768]{0} %reduce.5402), metadata={op_type="aten__mul" op_name="aten__norm.174/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5406 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.174/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5412 = bf16[] reduce(bf16[768]{0} %multiply.5405, bf16[] %constant.5406), dimensions={0}, to_apply=%AddComputation.5408, metadata={op_type="aten__sum" op_name="aten__norm.174/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5413 = bf16[] sqrt(bf16[] %reduce.5412), metadata={op_type="aten__sqrt" op_name="aten__norm.174/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10515 = bf16[1]{0} reshape(bf16[] %sqrt.5413), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5371 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5377 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.5370, bf16[] %constant.5371), dimensions={0,1}, to_apply=%AddComputation.5373, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5380 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5377, bf16[768]{0} %reduce.5377), metadata={op_type="aten__mul" op_name="aten__norm.175/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5381 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.175/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5387 = bf16[] reduce(bf16[768]{0} %multiply.5380, bf16[] %constant.5381), dimensions={0}, to_apply=%AddComputation.5383, metadata={op_type="aten__sum" op_name="aten__norm.175/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5388 = bf16[] sqrt(bf16[] %reduce.5387), metadata={op_type="aten__sqrt" op_name="aten__norm.175/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10516 = bf16[1]{0} reshape(bf16[] %sqrt.5388), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5347 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4157), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5348 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5347), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.64 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.5346, bf16[768,512]{0,1} %transpose.5348), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5351 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.64, bf16[3072,768]{0,1} %dot.64), metadata={op_type="aten__mul" op_name="aten__norm.176/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5352 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.176/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5358 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.5351, bf16[] %constant.5352), dimensions={0,1}, to_apply=%AddComputation.5354, metadata={op_type="aten__sum" op_name="aten__norm.176/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5359 = bf16[] sqrt(bf16[] %reduce.5358), metadata={op_type="aten__sqrt" op_name="aten__norm.176/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10517 = bf16[1]{0} reshape(bf16[] %sqrt.5359), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5328 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5334 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.20, bf16[] %constant.5328), dimensions={0,1}, to_apply=%AddComputation.5330, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5337 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.5334, bf16[3072]{0} %reduce.5334), metadata={op_type="aten__mul" op_name="aten__norm.177/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5338 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.177/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5344 = bf16[] reduce(bf16[3072]{0} %multiply.5337, bf16[] %constant.5338), dimensions={0}, to_apply=%AddComputation.5340, metadata={op_type="aten__sum" op_name="aten__norm.177/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5345 = bf16[] sqrt(bf16[] %reduce.5344), metadata={op_type="aten__sqrt" op_name="aten__norm.177/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10518 = bf16[1]{0} reshape(bf16[] %sqrt.5345), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5301 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.43), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5302 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.5301), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.65 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.5300, bf16[3072,512]{0,1} %transpose.5302), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5305 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.65, bf16[768,3072]{0,1} %dot.65), metadata={op_type="aten__mul" op_name="aten__norm.178/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5306 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.178/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5312 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.5305, bf16[] %constant.5306), dimensions={0,1}, to_apply=%AddComputation.5308, metadata={op_type="aten__sum" op_name="aten__norm.178/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5313 = bf16[] sqrt(bf16[] %reduce.5312), metadata={op_type="aten__sqrt" op_name="aten__norm.178/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10519 = bf16[1]{0} reshape(bf16[] %sqrt.5313), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5282 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5288 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5281, bf16[] %constant.5282), dimensions={0,1}, to_apply=%AddComputation.5284, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5291 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5288, bf16[768]{0} %reduce.5288), metadata={op_type="aten__mul" op_name="aten__norm.179/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5292 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.179/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5298 = bf16[] reduce(bf16[768]{0} %multiply.5291, bf16[] %constant.5292), dimensions={0}, to_apply=%AddComputation.5294, metadata={op_type="aten__sum" op_name="aten__norm.179/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5299 = bf16[] sqrt(bf16[] %reduce.5298), metadata={op_type="aten__sqrt" op_name="aten__norm.179/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10520 = bf16[1]{0} reshape(bf16[] %sqrt.5299), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.5236 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.5211, bf16[4,128,768]{2,1,0} %reshape.4242), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.5237 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5243 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.5236, bf16[] %constant.5237), dimensions={0,1}, to_apply=%AddComputation.5239, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5246 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5243, bf16[768]{0} %reduce.5243), metadata={op_type="aten__mul" op_name="aten__norm.180/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5247 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.180/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5253 = bf16[] reduce(bf16[768]{0} %multiply.5246, bf16[] %constant.5247), dimensions={0}, to_apply=%AddComputation.5249, metadata={op_type="aten__sum" op_name="aten__norm.180/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5254 = bf16[] sqrt(bf16[] %reduce.5253), metadata={op_type="aten__sqrt" op_name="aten__norm.180/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10521 = bf16[1]{0} reshape(bf16[] %sqrt.5254), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.5212 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5218 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.5211, bf16[] %constant.5212), dimensions={0,1}, to_apply=%AddComputation.5214, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5221 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5218, bf16[768]{0} %reduce.5218), metadata={op_type="aten__mul" op_name="aten__norm.181/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5222 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.181/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5228 = bf16[] reduce(bf16[768]{0} %multiply.5221, bf16[] %constant.5222), dimensions={0}, to_apply=%AddComputation.5224, metadata={op_type="aten__sum" op_name="aten__norm.181/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5229 = bf16[] sqrt(bf16[] %reduce.5228), metadata={op_type="aten__sqrt" op_name="aten__norm.181/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10522 = bf16[1]{0} reshape(bf16[] %sqrt.5229), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5166 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5167 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5166), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.66 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5165, bf16[768,512]{0,1} %transpose.5167), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5170 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.66, bf16[768,768]{0,1} %dot.66), metadata={op_type="aten__mul" op_name="aten__norm.182/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5171 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.182/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5177 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5170, bf16[] %constant.5171), dimensions={0,1}, to_apply=%AddComputation.5173, metadata={op_type="aten__sum" op_name="aten__norm.182/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5178 = bf16[] sqrt(bf16[] %reduce.5177), metadata={op_type="aten__sqrt" op_name="aten__norm.182/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10523 = bf16[1]{0} reshape(bf16[] %sqrt.5178), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5141 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5140), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5144 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.39 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.5141, bf16[] %constant.5144), dimensions={0,2}, to_apply=%AddComputation.5146, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.212 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.39, bf16[12,64]{1,0} %reduce.39), metadata={op_type="aten__mul" op_name="aten__norm.183/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5154 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.183/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5160 = bf16[] reduce(bf16[12,64]{1,0} %multiply.212, bf16[] %constant.5154), dimensions={0,1}, to_apply=%AddComputation.5156, metadata={op_type="aten__sum" op_name="aten__norm.183/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5161 = bf16[] sqrt(bf16[] %reduce.5160), metadata={op_type="aten__sqrt" op_name="aten__norm.183/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10524 = bf16[1]{0} reshape(bf16[] %sqrt.5161), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5125 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5126 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5125), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.67 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5124, bf16[768,512]{0,1} %transpose.5126), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5129 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.67, bf16[768,768]{0,1} %dot.67), metadata={op_type="aten__mul" op_name="aten__norm.184/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5130 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.184/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5136 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5129, bf16[] %constant.5130), dimensions={0,1}, to_apply=%AddComputation.5132, metadata={op_type="aten__sum" op_name="aten__norm.184/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5137 = bf16[] sqrt(bf16[] %reduce.5136), metadata={op_type="aten__sqrt" op_name="aten__norm.184/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10525 = bf16[1]{0} reshape(bf16[] %sqrt.5137), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5098 = bf16[4,12,64,128]{3,2,1,0} reshape(bf16[48,64,128]{2,1,0} %dot.5097), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5102 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.40 = bf16[12,64]{1,0} reduce(bf16[4,12,64,128]{3,2,1,0} %reshape.5098, bf16[] %constant.5102), dimensions={0,3}, to_apply=%AddComputation.5104, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.211 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.40, bf16[12,64]{1,0} %reduce.40), metadata={op_type="aten__mul" op_name="aten__norm.185/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5112 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.185/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5118 = bf16[] reduce(bf16[12,64]{1,0} %multiply.211, bf16[] %constant.5112), dimensions={0,1}, to_apply=%AddComputation.5114, metadata={op_type="aten__sum" op_name="aten__norm.185/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5119 = bf16[] sqrt(bf16[] %reduce.5118), metadata={op_type="aten__sqrt" op_name="aten__norm.185/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10526 = bf16[1]{0} reshape(bf16[] %sqrt.5119), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5064 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4250), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5065 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5064), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.68 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5063, bf16[768,512]{0,1} %transpose.5065), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5068 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.68, bf16[768,768]{0,1} %dot.68), metadata={op_type="aten__mul" op_name="aten__norm.186/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5069 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.186/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5075 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5068, bf16[] %constant.5069), dimensions={0,1}, to_apply=%AddComputation.5071, metadata={op_type="aten__sum" op_name="aten__norm.186/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5076 = bf16[] sqrt(bf16[] %reduce.5075), metadata={op_type="aten__sqrt" op_name="aten__norm.186/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10527 = bf16[1]{0} reshape(bf16[] %sqrt.5076), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5039 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.5038), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.5042 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.41 = bf16[12,64]{1,0} reduce(bf16[4,12,128,64]{3,2,1,0} %reshape.5039, bf16[] %constant.5042), dimensions={0,2}, to_apply=%AddComputation.5044, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.210 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.41, bf16[12,64]{1,0} %reduce.41), metadata={op_type="aten__mul" op_name="aten__norm.187/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5052 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.187/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5058 = bf16[] reduce(bf16[12,64]{1,0} %multiply.210, bf16[] %constant.5052), dimensions={0,1}, to_apply=%AddComputation.5054, metadata={op_type="aten__sum" op_name="aten__norm.187/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5059 = bf16[] sqrt(bf16[] %reduce.5058), metadata={op_type="aten__sqrt" op_name="aten__norm.187/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10528 = bf16[1]{0} reshape(bf16[] %sqrt.5059), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.5013 = bf16[4,12,128,64]{3,2,1,0} reshape(bf16[48,128,64]{2,1,0} %dot.4422), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5014 = bf16[4,128,12,64]{3,1,2,0} transpose(bf16[4,12,128,64]{3,2,1,0} %reshape.5013), dimensions={0,2,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.5016 = bf16[512,768]{1,0} reshape(bf16[4,128,12,64]{3,1,2,0} %transpose.5014), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.5017 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.5016), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.69 = bf16[768,768]{0,1} dot(bf16[512,768]{1,0} %reshape.5012, bf16[768,512]{0,1} %transpose.5017), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.5020 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.69, bf16[768,768]{0,1} %dot.69), metadata={op_type="aten__mul" op_name="aten__norm.188/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5021 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.188/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5027 = bf16[] reduce(bf16[768,768]{0,1} %multiply.5020, bf16[] %constant.5021), dimensions={0,1}, to_apply=%AddComputation.5023, metadata={op_type="aten__sum" op_name="aten__norm.188/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5028 = bf16[] sqrt(bf16[] %reduce.5027), metadata={op_type="aten__sqrt" op_name="aten__norm.188/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10529 = bf16[1]{0} reshape(bf16[] %sqrt.5028), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4994 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.5000 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.4993, bf16[] %constant.4994), dimensions={0,1}, to_apply=%AddComputation.4996, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.5003 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5000, bf16[768]{0} %reduce.5000), metadata={op_type="aten__mul" op_name="aten__norm.189/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.5004 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.189/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.5010 = bf16[] reduce(bf16[768]{0} %multiply.5003, bf16[] %constant.5004), dimensions={0}, to_apply=%AddComputation.5006, metadata={op_type="aten__sum" op_name="aten__norm.189/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.5011 = bf16[] sqrt(bf16[] %reduce.5010), metadata={op_type="aten__sqrt" op_name="aten__norm.189/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10530 = bf16[1]{0} reshape(bf16[] %sqrt.5011), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.4948 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %add.4923, bf16[4,128,768]{2,1,0} %reshape.4444), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4949 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4955 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.4948, bf16[] %constant.4949), dimensions={0,1}, to_apply=%AddComputation.4951, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4958 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4955, bf16[768]{0} %reduce.4955), metadata={op_type="aten__mul" op_name="aten__norm.190/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4959 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.190/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4965 = bf16[] reduce(bf16[768]{0} %multiply.4958, bf16[] %constant.4959), dimensions={0}, to_apply=%AddComputation.4961, metadata={op_type="aten__sum" op_name="aten__norm.190/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4966 = bf16[] sqrt(bf16[] %reduce.4965), metadata={op_type="aten__sqrt" op_name="aten__norm.190/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10531 = bf16[1]{0} reshape(bf16[] %sqrt.4966), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4924 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4930 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %add.4923, bf16[] %constant.4924), dimensions={0,1}, to_apply=%AddComputation.4926, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4933 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4930, bf16[768]{0} %reduce.4930), metadata={op_type="aten__mul" op_name="aten__norm.191/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4934 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.191/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4940 = bf16[] reduce(bf16[768]{0} %multiply.4933, bf16[] %constant.4934), dimensions={0}, to_apply=%AddComputation.4936, metadata={op_type="aten__sum" op_name="aten__norm.191/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4941 = bf16[] sqrt(bf16[] %reduce.4940), metadata={op_type="aten__sqrt" op_name="aten__norm.191/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10532 = bf16[1]{0} reshape(bf16[] %sqrt.4941), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.4900 = bf16[512,768]{1,0} reshape(bf16[4,128,768]{2,1,0} %add.4452), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.4901 = bf16[768,512]{0,1} transpose(bf16[512,768]{1,0} %reshape.4900), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.70 = bf16[3072,768]{0,1} dot(bf16[512,3072]{1,0} %reshape.4899, bf16[768,512]{0,1} %transpose.4901), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.4904 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.70, bf16[3072,768]{0,1} %dot.70), metadata={op_type="aten__mul" op_name="aten__norm.192/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4905 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.192/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4911 = bf16[] reduce(bf16[3072,768]{0,1} %multiply.4904, bf16[] %constant.4905), dimensions={0,1}, to_apply=%AddComputation.4907, metadata={op_type="aten__sum" op_name="aten__norm.192/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4912 = bf16[] sqrt(bf16[] %reduce.4911), metadata={op_type="aten__sqrt" op_name="aten__norm.192/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10533 = bf16[1]{0} reshape(bf16[] %sqrt.4912), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4881 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4887 = bf16[3072]{0} reduce(bf16[4,128,3072]{2,1,0} %multiply.19, bf16[] %constant.4881), dimensions={0,1}, to_apply=%AddComputation.4883, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4890 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.4887, bf16[3072]{0} %reduce.4887), metadata={op_type="aten__mul" op_name="aten__norm.193/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4891 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.193/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4897 = bf16[] reduce(bf16[3072]{0} %multiply.4890, bf16[] %constant.4891), dimensions={0}, to_apply=%AddComputation.4893, metadata={op_type="aten__sum" op_name="aten__norm.193/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4898 = bf16[] sqrt(bf16[] %reduce.4897), metadata={op_type="aten__sqrt" op_name="aten__norm.193/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10534 = bf16[1]{0} reshape(bf16[] %sqrt.4898), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %reshape.4854 = bf16[512,3072]{1,0} reshape(bf16[4,128,3072]{2,1,0} %custom-call.44), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.4855 = bf16[3072,512]{0,1} transpose(bf16[512,3072]{1,0} %reshape.4854), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.71 = bf16[768,3072]{0,1} dot(bf16[512,768]{1,0} %reshape.4853, bf16[3072,512]{0,1} %transpose.4855), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.4858 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.71, bf16[768,3072]{0,1} %dot.71), metadata={op_type="aten__mul" op_name="aten__norm.194/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4859 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.194/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4865 = bf16[] reduce(bf16[768,3072]{0,1} %multiply.4858, bf16[] %constant.4859), dimensions={0,1}, to_apply=%AddComputation.4861, metadata={op_type="aten__sum" op_name="aten__norm.194/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4866 = bf16[] sqrt(bf16[] %reduce.4865), metadata={op_type="aten__sqrt" op_name="aten__norm.194/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10535 = bf16[1]{0} reshape(bf16[] %sqrt.4866), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4835 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4841 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.4834, bf16[] %constant.4835), dimensions={0,1}, to_apply=%AddComputation.4837, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4844 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4841, bf16[768]{0} %reduce.4841), metadata={op_type="aten__mul" op_name="aten__norm.195/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4845 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.195/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4851 = bf16[] reduce(bf16[768]{0} %multiply.4844, bf16[] %constant.4845), dimensions={0}, to_apply=%AddComputation.4847, metadata={op_type="aten__sum" op_name="aten__norm.195/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4852 = bf16[] sqrt(bf16[] %reduce.4851), metadata={op_type="aten__sqrt" op_name="aten__norm.195/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10536 = bf16[1]{0} reshape(bf16[] %sqrt.4852), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.4789 = bf16[4,128,768]{2,1,0} multiply(bf16[4,128,768]{2,1,0} %pad, bf16[4,128,768]{2,1,0} %reshape.4537), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4790 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4796 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %multiply.4789, bf16[] %constant.4790), dimensions={0,1}, to_apply=%AddComputation.4792, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4799 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4796, bf16[768]{0} %reduce.4796), metadata={op_type="aten__mul" op_name="aten__norm.196/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4800 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.196/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4806 = bf16[] reduce(bf16[768]{0} %multiply.4799, bf16[] %constant.4800), dimensions={0}, to_apply=%AddComputation.4802, metadata={op_type="aten__sum" op_name="aten__norm.196/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4807 = bf16[] sqrt(bf16[] %reduce.4806), metadata={op_type="aten__sqrt" op_name="aten__norm.196/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10537 = bf16[1]{0} reshape(bf16[] %sqrt.4807), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4765 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4771 = bf16[768]{0} reduce(bf16[4,128,768]{2,1,0} %pad, bf16[] %constant.4765), dimensions={0,1}, to_apply=%AddComputation.4767, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4774 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4771, bf16[768]{0} %reduce.4771), metadata={op_type="aten__mul" op_name="aten__norm.197/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4775 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.197/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4781 = bf16[] reduce(bf16[768]{0} %multiply.4774, bf16[] %constant.4775), dimensions={0}, to_apply=%AddComputation.4777, metadata={op_type="aten__sum" op_name="aten__norm.197/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4782 = bf16[] sqrt(bf16[] %reduce.4781), metadata={op_type="aten__sqrt" op_name="aten__norm.197/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10538 = bf16[1]{0} reshape(bf16[] %sqrt.4782), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %slice.4733 = bf16[4,1,768]{2,1,0} slice(bf16[4,128,768]{2,1,0} %add.4545), slice={[0:4], [0:1], [0:768]}, metadata={op_type="xla__generic_slice" op_name="xla__generic_slice"}
  %reshape.4734 = bf16[4,768]{1,0} reshape(bf16[4,1,768]{2,1,0} %slice.4733), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.4735 = bf16[768,4]{0,1} transpose(bf16[4,768]{1,0} %reshape.4734), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.72 = bf16[768,768]{0,1} dot(bf16[4,768]{1,0} %multiply.4713, bf16[768,4]{0,1} %transpose.4735), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.4738 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.72, bf16[768,768]{0,1} %dot.72), metadata={op_type="aten__mul" op_name="aten__norm.198/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4739 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.198/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4745 = bf16[] reduce(bf16[768,768]{0,1} %multiply.4738, bf16[] %constant.4739), dimensions={0,1}, to_apply=%AddComputation.4741, metadata={op_type="aten__sum" op_name="aten__norm.198/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4746 = bf16[] sqrt(bf16[] %reduce.4745), metadata={op_type="aten__sqrt" op_name="aten__norm.198/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10539 = bf16[1]{0} reshape(bf16[] %sqrt.4746), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4714 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4720 = bf16[768]{0} reduce(bf16[4,768]{1,0} %multiply.4713, bf16[] %constant.4714), dimensions={0}, to_apply=%AddComputation.4716, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4723 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4720, bf16[768]{0} %reduce.4720), metadata={op_type="aten__mul" op_name="aten__norm.199/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4724 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.199/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4730 = bf16[] reduce(bf16[768]{0} %multiply.4723, bf16[] %constant.4724), dimensions={0}, to_apply=%AddComputation.4726, metadata={op_type="aten__sum" op_name="aten__norm.199/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4731 = bf16[] sqrt(bf16[] %reduce.4730), metadata={op_type="aten__sqrt" op_name="aten__norm.199/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10540 = bf16[1]{0} reshape(bf16[] %sqrt.4731), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %transpose.4679 = bf16[768,4]{0,1} transpose(bf16[4,768]{1,0} %multiply.4556), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.73 = bf16[2,768]{0,1} dot(bf16[4,2]{1,0} %add.0, bf16[768,4]{0,1} %transpose.4679), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %multiply.4682 = bf16[2,768]{0,1} multiply(bf16[2,768]{0,1} %dot.73, bf16[2,768]{0,1} %dot.73), metadata={op_type="aten__mul" op_name="aten__norm.200/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4683 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.200/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4689 = bf16[] reduce(bf16[2,768]{0,1} %multiply.4682, bf16[] %constant.4683), dimensions={0,1}, to_apply=%AddComputation.4685, metadata={op_type="aten__sum" op_name="aten__norm.200/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4690 = bf16[] sqrt(bf16[] %reduce.4689), metadata={op_type="aten__sqrt" op_name="aten__norm.200/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10541 = bf16[1]{0} reshape(bf16[] %sqrt.4690), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %constant.4661 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4667 = bf16[2]{0} reduce(bf16[4,2]{1,0} %add.0, bf16[] %constant.4661), dimensions={0}, to_apply=%AddComputation.4663, metadata={op_type="aten__sum" op_name="aten__sum"}
  %multiply.4670 = bf16[2]{0} multiply(bf16[2]{0} %reduce.4667, bf16[2]{0} %reduce.4667), metadata={op_type="aten__mul" op_name="aten__norm.201/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.4671 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.201/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.4677 = bf16[] reduce(bf16[2]{0} %multiply.4670, bf16[] %constant.4671), dimensions={0}, to_apply=%AddComputation.4673, metadata={op_type="aten__sum" op_name="aten__norm.201/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.4678 = bf16[] sqrt(bf16[] %reduce.4677), metadata={op_type="aten__sqrt" op_name="aten__norm.201/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reshape.10542 = bf16[1]{0} reshape(bf16[] %sqrt.4678), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %concatenate.10543 = bf16[201]{0} concatenate(bf16[1]{0} %reshape.10342, bf16[1]{0} %reshape.10343, bf16[1]{0} %reshape.10344, bf16[1]{0} %reshape.10345, bf16[1]{0} %reshape.10346, /*index=5*/bf16[1]{0} %reshape.10347, bf16[1]{0} %reshape.10348, bf16[1]{0} %reshape.10349, bf16[1]{0} %reshape.10350, bf16[1]{0} %reshape.10351, /*index=10*/bf16[1]{0} %reshape.10352, bf16[1]{0} %reshape.10353, bf16[1]{0} %reshape.10354, bf16[1]{0} %reshape.10355, bf16[1]{0} %reshape.10356, /*index=15*/bf16[1]{0} %reshape.10357, bf16[1]{0} %reshape.10358, bf16[1]{0} %reshape.10359, bf16[1]{0} %reshape.10360, bf16[1]{0} %reshape.10361, /*index=20*/bf16[1]{0} %reshape.10362, bf16[1]{0} %reshape.10363, bf16[1]{0} %reshape.10364, bf16[1]{0} %reshape.10365, bf16[1]{0} %reshape.10366, /*index=25*/bf16[1]{0} %reshape.10367, bf16[1]{0} %reshape.10368, bf16[1]{0} %reshape.10369, bf16[1]{0} %reshape.10370, bf16[1]{0} %reshape.10371, /*index=30*/bf16[1]{0} %reshape.10372, bf16[1]{0} %reshape.10373, bf16[1]{0} %reshape.10374, bf16[1]{0} %reshape.10375, bf16[1]{0} %reshape.10376, /*index=35*/bf16[1]{0} %reshape.10377, bf16[1]{0} %reshape.10378, bf16[1]{0} %reshape.10379, bf16[1]{0} %reshape.10380, bf16[1]{0} %reshape.10381, /*index=40*/bf16[1]{0} %reshape.10382, bf16[1]{0} %reshape.10383, bf16[1]{0} %reshape.10384, bf16[1]{0} %reshape.10385, bf16[1]{0} %reshape.10386, /*index=45*/bf16[1]{0} %reshape.10387, bf16[1]{0} %reshape.10388, bf16[1]{0} %reshape.10389, bf16[1]{0} %reshape.10390, bf16[1]{0} %reshape.10391, /*index=50*/bf16[1]{0} %reshape.10392, bf16[1]{0} %reshape.10393, bf16[1]{0} %reshape.10394, bf16[1]{0} %reshape.10395, bf16[1]{0} %reshape.10396, /*index=55*/bf16[1]{0} %reshape.10397, bf16[1]{0} %reshape.10398, bf16[1]{0} %reshape.10399, bf16[1]{0} %reshape.10400, bf16[1]{0} %reshape.10401, /*index=60*/bf16[1]{0} %reshape.10402, bf16[1]{0} %reshape.10403, bf16[1]{0} %reshape.10404, bf16[1]{0} %reshape.10405, bf16[1]{0} %reshape.10406, /*index=65*/bf16[1]{0} %reshape.10407, bf16[1]{0} %reshape.10408, bf16[1]{0} %reshape.10409, bf16[1]{0} %reshape.10410, bf16[1]{0} %reshape.10411, /*index=70*/bf16[1]{0} %reshape.10412, bf16[1]{0} %reshape.10413, bf16[1]{0} %reshape.10414, bf16[1]{0} %reshape.10415, bf16[1]{0} %reshape.10416, /*index=75*/bf16[1]{0} %reshape.10417, bf16[1]{0} %reshape.10418, bf16[1]{0} %reshape.10419, bf16[1]{0} %reshape.10420, bf16[1]{0} %reshape.10421, /*index=80*/bf16[1]{0} %reshape.10422, bf16[1]{0} %reshape.10423, bf16[1]{0} %reshape.10424, bf16[1]{0} %reshape.10425, bf16[1]{0} %reshape.10426, /*index=85*/bf16[1]{0} %reshape.10427, bf16[1]{0} %reshape.10428, bf16[1]{0} %reshape.10429, bf16[1]{0} %reshape.10430, bf16[1]{0} %reshape.10431, /*index=90*/bf16[1]{0} %reshape.10432, bf16[1]{0} %reshape.10433, bf16[1]{0} %reshape.10434, bf16[1]{0} %reshape.10435, bf16[1]{0} %reshape.10436, /*index=95*/bf16[1]{0} %reshape.10437, bf16[1]{0} %reshape.10438, bf16[1]{0} %reshape.10439, bf16[1]{0} %reshape.10440, bf16[1]{0} %reshape.10441, /*index=100*/bf16[1]{0} %reshape.10442, bf16[1]{0} %reshape.10443, bf16[1]{0} %reshape.10444, bf16[1]{0} %reshape.10445, bf16[1]{0} %reshape.10446, /*index=105*/bf16[1]{0} %reshape.10447, bf16[1]{0} %reshape.10448, bf16[1]{0} %reshape.10449, bf16[1]{0} %reshape.10450, bf16[1]{0} %reshape.10451, /*index=110*/bf16[1]{0} %reshape.10452, bf16[1]{0} %reshape.10453, bf16[1]{0} %reshape.10454, bf16[1]{0} %reshape.10455, bf16[1]{0} %reshape.10456, /*index=115*/bf16[1]{0} %reshape.10457, bf16[1]{0} %reshape.10458, bf16[1]{0} %reshape.10459, bf16[1]{0} %reshape.10460, bf16[1]{0} %reshape.10461, /*index=120*/bf16[1]{0} %reshape.10462, bf16[1]{0} %reshape.10463, bf16[1]{0} %reshape.10464, bf16[1]{0} %reshape.10465, bf16[1]{0} %reshape.10466, /*index=125*/bf16[1]{0} %reshape.10467, bf16[1]{0} %reshape.10468, bf16[1]{0} %reshape.10469, bf16[1]{0} %reshape.10470, bf16[1]{0} %reshape.10471, /*index=130*/bf16[1]{0} %reshape.10472, bf16[1]{0} %reshape.10473, bf16[1]{0} %reshape.10474, bf16[1]{0} %reshape.10475, bf16[1]{0} %reshape.10476, /*index=135*/bf16[1]{0} %reshape.10477, bf16[1]{0} %reshape.10478, bf16[1]{0} %reshape.10479, bf16[1]{0} %reshape.10480, bf16[1]{0} %reshape.10481, /*index=140*/bf16[1]{0} %reshape.10482, bf16[1]{0} %reshape.10483, bf16[1]{0} %reshape.10484, bf16[1]{0} %reshape.10485, bf16[1]{0} %reshape.10486, /*index=145*/bf16[1]{0} %reshape.10487, bf16[1]{0} %reshape.10488, bf16[1]{0} %reshape.10489, bf16[1]{0} %reshape.10490, bf16[1]{0} %reshape.10491, /*index=150*/bf16[1]{0} %reshape.10492, bf16[1]{0} %reshape.10493, bf16[1]{0} %reshape.10494, bf16[1]{0} %reshape.10495, bf16[1]{0} %reshape.10496, /*index=155*/bf16[1]{0} %reshape.10497, bf16[1]{0} %reshape.10498, bf16[1]{0} %reshape.10499, bf16[1]{0} %reshape.10500, bf16[1]{0} %reshape.10501, /*index=160*/bf16[1]{0} %reshape.10502, bf16[1]{0} %reshape.10503, bf16[1]{0} %reshape.10504, bf16[1]{0} %reshape.10505, bf16[1]{0} %reshape.10506, /*index=165*/bf16[1]{0} %reshape.10507, bf16[1]{0} %reshape.10508, bf16[1]{0} %reshape.10509, bf16[1]{0} %reshape.10510, bf16[1]{0} %reshape.10511, /*index=170*/bf16[1]{0} %reshape.10512, bf16[1]{0} %reshape.10513, bf16[1]{0} %reshape.10514, bf16[1]{0} %reshape.10515, bf16[1]{0} %reshape.10516, /*index=175*/bf16[1]{0} %reshape.10517, bf16[1]{0} %reshape.10518, bf16[1]{0} %reshape.10519, bf16[1]{0} %reshape.10520, bf16[1]{0} %reshape.10521, /*index=180*/bf16[1]{0} %reshape.10522, bf16[1]{0} %reshape.10523, bf16[1]{0} %reshape.10524, bf16[1]{0} %reshape.10525, bf16[1]{0} %reshape.10526, /*index=185*/bf16[1]{0} %reshape.10527, bf16[1]{0} %reshape.10528, bf16[1]{0} %reshape.10529, bf16[1]{0} %reshape.10530, bf16[1]{0} %reshape.10531, /*index=190*/bf16[1]{0} %reshape.10532, bf16[1]{0} %reshape.10533, bf16[1]{0} %reshape.10534, bf16[1]{0} %reshape.10535, bf16[1]{0} %reshape.10536, /*index=195*/bf16[1]{0} %reshape.10537, bf16[1]{0} %reshape.10538, bf16[1]{0} %reshape.10539, bf16[1]{0} %reshape.10540, bf16[1]{0} %reshape.10541, /*index=200*/bf16[1]{0} %reshape.10542), dimensions={0}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=41}
  %multiply.10544 = bf16[201]{0} multiply(bf16[201]{0} %concatenate.10543, bf16[201]{0} %concatenate.10543), metadata={op_type="aten__mul" op_name="aten__norm.202/aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %constant.10545 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.202/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %reduce.10551 = bf16[] reduce(bf16[201]{0} %multiply.10544, bf16[] %constant.10545), dimensions={0}, to_apply=%AddComputation.10547, metadata={op_type="aten__sum" op_name="aten__norm.202/aten__sum" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %sqrt.10552 = bf16[] sqrt(bf16[] %reduce.10551), metadata={op_type="aten__sqrt" op_name="aten__norm.202/aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch/functional.py" source_line=1624}
  %p3.8 = bf16[] parameter(3), frontend_attributes={neff_input_names="input3"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=49}
  %add.10553 = bf16[] add(bf16[] %sqrt.10552, bf16[] %p3.8), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=49}
  %divide.10555 = bf16[] divide(bf16[] %constant.10554, bf16[] %add.10553), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=49}
  %constant.477 = bf16[] constant(1), metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=50}
  %compare.10558 = pred[] compare(bf16[] %divide.10555, bf16[] %constant.477), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=50}
  %constant.6 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=51}
  %select.10559 = bf16[] select(pred[] %compare.10558, bf16[] %divide.10555, bf16[] %constant.6), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=50}
  %broadcast.10560 = bf16[28996,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10561 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %custom-call.57, bf16[28996,768]{1,0} %broadcast.10560), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %p217.10573 = bf16[] parameter(217), frontend_attributes={neff_input_names="input217"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10577 = bf16[28996,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10578 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %multiply.10561, bf16[28996,768]{1,0} %broadcast.10577), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10582 = bf16[28996,768]{1,0} add(bf16[28996,768]{1,0} %multiply.10581, bf16[28996,768]{1,0} %multiply.10578), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p216.10563 = bf16[28996,768]{1,0} parameter(216), frontend_attributes={neff_input_names="input216"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %p215.10562 = bf16[] parameter(215), frontend_attributes={neff_input_names="input215"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10564 = bf16[28996,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10565 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %p216.10563, bf16[28996,768]{1,0} %broadcast.10564), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10566 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %multiply.10561, bf16[28996,768]{1,0} %multiply.10561), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %p2.5 = bf16[] parameter(2), frontend_attributes={neff_input_names="input2"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10567 = bf16[28996,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10568 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %multiply.10566, bf16[28996,768]{1,0} %broadcast.10567), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10569 = bf16[28996,768]{1,0} add(bf16[28996,768]{1,0} %multiply.10565, bf16[28996,768]{1,0} %multiply.10568), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10570 = bf16[28996,768]{1,0} sqrt(bf16[28996,768]{1,0} %add.10569), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %p1.3 = bf16[] parameter(1), frontend_attributes={neff_input_names="input1"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10571 = bf16[28996,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10572 = bf16[28996,768]{1,0} add(bf16[28996,768]{1,0} %sqrt.10570, bf16[28996,768]{1,0} %broadcast.10571), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10583 = bf16[28996,768]{1,0} divide(bf16[28996,768]{1,0} %add.10582, bf16[28996,768]{1,0} %add.10572), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p0.1 = bf16[] parameter(0), frontend_attributes={neff_input_names="input0"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10584 = bf16[28996,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10585 = bf16[28996,768]{1,0} multiply(bf16[28996,768]{1,0} %divide.10583, bf16[28996,768]{1,0} %broadcast.10584), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10586 = bf16[28996,768]{1,0} add(bf16[28996,768]{1,0} %p41.949, bf16[28996,768]{1,0} %multiply.10585), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p220.10606 = bf16[512,768]{1,0} parameter(220), frontend_attributes={neff_input_names="input220"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10607 = bf16[512,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10608 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %p220.10606, bf16[512,768]{1,0} %broadcast.10607), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10589 = bf16[512,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10590 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %custom-call.58, bf16[512,768]{1,0} %broadcast.10589), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10604 = bf16[512,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10605 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %multiply.10590, bf16[512,768]{1,0} %broadcast.10604), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10609 = bf16[512,768]{1,0} add(bf16[512,768]{1,0} %multiply.10608, bf16[512,768]{1,0} %multiply.10605), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p219.10591 = bf16[512,768]{1,0} parameter(219), frontend_attributes={neff_input_names="input219"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10592 = bf16[512,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10593 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %p219.10591, bf16[512,768]{1,0} %broadcast.10592), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10594 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %multiply.10590, bf16[512,768]{1,0} %multiply.10590), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10595 = bf16[512,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10596 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %multiply.10594, bf16[512,768]{1,0} %broadcast.10595), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10597 = bf16[512,768]{1,0} add(bf16[512,768]{1,0} %multiply.10593, bf16[512,768]{1,0} %multiply.10596), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10598 = bf16[512,768]{1,0} sqrt(bf16[512,768]{1,0} %add.10597), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10599 = bf16[512,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10600 = bf16[512,768]{1,0} add(bf16[512,768]{1,0} %sqrt.10598, bf16[512,768]{1,0} %broadcast.10599), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10610 = bf16[512,768]{1,0} divide(bf16[512,768]{1,0} %add.10609, bf16[512,768]{1,0} %add.10600), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10611 = bf16[512,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10612 = bf16[512,768]{1,0} multiply(bf16[512,768]{1,0} %divide.10610, bf16[512,768]{1,0} %broadcast.10611), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10613 = bf16[512,768]{1,0} add(bf16[512,768]{1,0} %p37.916, bf16[512,768]{1,0} %multiply.10612), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p222.10633 = bf16[2,768]{1,0} parameter(222), frontend_attributes={neff_input_names="input222"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10634 = bf16[2,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10635 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %p222.10633, bf16[2,768]{1,0} %broadcast.10634), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10616 = bf16[2,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10617 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %custom-call.59, bf16[2,768]{1,0} %broadcast.10616), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10631 = bf16[2,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10632 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %multiply.10617, bf16[2,768]{1,0} %broadcast.10631), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10636 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %multiply.10635, bf16[2,768]{1,0} %multiply.10632), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p221.10618 = bf16[2,768]{1,0} parameter(221), frontend_attributes={neff_input_names="input221"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10619 = bf16[2,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10620 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %p221.10618, bf16[2,768]{1,0} %broadcast.10619), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10621 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %multiply.10617, bf16[2,768]{1,0} %multiply.10617), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10622 = bf16[2,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10623 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %multiply.10621, bf16[2,768]{1,0} %broadcast.10622), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10624 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %multiply.10620, bf16[2,768]{1,0} %multiply.10623), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10625 = bf16[2,768]{1,0} sqrt(bf16[2,768]{1,0} %add.10624), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10626 = bf16[2,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10627 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %sqrt.10625, bf16[2,768]{1,0} %broadcast.10626), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10637 = bf16[2,768]{1,0} divide(bf16[2,768]{1,0} %add.10636, bf16[2,768]{1,0} %add.10627), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10638 = bf16[2,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10639 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %divide.10637, bf16[2,768]{1,0} %broadcast.10638), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10640 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %p39.935, bf16[2,768]{1,0} %multiply.10639), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p224.10662 = bf16[768]{0} parameter(224), frontend_attributes={neff_input_names="input224"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10663 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10664 = bf16[768]{0} multiply(bf16[768]{0} %p224.10662, bf16[768]{0} %broadcast.10663), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10643 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10644 = bf16[768]{0} multiply(bf16[768]{0} %reduce.10161, bf16[768]{0} %broadcast.10643), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10660 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10661 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10644, bf16[768]{0} %broadcast.10660), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10665 = bf16[768]{0} add(bf16[768]{0} %multiply.10664, bf16[768]{0} %multiply.10661), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p223.10647 = bf16[768]{0} parameter(223), frontend_attributes={neff_input_names="input223"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10648 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10649 = bf16[768]{0} multiply(bf16[768]{0} %p223.10647, bf16[768]{0} %broadcast.10648), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10650 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10644, bf16[768]{0} %multiply.10644), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10651 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10652 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10650, bf16[768]{0} %broadcast.10651), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10653 = bf16[768]{0} add(bf16[768]{0} %multiply.10649, bf16[768]{0} %multiply.10652), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10654 = bf16[768]{0} sqrt(bf16[768]{0} %add.10653), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10655 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10656 = bf16[768]{0} add(bf16[768]{0} %sqrt.10654, bf16[768]{0} %broadcast.10655), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10666 = bf16[768]{0} divide(bf16[768]{0} %add.10665, bf16[768]{0} %add.10656), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10667 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10668 = bf16[768]{0} multiply(bf16[768]{0} %divide.10666, bf16[768]{0} %broadcast.10667), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10669 = bf16[768]{0} add(bf16[768]{0} %p35.886, bf16[768]{0} %multiply.10668), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p226.10691 = bf16[768]{0} parameter(226), frontend_attributes={neff_input_names="input226"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10692 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10693 = bf16[768]{0} multiply(bf16[768]{0} %p226.10691, bf16[768]{0} %broadcast.10692), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10672 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10673 = bf16[768]{0} multiply(bf16[768]{0} %reduce.10136, bf16[768]{0} %broadcast.10672), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10689 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10690 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10673, bf16[768]{0} %broadcast.10689), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10694 = bf16[768]{0} add(bf16[768]{0} %multiply.10693, bf16[768]{0} %multiply.10690), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p225.10676 = bf16[768]{0} parameter(225), frontend_attributes={neff_input_names="input225"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10677 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10678 = bf16[768]{0} multiply(bf16[768]{0} %p225.10676, bf16[768]{0} %broadcast.10677), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10679 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10673, bf16[768]{0} %multiply.10673), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10680 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10681 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10679, bf16[768]{0} %broadcast.10680), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10682 = bf16[768]{0} add(bf16[768]{0} %multiply.10678, bf16[768]{0} %multiply.10681), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10683 = bf16[768]{0} sqrt(bf16[768]{0} %add.10682), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10684 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10685 = bf16[768]{0} add(bf16[768]{0} %sqrt.10683, bf16[768]{0} %broadcast.10684), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10695 = bf16[768]{0} divide(bf16[768]{0} %add.10694, bf16[768]{0} %add.10685), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10696 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10697 = bf16[768]{0} multiply(bf16[768]{0} %divide.10695, bf16[768]{0} %broadcast.10696), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10698 = bf16[768]{0} add(bf16[768]{0} %p42.975, bf16[768]{0} %multiply.10697), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p228.10720 = bf16[768,768]{1,0} parameter(228), frontend_attributes={neff_input_names="input228"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10721 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10722 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p228.10720, bf16[768,768]{1,0} %broadcast.10721), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10701 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10702 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot, bf16[768,768]{1,0} %broadcast.10701), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10718 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10719 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10702, bf16[768,768]{1,0} %broadcast.10718), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10723 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10722, bf16[768,768]{0,1} %multiply.10719), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p227.10705 = bf16[768,768]{1,0} parameter(227), frontend_attributes={neff_input_names="input227"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10706 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10707 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p227.10705, bf16[768,768]{1,0} %broadcast.10706), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10708 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10702, bf16[768,768]{0,1} %multiply.10702), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10709 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10710 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10708, bf16[768,768]{1,0} %broadcast.10709), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10711 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10707, bf16[768,768]{0,1} %multiply.10710), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10712 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.10711), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10713 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10714 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.10712, bf16[768,768]{1,0} %broadcast.10713), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10724 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.10723, bf16[768,768]{1,0} %add.10714), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10725 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10726 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.10724, bf16[768,768]{1,0} %broadcast.10725), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10727 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p53.1136, bf16[768,768]{1,0} %multiply.10726), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p230.10749 = bf16[768]{0} parameter(230), frontend_attributes={neff_input_names="input230"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10750 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10751 = bf16[768]{0} multiply(bf16[768]{0} %p230.10749, bf16[768]{0} %broadcast.10750), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1021 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.246 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.6, bf16[12,64]{1,0} %broadcast.1021), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1130 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.284 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.246, bf16[12,64]{1,0} %broadcast.1130), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3506 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.284), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10752 = bf16[768]{0} add(bf16[768]{0} %multiply.10751, bf16[768]{0} %reshape.3506), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p229.10734 = bf16[768]{0} parameter(229), frontend_attributes={neff_input_names="input229"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10735 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10736 = bf16[768]{0} multiply(bf16[768]{0} %p229.10734, bf16[768]{0} %broadcast.10735), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.283 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.246, bf16[12,64]{1,0} %multiply.246), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1219 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.355 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.283, bf16[12,64]{1,0} %broadcast.1219), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3930 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.355), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10740 = bf16[768]{0} add(bf16[768]{0} %multiply.10736, bf16[768]{0} %reshape.3930), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10741 = bf16[768]{0} sqrt(bf16[768]{0} %add.10740), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10742 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10743 = bf16[768]{0} add(bf16[768]{0} %sqrt.10741, bf16[768]{0} %broadcast.10742), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10753 = bf16[768]{0} divide(bf16[768]{0} %add.10752, bf16[768]{0} %add.10743), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10754 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10755 = bf16[768]{0} multiply(bf16[768]{0} %divide.10753, bf16[768]{0} %broadcast.10754), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10756 = bf16[768]{0} add(bf16[768]{0} %p52.1134, bf16[768]{0} %multiply.10755), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p232.10778 = bf16[768,768]{1,0} parameter(232), frontend_attributes={neff_input_names="input232"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10779 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10780 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p232.10778, bf16[768,768]{1,0} %broadcast.10779), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10759 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10760 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.1, bf16[768,768]{1,0} %broadcast.10759), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10776 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10777 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10760, bf16[768,768]{1,0} %broadcast.10776), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10781 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10780, bf16[768,768]{0,1} %multiply.10777), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p231.10763 = bf16[768,768]{1,0} parameter(231), frontend_attributes={neff_input_names="input231"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10764 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10765 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p231.10763, bf16[768,768]{1,0} %broadcast.10764), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10766 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10760, bf16[768,768]{0,1} %multiply.10760), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10767 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10768 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10766, bf16[768,768]{1,0} %broadcast.10767), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10769 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10765, bf16[768,768]{0,1} %multiply.10768), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10770 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.10769), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10771 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10772 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.10770, bf16[768,768]{1,0} %broadcast.10771), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10782 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.10781, bf16[768,768]{1,0} %add.10772), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10783 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10784 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.10782, bf16[768,768]{1,0} %broadcast.10783), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10785 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p51.1115, bf16[768,768]{1,0} %multiply.10784), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p234.10807 = bf16[768]{0} parameter(234), frontend_attributes={neff_input_names="input234"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10808 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10809 = bf16[768]{0} multiply(bf16[768]{0} %p234.10807, bf16[768]{0} %broadcast.10808), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1024 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.247 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.7, bf16[12,64]{1,0} %broadcast.1024), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1134 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.286 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.247, bf16[12,64]{1,0} %broadcast.1134), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3515 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.286), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10810 = bf16[768]{0} add(bf16[768]{0} %multiply.10809, bf16[768]{0} %reshape.3515), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p233.10792 = bf16[768]{0} parameter(233), frontend_attributes={neff_input_names="input233"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10793 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10794 = bf16[768]{0} multiply(bf16[768]{0} %p233.10792, bf16[768]{0} %broadcast.10793), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.285 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.247, bf16[12,64]{1,0} %multiply.247), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1221 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.356 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.285, bf16[12,64]{1,0} %broadcast.1221), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3936 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.356), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10798 = bf16[768]{0} add(bf16[768]{0} %multiply.10794, bf16[768]{0} %reshape.3936), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10799 = bf16[768]{0} sqrt(bf16[768]{0} %add.10798), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10800 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10801 = bf16[768]{0} add(bf16[768]{0} %sqrt.10799, bf16[768]{0} %broadcast.10800), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10811 = bf16[768]{0} divide(bf16[768]{0} %add.10810, bf16[768]{0} %add.10801), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10812 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10813 = bf16[768]{0} multiply(bf16[768]{0} %divide.10811, bf16[768]{0} %broadcast.10812), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10814 = bf16[768]{0} add(bf16[768]{0} %p50.1113, bf16[768]{0} %multiply.10813), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p236.10836 = bf16[768,768]{1,0} parameter(236), frontend_attributes={neff_input_names="input236"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10837 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10838 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p236.10836, bf16[768,768]{1,0} %broadcast.10837), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10817 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10818 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.2, bf16[768,768]{1,0} %broadcast.10817), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10834 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10835 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10818, bf16[768,768]{1,0} %broadcast.10834), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10839 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10838, bf16[768,768]{0,1} %multiply.10835), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p235.10821 = bf16[768,768]{1,0} parameter(235), frontend_attributes={neff_input_names="input235"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10822 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10823 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p235.10821, bf16[768,768]{1,0} %broadcast.10822), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10824 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10818, bf16[768,768]{0,1} %multiply.10818), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10825 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10826 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10824, bf16[768,768]{1,0} %broadcast.10825), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10827 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10823, bf16[768,768]{0,1} %multiply.10826), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10828 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.10827), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10829 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10830 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.10828, bf16[768,768]{1,0} %broadcast.10829), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10840 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.10839, bf16[768,768]{1,0} %add.10830), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10841 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10842 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.10840, bf16[768,768]{1,0} %broadcast.10841), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10843 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p46.1034, bf16[768,768]{1,0} %multiply.10842), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p238.10865 = bf16[768]{0} parameter(238), frontend_attributes={neff_input_names="input238"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10866 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10867 = bf16[768]{0} multiply(bf16[768]{0} %p238.10865, bf16[768]{0} %broadcast.10866), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1026 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.248 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.8, bf16[12,64]{1,0} %broadcast.1026), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1136 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.288 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.248, bf16[12,64]{1,0} %broadcast.1136), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3521 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.288), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10868 = bf16[768]{0} add(bf16[768]{0} %multiply.10867, bf16[768]{0} %reshape.3521), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p237.10850 = bf16[768]{0} parameter(237), frontend_attributes={neff_input_names="input237"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10851 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10852 = bf16[768]{0} multiply(bf16[768]{0} %p237.10850, bf16[768]{0} %broadcast.10851), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.287 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.248, bf16[12,64]{1,0} %multiply.248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1223 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.357 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.287, bf16[12,64]{1,0} %broadcast.1223), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3940 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.357), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10856 = bf16[768]{0} add(bf16[768]{0} %multiply.10852, bf16[768]{0} %reshape.3940), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10857 = bf16[768]{0} sqrt(bf16[768]{0} %add.10856), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10858 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10859 = bf16[768]{0} add(bf16[768]{0} %sqrt.10857, bf16[768]{0} %broadcast.10858), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10869 = bf16[768]{0} divide(bf16[768]{0} %add.10868, bf16[768]{0} %add.10859), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10870 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10871 = bf16[768]{0} multiply(bf16[768]{0} %divide.10869, bf16[768]{0} %broadcast.10870), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10872 = bf16[768]{0} add(bf16[768]{0} %p45.1032, bf16[768]{0} %multiply.10871), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p240.10894 = bf16[768,768]{1,0} parameter(240), frontend_attributes={neff_input_names="input240"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10895 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10896 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p240.10894, bf16[768,768]{1,0} %broadcast.10895), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10875 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10876 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.3, bf16[768,768]{1,0} %broadcast.10875), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10892 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10893 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10876, bf16[768,768]{1,0} %broadcast.10892), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10897 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10896, bf16[768,768]{0,1} %multiply.10893), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p239.10879 = bf16[768,768]{1,0} parameter(239), frontend_attributes={neff_input_names="input239"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10880 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10881 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p239.10879, bf16[768,768]{1,0} %broadcast.10880), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10882 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10876, bf16[768,768]{0,1} %multiply.10876), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10883 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10884 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.10882, bf16[768,768]{1,0} %broadcast.10883), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10885 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.10881, bf16[768,768]{0,1} %multiply.10884), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10886 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.10885), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10887 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10888 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.10886, bf16[768,768]{1,0} %broadcast.10887), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10898 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.10897, bf16[768,768]{1,0} %add.10888), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10899 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10900 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.10898, bf16[768,768]{1,0} %broadcast.10899), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10901 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p44.1025, bf16[768,768]{1,0} %multiply.10900), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p242.10923 = bf16[768]{0} parameter(242), frontend_attributes={neff_input_names="input242"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10924 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10925 = bf16[768]{0} multiply(bf16[768]{0} %p242.10923, bf16[768]{0} %broadcast.10924), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10904 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10905 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9917, bf16[768]{0} %broadcast.10904), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10921 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10922 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10905, bf16[768]{0} %broadcast.10921), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10926 = bf16[768]{0} add(bf16[768]{0} %multiply.10925, bf16[768]{0} %multiply.10922), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p241.10908 = bf16[768]{0} parameter(241), frontend_attributes={neff_input_names="input241"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10909 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10910 = bf16[768]{0} multiply(bf16[768]{0} %p241.10908, bf16[768]{0} %broadcast.10909), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10911 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10905, bf16[768]{0} %multiply.10905), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10912 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10913 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10911, bf16[768]{0} %broadcast.10912), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10914 = bf16[768]{0} add(bf16[768]{0} %multiply.10910, bf16[768]{0} %multiply.10913), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10915 = bf16[768]{0} sqrt(bf16[768]{0} %add.10914), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10916 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10917 = bf16[768]{0} add(bf16[768]{0} %sqrt.10915, bf16[768]{0} %broadcast.10916), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10927 = bf16[768]{0} divide(bf16[768]{0} %add.10926, bf16[768]{0} %add.10917), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10928 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10929 = bf16[768]{0} multiply(bf16[768]{0} %divide.10927, bf16[768]{0} %broadcast.10928), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10930 = bf16[768]{0} add(bf16[768]{0} %p43.1023, bf16[768]{0} %multiply.10929), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p244.10952 = bf16[768]{0} parameter(244), frontend_attributes={neff_input_names="input244"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10953 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10954 = bf16[768]{0} multiply(bf16[768]{0} %p244.10952, bf16[768]{0} %broadcast.10953), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10933 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10934 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9872, bf16[768]{0} %broadcast.10933), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10950 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10951 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10934, bf16[768]{0} %broadcast.10950), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10955 = bf16[768]{0} add(bf16[768]{0} %multiply.10954, bf16[768]{0} %multiply.10951), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p243.10937 = bf16[768]{0} parameter(243), frontend_attributes={neff_input_names="input243"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10938 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10939 = bf16[768]{0} multiply(bf16[768]{0} %p243.10937, bf16[768]{0} %broadcast.10938), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10940 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10934, bf16[768]{0} %multiply.10934), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10941 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10942 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10940, bf16[768]{0} %broadcast.10941), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10943 = bf16[768]{0} add(bf16[768]{0} %multiply.10939, bf16[768]{0} %multiply.10942), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10944 = bf16[768]{0} sqrt(bf16[768]{0} %add.10943), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10945 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10946 = bf16[768]{0} add(bf16[768]{0} %sqrt.10944, bf16[768]{0} %broadcast.10945), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10956 = bf16[768]{0} divide(bf16[768]{0} %add.10955, bf16[768]{0} %add.10946), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10957 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10958 = bf16[768]{0} multiply(bf16[768]{0} %divide.10956, bf16[768]{0} %broadcast.10957), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10959 = bf16[768]{0} add(bf16[768]{0} %p34.826, bf16[768]{0} %multiply.10958), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p246.10981 = bf16[768]{0} parameter(246), frontend_attributes={neff_input_names="input246"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10982 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10983 = bf16[768]{0} multiply(bf16[768]{0} %p246.10981, bf16[768]{0} %broadcast.10982), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10962 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10963 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9847, bf16[768]{0} %broadcast.10962), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10979 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.10980 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10963, bf16[768]{0} %broadcast.10979), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.10984 = bf16[768]{0} add(bf16[768]{0} %multiply.10983, bf16[768]{0} %multiply.10980), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p245.10966 = bf16[768]{0} parameter(245), frontend_attributes={neff_input_names="input245"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10967 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10968 = bf16[768]{0} multiply(bf16[768]{0} %p245.10966, bf16[768]{0} %broadcast.10967), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10969 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10963, bf16[768]{0} %multiply.10963), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10970 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10971 = bf16[768]{0} multiply(bf16[768]{0} %multiply.10969, bf16[768]{0} %broadcast.10970), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.10972 = bf16[768]{0} add(bf16[768]{0} %multiply.10968, bf16[768]{0} %multiply.10971), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.10973 = bf16[768]{0} sqrt(bf16[768]{0} %add.10972), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.10974 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.10975 = bf16[768]{0} add(bf16[768]{0} %sqrt.10973, bf16[768]{0} %broadcast.10974), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.10985 = bf16[768]{0} divide(bf16[768]{0} %add.10984, bf16[768]{0} %add.10975), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.10986 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.10987 = bf16[768]{0} multiply(bf16[768]{0} %divide.10985, bf16[768]{0} %broadcast.10986), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.10988 = bf16[768]{0} add(bf16[768]{0} %p54.1200, bf16[768]{0} %multiply.10987), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p248.11010 = bf16[3072,768]{1,0} parameter(248), frontend_attributes={neff_input_names="input248"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11011 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11012 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p248.11010, bf16[3072,768]{1,0} %broadcast.11011), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.10991 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.10992 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.4, bf16[3072,768]{1,0} %broadcast.10991), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11008 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11009 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.10992, bf16[3072,768]{1,0} %broadcast.11008), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11013 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.11012, bf16[3072,768]{0,1} %multiply.11009), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p247.10995 = bf16[3072,768]{1,0} parameter(247), frontend_attributes={neff_input_names="input247"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10996 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10997 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p247.10995, bf16[3072,768]{1,0} %broadcast.10996), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.10998 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.10992, bf16[3072,768]{0,1} %multiply.10992), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.10999 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11000 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.10998, bf16[3072,768]{1,0} %broadcast.10999), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11001 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.10997, bf16[3072,768]{0,1} %multiply.11000), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11002 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.11001), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11003 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11004 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.11002, bf16[3072,768]{1,0} %broadcast.11003), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11014 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.11013, bf16[3072,768]{1,0} %add.11004), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11015 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11016 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.11014, bf16[3072,768]{1,0} %broadcast.11015), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11017 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p58.1258, bf16[3072,768]{1,0} %multiply.11016), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p250.11039 = bf16[3072]{0} parameter(250), frontend_attributes={neff_input_names="input250"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11040 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11041 = bf16[3072]{0} multiply(bf16[3072]{0} %p250.11039, bf16[3072]{0} %broadcast.11040), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11020 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11021 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.9804, bf16[3072]{0} %broadcast.11020), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11037 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11038 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11021, bf16[3072]{0} %broadcast.11037), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11042 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11041, bf16[3072]{0} %multiply.11038), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p249.11024 = bf16[3072]{0} parameter(249), frontend_attributes={neff_input_names="input249"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11025 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11026 = bf16[3072]{0} multiply(bf16[3072]{0} %p249.11024, bf16[3072]{0} %broadcast.11025), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11027 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11021, bf16[3072]{0} %multiply.11021), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11028 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11029 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11027, bf16[3072]{0} %broadcast.11028), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11030 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11026, bf16[3072]{0} %multiply.11029), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11031 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.11030), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11032 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11033 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.11031, bf16[3072]{0} %broadcast.11032), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11043 = bf16[3072]{0} divide(bf16[3072]{0} %add.11042, bf16[3072]{0} %add.11033), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11044 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11045 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.11043, bf16[3072]{0} %broadcast.11044), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11046 = bf16[3072]{0} add(bf16[3072]{0} %p57.1256, bf16[3072]{0} %multiply.11045), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p252.11068 = bf16[768,3072]{1,0} parameter(252), frontend_attributes={neff_input_names="input252"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11069 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11070 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p252.11068, bf16[768,3072]{1,0} %broadcast.11069), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11049 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11050 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.5, bf16[768,3072]{1,0} %broadcast.11049), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11066 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11067 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11050, bf16[768,3072]{1,0} %broadcast.11066), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11071 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11070, bf16[768,3072]{0,1} %multiply.11067), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p251.11053 = bf16[768,3072]{1,0} parameter(251), frontend_attributes={neff_input_names="input251"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11054 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11055 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p251.11053, bf16[768,3072]{1,0} %broadcast.11054), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11056 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11050, bf16[768,3072]{0,1} %multiply.11050), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11057 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11058 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11056, bf16[768,3072]{1,0} %broadcast.11057), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11059 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11055, bf16[768,3072]{0,1} %multiply.11058), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11060 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.11059), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11061 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11062 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.11060, bf16[768,3072]{1,0} %broadcast.11061), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11072 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.11071, bf16[768,3072]{1,0} %add.11062), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11073 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11074 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.11072, bf16[768,3072]{1,0} %broadcast.11073), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11075 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p56.1249, bf16[768,3072]{1,0} %multiply.11074), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p254.11097 = bf16[768]{0} parameter(254), frontend_attributes={neff_input_names="input254"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11098 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11099 = bf16[768]{0} multiply(bf16[768]{0} %p254.11097, bf16[768]{0} %broadcast.11098), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11078 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11079 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9758, bf16[768]{0} %broadcast.11078), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11095 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11096 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11079, bf16[768]{0} %broadcast.11095), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11100 = bf16[768]{0} add(bf16[768]{0} %multiply.11099, bf16[768]{0} %multiply.11096), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p253.11082 = bf16[768]{0} parameter(253), frontend_attributes={neff_input_names="input253"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11083 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11084 = bf16[768]{0} multiply(bf16[768]{0} %p253.11082, bf16[768]{0} %broadcast.11083), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11085 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11079, bf16[768]{0} %multiply.11079), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11086 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11087 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11085, bf16[768]{0} %broadcast.11086), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11088 = bf16[768]{0} add(bf16[768]{0} %multiply.11084, bf16[768]{0} %multiply.11087), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11089 = bf16[768]{0} sqrt(bf16[768]{0} %add.11088), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11090 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11091 = bf16[768]{0} add(bf16[768]{0} %sqrt.11089, bf16[768]{0} %broadcast.11090), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11101 = bf16[768]{0} divide(bf16[768]{0} %add.11100, bf16[768]{0} %add.11091), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11102 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11103 = bf16[768]{0} multiply(bf16[768]{0} %divide.11101, bf16[768]{0} %broadcast.11102), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11104 = bf16[768]{0} add(bf16[768]{0} %p55.1247, bf16[768]{0} %multiply.11103), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p256.11126 = bf16[768]{0} parameter(256), frontend_attributes={neff_input_names="input256"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11127 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11128 = bf16[768]{0} multiply(bf16[768]{0} %p256.11126, bf16[768]{0} %broadcast.11127), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11107 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11108 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9713, bf16[768]{0} %broadcast.11107), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11124 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11125 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11108, bf16[768]{0} %broadcast.11124), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11129 = bf16[768]{0} add(bf16[768]{0} %multiply.11128, bf16[768]{0} %multiply.11125), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p255.11111 = bf16[768]{0} parameter(255), frontend_attributes={neff_input_names="input255"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11112 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11113 = bf16[768]{0} multiply(bf16[768]{0} %p255.11111, bf16[768]{0} %broadcast.11112), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11114 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11108, bf16[768]{0} %multiply.11108), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11115 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11116 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11114, bf16[768]{0} %broadcast.11115), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11117 = bf16[768]{0} add(bf16[768]{0} %multiply.11113, bf16[768]{0} %multiply.11116), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11118 = bf16[768]{0} sqrt(bf16[768]{0} %add.11117), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11119 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11120 = bf16[768]{0} add(bf16[768]{0} %sqrt.11118, bf16[768]{0} %broadcast.11119), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11130 = bf16[768]{0} divide(bf16[768]{0} %add.11129, bf16[768]{0} %add.11120), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11131 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11132 = bf16[768]{0} multiply(bf16[768]{0} %divide.11130, bf16[768]{0} %broadcast.11131), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11133 = bf16[768]{0} add(bf16[768]{0} %p33.799, bf16[768]{0} %multiply.11132), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p258.11155 = bf16[768]{0} parameter(258), frontend_attributes={neff_input_names="input258"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11156 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11157 = bf16[768]{0} multiply(bf16[768]{0} %p258.11155, bf16[768]{0} %broadcast.11156), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11136 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11137 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9688, bf16[768]{0} %broadcast.11136), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11153 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11154 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11137, bf16[768]{0} %broadcast.11153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11158 = bf16[768]{0} add(bf16[768]{0} %multiply.11157, bf16[768]{0} %multiply.11154), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p257.11140 = bf16[768]{0} parameter(257), frontend_attributes={neff_input_names="input257"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11141 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11142 = bf16[768]{0} multiply(bf16[768]{0} %p257.11140, bf16[768]{0} %broadcast.11141), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11143 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11137, bf16[768]{0} %multiply.11137), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11144 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11145 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11143, bf16[768]{0} %broadcast.11144), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11146 = bf16[768]{0} add(bf16[768]{0} %multiply.11142, bf16[768]{0} %multiply.11145), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11147 = bf16[768]{0} sqrt(bf16[768]{0} %add.11146), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11148 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11149 = bf16[768]{0} add(bf16[768]{0} %sqrt.11147, bf16[768]{0} %broadcast.11148), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11159 = bf16[768]{0} divide(bf16[768]{0} %add.11158, bf16[768]{0} %add.11149), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11160 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11161 = bf16[768]{0} multiply(bf16[768]{0} %divide.11159, bf16[768]{0} %broadcast.11160), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11162 = bf16[768]{0} add(bf16[768]{0} %p59.1293, bf16[768]{0} %multiply.11161), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p260.11184 = bf16[768,768]{1,0} parameter(260), frontend_attributes={neff_input_names="input260"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11185 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11186 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p260.11184, bf16[768,768]{1,0} %broadcast.11185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11165 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11166 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.6, bf16[768,768]{1,0} %broadcast.11165), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11182 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11183 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11166, bf16[768,768]{1,0} %broadcast.11182), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11187 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11186, bf16[768,768]{0,1} %multiply.11183), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p259.11169 = bf16[768,768]{1,0} parameter(259), frontend_attributes={neff_input_names="input259"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11170 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11171 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p259.11169, bf16[768,768]{1,0} %broadcast.11170), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11172 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11166, bf16[768,768]{0,1} %multiply.11166), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11173 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11174 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11172, bf16[768,768]{1,0} %broadcast.11173), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11175 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11171, bf16[768,768]{0,1} %multiply.11174), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11176 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11175), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11177 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11178 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11176, bf16[768,768]{1,0} %broadcast.11177), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11188 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11187, bf16[768,768]{1,0} %add.11178), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11189 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11190 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11188, bf16[768,768]{1,0} %broadcast.11189), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11191 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p67.1431, bf16[768,768]{1,0} %multiply.11190), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p262.11213 = bf16[768]{0} parameter(262), frontend_attributes={neff_input_names="input262"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11214 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11215 = bf16[768]{0} multiply(bf16[768]{0} %p262.11213, bf16[768]{0} %broadcast.11214), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1028 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.249 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.9, bf16[12,64]{1,0} %broadcast.1028), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1138 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.290 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.249, bf16[12,64]{1,0} %broadcast.1138), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3527 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.290), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11216 = bf16[768]{0} add(bf16[768]{0} %multiply.11215, bf16[768]{0} %reshape.3527), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p261.11198 = bf16[768]{0} parameter(261), frontend_attributes={neff_input_names="input261"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11199 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11200 = bf16[768]{0} multiply(bf16[768]{0} %p261.11198, bf16[768]{0} %broadcast.11199), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.289 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.249, bf16[12,64]{1,0} %multiply.249), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1225 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.358 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.289, bf16[12,64]{1,0} %broadcast.1225), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3943 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.358), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11204 = bf16[768]{0} add(bf16[768]{0} %multiply.11200, bf16[768]{0} %reshape.3943), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11205 = bf16[768]{0} sqrt(bf16[768]{0} %add.11204), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11206 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11207 = bf16[768]{0} add(bf16[768]{0} %sqrt.11205, bf16[768]{0} %broadcast.11206), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11217 = bf16[768]{0} divide(bf16[768]{0} %add.11216, bf16[768]{0} %add.11207), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11218 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11219 = bf16[768]{0} multiply(bf16[768]{0} %divide.11217, bf16[768]{0} %broadcast.11218), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11220 = bf16[768]{0} add(bf16[768]{0} %p66.1429, bf16[768]{0} %multiply.11219), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p264.11242 = bf16[768,768]{1,0} parameter(264), frontend_attributes={neff_input_names="input264"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11243 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11244 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p264.11242, bf16[768,768]{1,0} %broadcast.11243), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11223 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11224 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.7, bf16[768,768]{1,0} %broadcast.11223), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11240 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11241 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11224, bf16[768,768]{1,0} %broadcast.11240), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11245 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11244, bf16[768,768]{0,1} %multiply.11241), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p263.11227 = bf16[768,768]{1,0} parameter(263), frontend_attributes={neff_input_names="input263"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11228 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11229 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p263.11227, bf16[768,768]{1,0} %broadcast.11228), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11230 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11224, bf16[768,768]{0,1} %multiply.11224), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11231 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11232 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11230, bf16[768,768]{1,0} %broadcast.11231), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11233 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11229, bf16[768,768]{0,1} %multiply.11232), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11234 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11233), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11235 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11236 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11234, bf16[768,768]{1,0} %broadcast.11235), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11246 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11245, bf16[768,768]{1,0} %add.11236), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11247 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11248 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11246, bf16[768,768]{1,0} %broadcast.11247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11249 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p65.1410, bf16[768,768]{1,0} %multiply.11248), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p266.11271 = bf16[768]{0} parameter(266), frontend_attributes={neff_input_names="input266"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11272 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11273 = bf16[768]{0} multiply(bf16[768]{0} %p266.11271, bf16[768]{0} %broadcast.11272), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1032 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.250 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.10, bf16[12,64]{1,0} %broadcast.1032), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1140 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.292 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.250, bf16[12,64]{1,0} %broadcast.1140), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3533 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.292), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11274 = bf16[768]{0} add(bf16[768]{0} %multiply.11273, bf16[768]{0} %reshape.3533), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p265.11256 = bf16[768]{0} parameter(265), frontend_attributes={neff_input_names="input265"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11257 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11258 = bf16[768]{0} multiply(bf16[768]{0} %p265.11256, bf16[768]{0} %broadcast.11257), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.291 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.250, bf16[12,64]{1,0} %multiply.250), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1228 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.359 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.291, bf16[12,64]{1,0} %broadcast.1228), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3946 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.359), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11262 = bf16[768]{0} add(bf16[768]{0} %multiply.11258, bf16[768]{0} %reshape.3946), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11263 = bf16[768]{0} sqrt(bf16[768]{0} %add.11262), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11264 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11265 = bf16[768]{0} add(bf16[768]{0} %sqrt.11263, bf16[768]{0} %broadcast.11264), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11275 = bf16[768]{0} divide(bf16[768]{0} %add.11274, bf16[768]{0} %add.11265), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11276 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11277 = bf16[768]{0} multiply(bf16[768]{0} %divide.11275, bf16[768]{0} %broadcast.11276), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11278 = bf16[768]{0} add(bf16[768]{0} %p64.1408, bf16[768]{0} %multiply.11277), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p268.11300 = bf16[768,768]{1,0} parameter(268), frontend_attributes={neff_input_names="input268"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11301 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11302 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p268.11300, bf16[768,768]{1,0} %broadcast.11301), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11281 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11282 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.8, bf16[768,768]{1,0} %broadcast.11281), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11298 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11299 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11282, bf16[768,768]{1,0} %broadcast.11298), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11303 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11302, bf16[768,768]{0,1} %multiply.11299), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p267.11285 = bf16[768,768]{1,0} parameter(267), frontend_attributes={neff_input_names="input267"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11286 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11287 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p267.11285, bf16[768,768]{1,0} %broadcast.11286), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11288 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11282, bf16[768,768]{0,1} %multiply.11282), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11289 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11290 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11288, bf16[768,768]{1,0} %broadcast.11289), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11291 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11287, bf16[768,768]{0,1} %multiply.11290), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11292 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11291), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11293 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11294 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11292, bf16[768,768]{1,0} %broadcast.11293), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11304 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11303, bf16[768,768]{1,0} %add.11294), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11305 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11306 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11304, bf16[768,768]{1,0} %broadcast.11305), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11307 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p63.1351, bf16[768,768]{1,0} %multiply.11306), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p270.11329 = bf16[768]{0} parameter(270), frontend_attributes={neff_input_names="input270"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11330 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11331 = bf16[768]{0} multiply(bf16[768]{0} %p270.11329, bf16[768]{0} %broadcast.11330), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1034 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.251 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.11, bf16[12,64]{1,0} %broadcast.1034), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1143 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.294 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.251, bf16[12,64]{1,0} %broadcast.1143), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3544 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.294), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11332 = bf16[768]{0} add(bf16[768]{0} %multiply.11331, bf16[768]{0} %reshape.3544), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p269.11314 = bf16[768]{0} parameter(269), frontend_attributes={neff_input_names="input269"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11315 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11316 = bf16[768]{0} multiply(bf16[768]{0} %p269.11314, bf16[768]{0} %broadcast.11315), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.293 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.251, bf16[12,64]{1,0} %multiply.251), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1231 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.360 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.293, bf16[12,64]{1,0} %broadcast.1231), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3950 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.360), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11320 = bf16[768]{0} add(bf16[768]{0} %multiply.11316, bf16[768]{0} %reshape.3950), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11321 = bf16[768]{0} sqrt(bf16[768]{0} %add.11320), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11322 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11323 = bf16[768]{0} add(bf16[768]{0} %sqrt.11321, bf16[768]{0} %broadcast.11322), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11333 = bf16[768]{0} divide(bf16[768]{0} %add.11332, bf16[768]{0} %add.11323), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11334 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11335 = bf16[768]{0} multiply(bf16[768]{0} %divide.11333, bf16[768]{0} %broadcast.11334), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11336 = bf16[768]{0} add(bf16[768]{0} %p62.1349, bf16[768]{0} %multiply.11335), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p272.11358 = bf16[768,768]{1,0} parameter(272), frontend_attributes={neff_input_names="input272"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11359 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11360 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p272.11358, bf16[768,768]{1,0} %broadcast.11359), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11339 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11340 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.9, bf16[768,768]{1,0} %broadcast.11339), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11356 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11357 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11340, bf16[768,768]{1,0} %broadcast.11356), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11361 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11360, bf16[768,768]{0,1} %multiply.11357), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p271.11343 = bf16[768,768]{1,0} parameter(271), frontend_attributes={neff_input_names="input271"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11344 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11345 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p271.11343, bf16[768,768]{1,0} %broadcast.11344), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11346 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11340, bf16[768,768]{0,1} %multiply.11340), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11347 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11348 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11346, bf16[768,768]{1,0} %broadcast.11347), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11349 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11345, bf16[768,768]{0,1} %multiply.11348), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11350 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11349), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11351 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11352 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11350, bf16[768,768]{1,0} %broadcast.11351), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11362 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11361, bf16[768,768]{1,0} %add.11352), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11363 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11364 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11362, bf16[768,768]{1,0} %broadcast.11363), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11365 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p61.1342, bf16[768,768]{1,0} %multiply.11364), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p274.11387 = bf16[768]{0} parameter(274), frontend_attributes={neff_input_names="input274"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11388 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11389 = bf16[768]{0} multiply(bf16[768]{0} %p274.11387, bf16[768]{0} %broadcast.11388), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11368 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11369 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9470, bf16[768]{0} %broadcast.11368), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11385 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11386 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11369, bf16[768]{0} %broadcast.11385), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11390 = bf16[768]{0} add(bf16[768]{0} %multiply.11389, bf16[768]{0} %multiply.11386), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p273.11372 = bf16[768]{0} parameter(273), frontend_attributes={neff_input_names="input273"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11373 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11374 = bf16[768]{0} multiply(bf16[768]{0} %p273.11372, bf16[768]{0} %broadcast.11373), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11375 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11369, bf16[768]{0} %multiply.11369), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11376 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11377 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11375, bf16[768]{0} %broadcast.11376), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11378 = bf16[768]{0} add(bf16[768]{0} %multiply.11374, bf16[768]{0} %multiply.11377), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11379 = bf16[768]{0} sqrt(bf16[768]{0} %add.11378), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11380 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11381 = bf16[768]{0} add(bf16[768]{0} %sqrt.11379, bf16[768]{0} %broadcast.11380), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11391 = bf16[768]{0} divide(bf16[768]{0} %add.11390, bf16[768]{0} %add.11381), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11392 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11393 = bf16[768]{0} multiply(bf16[768]{0} %divide.11391, bf16[768]{0} %broadcast.11392), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11394 = bf16[768]{0} add(bf16[768]{0} %p60.1340, bf16[768]{0} %multiply.11393), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p276.11416 = bf16[768]{0} parameter(276), frontend_attributes={neff_input_names="input276"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11417 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11418 = bf16[768]{0} multiply(bf16[768]{0} %p276.11416, bf16[768]{0} %broadcast.11417), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11397 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11398 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9425, bf16[768]{0} %broadcast.11397), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11414 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11415 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11398, bf16[768]{0} %broadcast.11414), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11419 = bf16[768]{0} add(bf16[768]{0} %multiply.11418, bf16[768]{0} %multiply.11415), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p275.11401 = bf16[768]{0} parameter(275), frontend_attributes={neff_input_names="input275"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11402 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11403 = bf16[768]{0} multiply(bf16[768]{0} %p275.11401, bf16[768]{0} %broadcast.11402), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11404 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11398, bf16[768]{0} %multiply.11398), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11405 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11406 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11404, bf16[768]{0} %broadcast.11405), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11407 = bf16[768]{0} add(bf16[768]{0} %multiply.11403, bf16[768]{0} %multiply.11406), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11408 = bf16[768]{0} sqrt(bf16[768]{0} %add.11407), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11409 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11410 = bf16[768]{0} add(bf16[768]{0} %sqrt.11408, bf16[768]{0} %broadcast.11409), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11420 = bf16[768]{0} divide(bf16[768]{0} %add.11419, bf16[768]{0} %add.11410), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11421 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11422 = bf16[768]{0} multiply(bf16[768]{0} %divide.11420, bf16[768]{0} %broadcast.11421), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11423 = bf16[768]{0} add(bf16[768]{0} %p32.772, bf16[768]{0} %multiply.11422), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p278.11445 = bf16[768]{0} parameter(278), frontend_attributes={neff_input_names="input278"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11446 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11447 = bf16[768]{0} multiply(bf16[768]{0} %p278.11445, bf16[768]{0} %broadcast.11446), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11426 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11427 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9400, bf16[768]{0} %broadcast.11426), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11443 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11444 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11427, bf16[768]{0} %broadcast.11443), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11448 = bf16[768]{0} add(bf16[768]{0} %multiply.11447, bf16[768]{0} %multiply.11444), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p277.11430 = bf16[768]{0} parameter(277), frontend_attributes={neff_input_names="input277"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11431 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11432 = bf16[768]{0} multiply(bf16[768]{0} %p277.11430, bf16[768]{0} %broadcast.11431), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11433 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11427, bf16[768]{0} %multiply.11427), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11434 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11435 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11433, bf16[768]{0} %broadcast.11434), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11436 = bf16[768]{0} add(bf16[768]{0} %multiply.11432, bf16[768]{0} %multiply.11435), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11437 = bf16[768]{0} sqrt(bf16[768]{0} %add.11436), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11438 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11439 = bf16[768]{0} add(bf16[768]{0} %sqrt.11437, bf16[768]{0} %broadcast.11438), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11449 = bf16[768]{0} divide(bf16[768]{0} %add.11448, bf16[768]{0} %add.11439), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11450 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11451 = bf16[768]{0} multiply(bf16[768]{0} %divide.11449, bf16[768]{0} %broadcast.11450), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11452 = bf16[768]{0} add(bf16[768]{0} %p68.1495, bf16[768]{0} %multiply.11451), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p280.11474 = bf16[3072,768]{1,0} parameter(280), frontend_attributes={neff_input_names="input280"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11475 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11476 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p280.11474, bf16[3072,768]{1,0} %broadcast.11475), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11455 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11456 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.10, bf16[3072,768]{1,0} %broadcast.11455), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11472 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11473 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11456, bf16[3072,768]{1,0} %broadcast.11472), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11477 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.11476, bf16[3072,768]{0,1} %multiply.11473), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p279.11459 = bf16[3072,768]{1,0} parameter(279), frontend_attributes={neff_input_names="input279"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11460 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11461 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p279.11459, bf16[3072,768]{1,0} %broadcast.11460), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11462 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11456, bf16[3072,768]{0,1} %multiply.11456), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11463 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11464 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11462, bf16[3072,768]{1,0} %broadcast.11463), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11465 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.11461, bf16[3072,768]{0,1} %multiply.11464), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11466 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.11465), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11467 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11468 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.11466, bf16[3072,768]{1,0} %broadcast.11467), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11478 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.11477, bf16[3072,768]{1,0} %add.11468), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11479 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11480 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.11478, bf16[3072,768]{1,0} %broadcast.11479), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11481 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p72.1553, bf16[3072,768]{1,0} %multiply.11480), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p282.11503 = bf16[3072]{0} parameter(282), frontend_attributes={neff_input_names="input282"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11504 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11505 = bf16[3072]{0} multiply(bf16[3072]{0} %p282.11503, bf16[3072]{0} %broadcast.11504), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11484 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11485 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.9357, bf16[3072]{0} %broadcast.11484), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11501 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11502 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11485, bf16[3072]{0} %broadcast.11501), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11506 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11505, bf16[3072]{0} %multiply.11502), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p281.11488 = bf16[3072]{0} parameter(281), frontend_attributes={neff_input_names="input281"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11489 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11490 = bf16[3072]{0} multiply(bf16[3072]{0} %p281.11488, bf16[3072]{0} %broadcast.11489), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11491 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11485, bf16[3072]{0} %multiply.11485), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11492 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11493 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11491, bf16[3072]{0} %broadcast.11492), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11494 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11490, bf16[3072]{0} %multiply.11493), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11495 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.11494), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11496 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11497 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.11495, bf16[3072]{0} %broadcast.11496), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11507 = bf16[3072]{0} divide(bf16[3072]{0} %add.11506, bf16[3072]{0} %add.11497), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11508 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11509 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.11507, bf16[3072]{0} %broadcast.11508), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11510 = bf16[3072]{0} add(bf16[3072]{0} %p71.1551, bf16[3072]{0} %multiply.11509), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p284.11532 = bf16[768,3072]{1,0} parameter(284), frontend_attributes={neff_input_names="input284"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11533 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11534 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p284.11532, bf16[768,3072]{1,0} %broadcast.11533), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11513 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11514 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.11, bf16[768,3072]{1,0} %broadcast.11513), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11530 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11531 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11514, bf16[768,3072]{1,0} %broadcast.11530), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11535 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11534, bf16[768,3072]{0,1} %multiply.11531), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p283.11517 = bf16[768,3072]{1,0} parameter(283), frontend_attributes={neff_input_names="input283"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11518 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11519 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p283.11517, bf16[768,3072]{1,0} %broadcast.11518), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11520 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11514, bf16[768,3072]{0,1} %multiply.11514), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11521 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11522 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11520, bf16[768,3072]{1,0} %broadcast.11521), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11523 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11519, bf16[768,3072]{0,1} %multiply.11522), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11524 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.11523), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11525 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11526 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.11524, bf16[768,3072]{1,0} %broadcast.11525), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11536 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.11535, bf16[768,3072]{1,0} %add.11526), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11537 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11538 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.11536, bf16[768,3072]{1,0} %broadcast.11537), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11539 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p70.1544, bf16[768,3072]{1,0} %multiply.11538), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p286.11561 = bf16[768]{0} parameter(286), frontend_attributes={neff_input_names="input286"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11562 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11563 = bf16[768]{0} multiply(bf16[768]{0} %p286.11561, bf16[768]{0} %broadcast.11562), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11542 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11543 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9311, bf16[768]{0} %broadcast.11542), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11559 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11560 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11543, bf16[768]{0} %broadcast.11559), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11564 = bf16[768]{0} add(bf16[768]{0} %multiply.11563, bf16[768]{0} %multiply.11560), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p285.11546 = bf16[768]{0} parameter(285), frontend_attributes={neff_input_names="input285"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11547 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11548 = bf16[768]{0} multiply(bf16[768]{0} %p285.11546, bf16[768]{0} %broadcast.11547), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11549 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11543, bf16[768]{0} %multiply.11543), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11550 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11551 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11549, bf16[768]{0} %broadcast.11550), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11552 = bf16[768]{0} add(bf16[768]{0} %multiply.11548, bf16[768]{0} %multiply.11551), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11553 = bf16[768]{0} sqrt(bf16[768]{0} %add.11552), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11554 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11555 = bf16[768]{0} add(bf16[768]{0} %sqrt.11553, bf16[768]{0} %broadcast.11554), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11565 = bf16[768]{0} divide(bf16[768]{0} %add.11564, bf16[768]{0} %add.11555), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11566 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11567 = bf16[768]{0} multiply(bf16[768]{0} %divide.11565, bf16[768]{0} %broadcast.11566), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11568 = bf16[768]{0} add(bf16[768]{0} %p69.1542, bf16[768]{0} %multiply.11567), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p288.11590 = bf16[768]{0} parameter(288), frontend_attributes={neff_input_names="input288"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11591 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11592 = bf16[768]{0} multiply(bf16[768]{0} %p288.11590, bf16[768]{0} %broadcast.11591), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11571 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11572 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9266, bf16[768]{0} %broadcast.11571), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11588 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11589 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11572, bf16[768]{0} %broadcast.11588), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11593 = bf16[768]{0} add(bf16[768]{0} %multiply.11592, bf16[768]{0} %multiply.11589), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p287.11575 = bf16[768]{0} parameter(287), frontend_attributes={neff_input_names="input287"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11576 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11577 = bf16[768]{0} multiply(bf16[768]{0} %p287.11575, bf16[768]{0} %broadcast.11576), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11578 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11572, bf16[768]{0} %multiply.11572), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11579 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11580 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11578, bf16[768]{0} %broadcast.11579), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11581 = bf16[768]{0} add(bf16[768]{0} %multiply.11577, bf16[768]{0} %multiply.11580), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11582 = bf16[768]{0} sqrt(bf16[768]{0} %add.11581), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11583 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11584 = bf16[768]{0} add(bf16[768]{0} %sqrt.11582, bf16[768]{0} %broadcast.11583), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11594 = bf16[768]{0} divide(bf16[768]{0} %add.11593, bf16[768]{0} %add.11584), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11595 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11596 = bf16[768]{0} multiply(bf16[768]{0} %divide.11594, bf16[768]{0} %broadcast.11595), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11597 = bf16[768]{0} add(bf16[768]{0} %p31.745, bf16[768]{0} %multiply.11596), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p290.11619 = bf16[768]{0} parameter(290), frontend_attributes={neff_input_names="input290"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11620 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11621 = bf16[768]{0} multiply(bf16[768]{0} %p290.11619, bf16[768]{0} %broadcast.11620), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11600 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11601 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9241, bf16[768]{0} %broadcast.11600), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11617 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11618 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11601, bf16[768]{0} %broadcast.11617), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11622 = bf16[768]{0} add(bf16[768]{0} %multiply.11621, bf16[768]{0} %multiply.11618), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p289.11604 = bf16[768]{0} parameter(289), frontend_attributes={neff_input_names="input289"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11605 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11606 = bf16[768]{0} multiply(bf16[768]{0} %p289.11604, bf16[768]{0} %broadcast.11605), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11607 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11601, bf16[768]{0} %multiply.11601), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11608 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11609 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11607, bf16[768]{0} %broadcast.11608), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11610 = bf16[768]{0} add(bf16[768]{0} %multiply.11606, bf16[768]{0} %multiply.11609), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11611 = bf16[768]{0} sqrt(bf16[768]{0} %add.11610), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11612 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11613 = bf16[768]{0} add(bf16[768]{0} %sqrt.11611, bf16[768]{0} %broadcast.11612), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11623 = bf16[768]{0} divide(bf16[768]{0} %add.11622, bf16[768]{0} %add.11613), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11624 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11625 = bf16[768]{0} multiply(bf16[768]{0} %divide.11623, bf16[768]{0} %broadcast.11624), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11626 = bf16[768]{0} add(bf16[768]{0} %p73.1588, bf16[768]{0} %multiply.11625), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p292.11648 = bf16[768,768]{1,0} parameter(292), frontend_attributes={neff_input_names="input292"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11649 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11650 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p292.11648, bf16[768,768]{1,0} %broadcast.11649), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11629 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11630 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.12, bf16[768,768]{1,0} %broadcast.11629), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11646 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11647 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11630, bf16[768,768]{1,0} %broadcast.11646), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11651 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11650, bf16[768,768]{0,1} %multiply.11647), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p291.11633 = bf16[768,768]{1,0} parameter(291), frontend_attributes={neff_input_names="input291"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11634 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11635 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p291.11633, bf16[768,768]{1,0} %broadcast.11634), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11636 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11630, bf16[768,768]{0,1} %multiply.11630), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11637 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11638 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11636, bf16[768,768]{1,0} %broadcast.11637), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11639 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11635, bf16[768,768]{0,1} %multiply.11638), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11640 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11639), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11641 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11642 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11640, bf16[768,768]{1,0} %broadcast.11641), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11652 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11651, bf16[768,768]{1,0} %add.11642), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11653 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11654 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11652, bf16[768,768]{1,0} %broadcast.11653), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11655 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p81.1726, bf16[768,768]{1,0} %multiply.11654), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p294.11677 = bf16[768]{0} parameter(294), frontend_attributes={neff_input_names="input294"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11678 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11679 = bf16[768]{0} multiply(bf16[768]{0} %p294.11677, bf16[768]{0} %broadcast.11678), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1036 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.252 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.12, bf16[12,64]{1,0} %broadcast.1036), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1145 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.296 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.252, bf16[12,64]{1,0} %broadcast.1145), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3553 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.296), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11680 = bf16[768]{0} add(bf16[768]{0} %multiply.11679, bf16[768]{0} %reshape.3553), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p293.11662 = bf16[768]{0} parameter(293), frontend_attributes={neff_input_names="input293"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11663 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11664 = bf16[768]{0} multiply(bf16[768]{0} %p293.11662, bf16[768]{0} %broadcast.11663), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.295 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.252, bf16[12,64]{1,0} %multiply.252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1234 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.361 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.295, bf16[12,64]{1,0} %broadcast.1234), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3953 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.361), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11668 = bf16[768]{0} add(bf16[768]{0} %multiply.11664, bf16[768]{0} %reshape.3953), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11669 = bf16[768]{0} sqrt(bf16[768]{0} %add.11668), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11670 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11671 = bf16[768]{0} add(bf16[768]{0} %sqrt.11669, bf16[768]{0} %broadcast.11670), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11681 = bf16[768]{0} divide(bf16[768]{0} %add.11680, bf16[768]{0} %add.11671), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11682 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11683 = bf16[768]{0} multiply(bf16[768]{0} %divide.11681, bf16[768]{0} %broadcast.11682), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11684 = bf16[768]{0} add(bf16[768]{0} %p80.1724, bf16[768]{0} %multiply.11683), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p296.11706 = bf16[768,768]{1,0} parameter(296), frontend_attributes={neff_input_names="input296"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11707 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11708 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p296.11706, bf16[768,768]{1,0} %broadcast.11707), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11687 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11688 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.13, bf16[768,768]{1,0} %broadcast.11687), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11704 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11705 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11688, bf16[768,768]{1,0} %broadcast.11704), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11709 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11708, bf16[768,768]{0,1} %multiply.11705), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p295.11691 = bf16[768,768]{1,0} parameter(295), frontend_attributes={neff_input_names="input295"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11692 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11693 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p295.11691, bf16[768,768]{1,0} %broadcast.11692), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11694 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11688, bf16[768,768]{0,1} %multiply.11688), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11695 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11696 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11694, bf16[768,768]{1,0} %broadcast.11695), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11697 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11693, bf16[768,768]{0,1} %multiply.11696), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11698 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11697), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11699 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11700 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11698, bf16[768,768]{1,0} %broadcast.11699), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11710 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11709, bf16[768,768]{1,0} %add.11700), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11711 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11712 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11710, bf16[768,768]{1,0} %broadcast.11711), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11713 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p79.1705, bf16[768,768]{1,0} %multiply.11712), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p298.11735 = bf16[768]{0} parameter(298), frontend_attributes={neff_input_names="input298"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11736 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11737 = bf16[768]{0} multiply(bf16[768]{0} %p298.11735, bf16[768]{0} %broadcast.11736), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1038 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.253 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.13, bf16[12,64]{1,0} %broadcast.1038), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1148 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.298 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.253, bf16[12,64]{1,0} %broadcast.1148), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3560 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.298), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11738 = bf16[768]{0} add(bf16[768]{0} %multiply.11737, bf16[768]{0} %reshape.3560), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p297.11720 = bf16[768]{0} parameter(297), frontend_attributes={neff_input_names="input297"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11721 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11722 = bf16[768]{0} multiply(bf16[768]{0} %p297.11720, bf16[768]{0} %broadcast.11721), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.297 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.253, bf16[12,64]{1,0} %multiply.253), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1237 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.362 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.297, bf16[12,64]{1,0} %broadcast.1237), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3956 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.362), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11726 = bf16[768]{0} add(bf16[768]{0} %multiply.11722, bf16[768]{0} %reshape.3956), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11727 = bf16[768]{0} sqrt(bf16[768]{0} %add.11726), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11728 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11729 = bf16[768]{0} add(bf16[768]{0} %sqrt.11727, bf16[768]{0} %broadcast.11728), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11739 = bf16[768]{0} divide(bf16[768]{0} %add.11738, bf16[768]{0} %add.11729), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11740 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11741 = bf16[768]{0} multiply(bf16[768]{0} %divide.11739, bf16[768]{0} %broadcast.11740), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11742 = bf16[768]{0} add(bf16[768]{0} %p78.1703, bf16[768]{0} %multiply.11741), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p300.11764 = bf16[768,768]{1,0} parameter(300), frontend_attributes={neff_input_names="input300"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11765 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11766 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p300.11764, bf16[768,768]{1,0} %broadcast.11765), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11745 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11746 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.14, bf16[768,768]{1,0} %broadcast.11745), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11762 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11763 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11746, bf16[768,768]{1,0} %broadcast.11762), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11767 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11766, bf16[768,768]{0,1} %multiply.11763), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p299.11749 = bf16[768,768]{1,0} parameter(299), frontend_attributes={neff_input_names="input299"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11750 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11751 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p299.11749, bf16[768,768]{1,0} %broadcast.11750), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11752 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11746, bf16[768,768]{0,1} %multiply.11746), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11753 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11754 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11752, bf16[768,768]{1,0} %broadcast.11753), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11755 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11751, bf16[768,768]{0,1} %multiply.11754), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11756 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11755), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11757 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11758 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11756, bf16[768,768]{1,0} %broadcast.11757), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11768 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11767, bf16[768,768]{1,0} %add.11758), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11769 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11770 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11768, bf16[768,768]{1,0} %broadcast.11769), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11771 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p77.1646, bf16[768,768]{1,0} %multiply.11770), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p302.11793 = bf16[768]{0} parameter(302), frontend_attributes={neff_input_names="input302"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11794 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11795 = bf16[768]{0} multiply(bf16[768]{0} %p302.11793, bf16[768]{0} %broadcast.11794), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1041 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.254 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.14, bf16[12,64]{1,0} %broadcast.1041), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1150 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.300 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.254, bf16[12,64]{1,0} %broadcast.1150), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3566 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.300), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11796 = bf16[768]{0} add(bf16[768]{0} %multiply.11795, bf16[768]{0} %reshape.3566), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p301.11778 = bf16[768]{0} parameter(301), frontend_attributes={neff_input_names="input301"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11779 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11780 = bf16[768]{0} multiply(bf16[768]{0} %p301.11778, bf16[768]{0} %broadcast.11779), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.299 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.254, bf16[12,64]{1,0} %multiply.254), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1239 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.363 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.299, bf16[12,64]{1,0} %broadcast.1239), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3961 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.363), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11784 = bf16[768]{0} add(bf16[768]{0} %multiply.11780, bf16[768]{0} %reshape.3961), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11785 = bf16[768]{0} sqrt(bf16[768]{0} %add.11784), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11786 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11787 = bf16[768]{0} add(bf16[768]{0} %sqrt.11785, bf16[768]{0} %broadcast.11786), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11797 = bf16[768]{0} divide(bf16[768]{0} %add.11796, bf16[768]{0} %add.11787), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11798 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11799 = bf16[768]{0} multiply(bf16[768]{0} %divide.11797, bf16[768]{0} %broadcast.11798), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11800 = bf16[768]{0} add(bf16[768]{0} %p76.1644, bf16[768]{0} %multiply.11799), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p304.11822 = bf16[768,768]{1,0} parameter(304), frontend_attributes={neff_input_names="input304"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11823 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11824 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p304.11822, bf16[768,768]{1,0} %broadcast.11823), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11803 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11804 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.15, bf16[768,768]{1,0} %broadcast.11803), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11820 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11821 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11804, bf16[768,768]{1,0} %broadcast.11820), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11825 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11824, bf16[768,768]{0,1} %multiply.11821), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p303.11807 = bf16[768,768]{1,0} parameter(303), frontend_attributes={neff_input_names="input303"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11808 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11809 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p303.11807, bf16[768,768]{1,0} %broadcast.11808), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11810 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11804, bf16[768,768]{0,1} %multiply.11804), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11811 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11812 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.11810, bf16[768,768]{1,0} %broadcast.11811), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11813 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.11809, bf16[768,768]{0,1} %multiply.11812), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11814 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.11813), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11815 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11816 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.11814, bf16[768,768]{1,0} %broadcast.11815), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11826 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.11825, bf16[768,768]{1,0} %add.11816), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11827 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11828 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.11826, bf16[768,768]{1,0} %broadcast.11827), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11829 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p75.1637, bf16[768,768]{1,0} %multiply.11828), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p306.11851 = bf16[768]{0} parameter(306), frontend_attributes={neff_input_names="input306"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11852 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11853 = bf16[768]{0} multiply(bf16[768]{0} %p306.11851, bf16[768]{0} %broadcast.11852), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11832 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11833 = bf16[768]{0} multiply(bf16[768]{0} %reduce.9023, bf16[768]{0} %broadcast.11832), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11849 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11850 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11833, bf16[768]{0} %broadcast.11849), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11854 = bf16[768]{0} add(bf16[768]{0} %multiply.11853, bf16[768]{0} %multiply.11850), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p305.11836 = bf16[768]{0} parameter(305), frontend_attributes={neff_input_names="input305"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11837 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11838 = bf16[768]{0} multiply(bf16[768]{0} %p305.11836, bf16[768]{0} %broadcast.11837), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11839 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11833, bf16[768]{0} %multiply.11833), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11840 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11841 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11839, bf16[768]{0} %broadcast.11840), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11842 = bf16[768]{0} add(bf16[768]{0} %multiply.11838, bf16[768]{0} %multiply.11841), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11843 = bf16[768]{0} sqrt(bf16[768]{0} %add.11842), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11844 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11845 = bf16[768]{0} add(bf16[768]{0} %sqrt.11843, bf16[768]{0} %broadcast.11844), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11855 = bf16[768]{0} divide(bf16[768]{0} %add.11854, bf16[768]{0} %add.11845), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11856 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11857 = bf16[768]{0} multiply(bf16[768]{0} %divide.11855, bf16[768]{0} %broadcast.11856), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11858 = bf16[768]{0} add(bf16[768]{0} %p74.1635, bf16[768]{0} %multiply.11857), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p308.11880 = bf16[768]{0} parameter(308), frontend_attributes={neff_input_names="input308"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11881 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11882 = bf16[768]{0} multiply(bf16[768]{0} %p308.11880, bf16[768]{0} %broadcast.11881), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11861 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11862 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8978, bf16[768]{0} %broadcast.11861), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11878 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11879 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11862, bf16[768]{0} %broadcast.11878), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11883 = bf16[768]{0} add(bf16[768]{0} %multiply.11882, bf16[768]{0} %multiply.11879), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p307.11865 = bf16[768]{0} parameter(307), frontend_attributes={neff_input_names="input307"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11866 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11867 = bf16[768]{0} multiply(bf16[768]{0} %p307.11865, bf16[768]{0} %broadcast.11866), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11868 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11862, bf16[768]{0} %multiply.11862), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11869 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11870 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11868, bf16[768]{0} %broadcast.11869), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11871 = bf16[768]{0} add(bf16[768]{0} %multiply.11867, bf16[768]{0} %multiply.11870), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11872 = bf16[768]{0} sqrt(bf16[768]{0} %add.11871), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11873 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11874 = bf16[768]{0} add(bf16[768]{0} %sqrt.11872, bf16[768]{0} %broadcast.11873), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11884 = bf16[768]{0} divide(bf16[768]{0} %add.11883, bf16[768]{0} %add.11874), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11885 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11886 = bf16[768]{0} multiply(bf16[768]{0} %divide.11884, bf16[768]{0} %broadcast.11885), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11887 = bf16[768]{0} add(bf16[768]{0} %p30.718, bf16[768]{0} %multiply.11886), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p310.11909 = bf16[768]{0} parameter(310), frontend_attributes={neff_input_names="input310"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11910 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11911 = bf16[768]{0} multiply(bf16[768]{0} %p310.11909, bf16[768]{0} %broadcast.11910), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11890 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11891 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8953, bf16[768]{0} %broadcast.11890), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11907 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11908 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11891, bf16[768]{0} %broadcast.11907), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11912 = bf16[768]{0} add(bf16[768]{0} %multiply.11911, bf16[768]{0} %multiply.11908), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p309.11894 = bf16[768]{0} parameter(309), frontend_attributes={neff_input_names="input309"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11895 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11896 = bf16[768]{0} multiply(bf16[768]{0} %p309.11894, bf16[768]{0} %broadcast.11895), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11897 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11891, bf16[768]{0} %multiply.11891), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11898 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11899 = bf16[768]{0} multiply(bf16[768]{0} %multiply.11897, bf16[768]{0} %broadcast.11898), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11900 = bf16[768]{0} add(bf16[768]{0} %multiply.11896, bf16[768]{0} %multiply.11899), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11901 = bf16[768]{0} sqrt(bf16[768]{0} %add.11900), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11902 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11903 = bf16[768]{0} add(bf16[768]{0} %sqrt.11901, bf16[768]{0} %broadcast.11902), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11913 = bf16[768]{0} divide(bf16[768]{0} %add.11912, bf16[768]{0} %add.11903), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11914 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11915 = bf16[768]{0} multiply(bf16[768]{0} %divide.11913, bf16[768]{0} %broadcast.11914), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11916 = bf16[768]{0} add(bf16[768]{0} %p82.1790, bf16[768]{0} %multiply.11915), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p312.11938 = bf16[3072,768]{1,0} parameter(312), frontend_attributes={neff_input_names="input312"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11939 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11940 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p312.11938, bf16[3072,768]{1,0} %broadcast.11939), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11919 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11920 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.16, bf16[3072,768]{1,0} %broadcast.11919), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11936 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11937 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11920, bf16[3072,768]{1,0} %broadcast.11936), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11941 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.11940, bf16[3072,768]{0,1} %multiply.11937), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p311.11923 = bf16[3072,768]{1,0} parameter(311), frontend_attributes={neff_input_names="input311"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11924 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11925 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p311.11923, bf16[3072,768]{1,0} %broadcast.11924), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11926 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11920, bf16[3072,768]{0,1} %multiply.11920), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11927 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11928 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.11926, bf16[3072,768]{1,0} %broadcast.11927), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11929 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.11925, bf16[3072,768]{0,1} %multiply.11928), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11930 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.11929), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11931 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11932 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.11930, bf16[3072,768]{1,0} %broadcast.11931), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11942 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.11941, bf16[3072,768]{1,0} %add.11932), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11943 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11944 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.11942, bf16[3072,768]{1,0} %broadcast.11943), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11945 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p86.1848, bf16[3072,768]{1,0} %multiply.11944), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p314.11967 = bf16[3072]{0} parameter(314), frontend_attributes={neff_input_names="input314"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11968 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11969 = bf16[3072]{0} multiply(bf16[3072]{0} %p314.11967, bf16[3072]{0} %broadcast.11968), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11948 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11949 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8910, bf16[3072]{0} %broadcast.11948), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11965 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11966 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11949, bf16[3072]{0} %broadcast.11965), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11970 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11969, bf16[3072]{0} %multiply.11966), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p313.11952 = bf16[3072]{0} parameter(313), frontend_attributes={neff_input_names="input313"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11953 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11954 = bf16[3072]{0} multiply(bf16[3072]{0} %p313.11952, bf16[3072]{0} %broadcast.11953), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11955 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11949, bf16[3072]{0} %multiply.11949), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11956 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11957 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.11955, bf16[3072]{0} %broadcast.11956), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11958 = bf16[3072]{0} add(bf16[3072]{0} %multiply.11954, bf16[3072]{0} %multiply.11957), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11959 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.11958), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11960 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11961 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.11959, bf16[3072]{0} %broadcast.11960), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.11971 = bf16[3072]{0} divide(bf16[3072]{0} %add.11970, bf16[3072]{0} %add.11961), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.11972 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.11973 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.11971, bf16[3072]{0} %broadcast.11972), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.11974 = bf16[3072]{0} add(bf16[3072]{0} %p85.1846, bf16[3072]{0} %multiply.11973), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p316.11996 = bf16[768,3072]{1,0} parameter(316), frontend_attributes={neff_input_names="input316"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11997 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11998 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p316.11996, bf16[768,3072]{1,0} %broadcast.11997), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.11977 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.11978 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.17, bf16[768,3072]{1,0} %broadcast.11977), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11994 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.11995 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11978, bf16[768,3072]{1,0} %broadcast.11994), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.11999 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11998, bf16[768,3072]{0,1} %multiply.11995), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p315.11981 = bf16[768,3072]{1,0} parameter(315), frontend_attributes={neff_input_names="input315"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11982 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11983 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p315.11981, bf16[768,3072]{1,0} %broadcast.11982), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11984 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11978, bf16[768,3072]{0,1} %multiply.11978), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.11985 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.11986 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.11984, bf16[768,3072]{1,0} %broadcast.11985), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.11987 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.11983, bf16[768,3072]{0,1} %multiply.11986), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.11988 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.11987), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.11989 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.11990 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.11988, bf16[768,3072]{1,0} %broadcast.11989), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12000 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.11999, bf16[768,3072]{1,0} %add.11990), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12001 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12002 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.12000, bf16[768,3072]{1,0} %broadcast.12001), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12003 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p84.1839, bf16[768,3072]{1,0} %multiply.12002), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p318.12025 = bf16[768]{0} parameter(318), frontend_attributes={neff_input_names="input318"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12026 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12027 = bf16[768]{0} multiply(bf16[768]{0} %p318.12025, bf16[768]{0} %broadcast.12026), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12006 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12007 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8864, bf16[768]{0} %broadcast.12006), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12023 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12024 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12007, bf16[768]{0} %broadcast.12023), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12028 = bf16[768]{0} add(bf16[768]{0} %multiply.12027, bf16[768]{0} %multiply.12024), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p317.12010 = bf16[768]{0} parameter(317), frontend_attributes={neff_input_names="input317"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12011 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12012 = bf16[768]{0} multiply(bf16[768]{0} %p317.12010, bf16[768]{0} %broadcast.12011), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12013 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12007, bf16[768]{0} %multiply.12007), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12014 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12015 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12013, bf16[768]{0} %broadcast.12014), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12016 = bf16[768]{0} add(bf16[768]{0} %multiply.12012, bf16[768]{0} %multiply.12015), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12017 = bf16[768]{0} sqrt(bf16[768]{0} %add.12016), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12018 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12019 = bf16[768]{0} add(bf16[768]{0} %sqrt.12017, bf16[768]{0} %broadcast.12018), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12029 = bf16[768]{0} divide(bf16[768]{0} %add.12028, bf16[768]{0} %add.12019), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12030 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12031 = bf16[768]{0} multiply(bf16[768]{0} %divide.12029, bf16[768]{0} %broadcast.12030), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12032 = bf16[768]{0} add(bf16[768]{0} %p83.1837, bf16[768]{0} %multiply.12031), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p320.12054 = bf16[768]{0} parameter(320), frontend_attributes={neff_input_names="input320"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12055 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12056 = bf16[768]{0} multiply(bf16[768]{0} %p320.12054, bf16[768]{0} %broadcast.12055), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12035 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12036 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8819, bf16[768]{0} %broadcast.12035), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12052 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12053 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12036, bf16[768]{0} %broadcast.12052), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12057 = bf16[768]{0} add(bf16[768]{0} %multiply.12056, bf16[768]{0} %multiply.12053), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p319.12039 = bf16[768]{0} parameter(319), frontend_attributes={neff_input_names="input319"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12040 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12041 = bf16[768]{0} multiply(bf16[768]{0} %p319.12039, bf16[768]{0} %broadcast.12040), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12042 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12036, bf16[768]{0} %multiply.12036), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12043 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12044 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12042, bf16[768]{0} %broadcast.12043), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12045 = bf16[768]{0} add(bf16[768]{0} %multiply.12041, bf16[768]{0} %multiply.12044), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12046 = bf16[768]{0} sqrt(bf16[768]{0} %add.12045), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12047 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12048 = bf16[768]{0} add(bf16[768]{0} %sqrt.12046, bf16[768]{0} %broadcast.12047), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12058 = bf16[768]{0} divide(bf16[768]{0} %add.12057, bf16[768]{0} %add.12048), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12059 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12060 = bf16[768]{0} multiply(bf16[768]{0} %divide.12058, bf16[768]{0} %broadcast.12059), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12061 = bf16[768]{0} add(bf16[768]{0} %p29.691, bf16[768]{0} %multiply.12060), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p322.12083 = bf16[768]{0} parameter(322), frontend_attributes={neff_input_names="input322"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12084 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12085 = bf16[768]{0} multiply(bf16[768]{0} %p322.12083, bf16[768]{0} %broadcast.12084), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12064 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12065 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8794, bf16[768]{0} %broadcast.12064), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12081 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12082 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12065, bf16[768]{0} %broadcast.12081), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12086 = bf16[768]{0} add(bf16[768]{0} %multiply.12085, bf16[768]{0} %multiply.12082), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p321.12068 = bf16[768]{0} parameter(321), frontend_attributes={neff_input_names="input321"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12069 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12070 = bf16[768]{0} multiply(bf16[768]{0} %p321.12068, bf16[768]{0} %broadcast.12069), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12071 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12065, bf16[768]{0} %multiply.12065), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12072 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12073 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12071, bf16[768]{0} %broadcast.12072), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12074 = bf16[768]{0} add(bf16[768]{0} %multiply.12070, bf16[768]{0} %multiply.12073), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12075 = bf16[768]{0} sqrt(bf16[768]{0} %add.12074), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12076 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12077 = bf16[768]{0} add(bf16[768]{0} %sqrt.12075, bf16[768]{0} %broadcast.12076), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12087 = bf16[768]{0} divide(bf16[768]{0} %add.12086, bf16[768]{0} %add.12077), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12088 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12089 = bf16[768]{0} multiply(bf16[768]{0} %divide.12087, bf16[768]{0} %broadcast.12088), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12090 = bf16[768]{0} add(bf16[768]{0} %p87.1883, bf16[768]{0} %multiply.12089), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p324.12112 = bf16[768,768]{1,0} parameter(324), frontend_attributes={neff_input_names="input324"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12113 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12114 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p324.12112, bf16[768,768]{1,0} %broadcast.12113), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12093 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12094 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.18, bf16[768,768]{1,0} %broadcast.12093), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12110 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12111 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12094, bf16[768,768]{1,0} %broadcast.12110), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12115 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12114, bf16[768,768]{0,1} %multiply.12111), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p323.12097 = bf16[768,768]{1,0} parameter(323), frontend_attributes={neff_input_names="input323"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12098 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12099 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p323.12097, bf16[768,768]{1,0} %broadcast.12098), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12100 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12094, bf16[768,768]{0,1} %multiply.12094), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12101 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12102 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12100, bf16[768,768]{1,0} %broadcast.12101), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12103 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12099, bf16[768,768]{0,1} %multiply.12102), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12104 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12103), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12105 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12106 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12104, bf16[768,768]{1,0} %broadcast.12105), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12116 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12115, bf16[768,768]{1,0} %add.12106), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12117 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12118 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12116, bf16[768,768]{1,0} %broadcast.12117), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12119 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p95.2021, bf16[768,768]{1,0} %multiply.12118), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p326.12141 = bf16[768]{0} parameter(326), frontend_attributes={neff_input_names="input326"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12142 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12143 = bf16[768]{0} multiply(bf16[768]{0} %p326.12141, bf16[768]{0} %broadcast.12142), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1043 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.255 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.15, bf16[12,64]{1,0} %broadcast.1043), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1154 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.302 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.255, bf16[12,64]{1,0} %broadcast.1154), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3574 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.302), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12144 = bf16[768]{0} add(bf16[768]{0} %multiply.12143, bf16[768]{0} %reshape.3574), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p325.12126 = bf16[768]{0} parameter(325), frontend_attributes={neff_input_names="input325"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12127 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12128 = bf16[768]{0} multiply(bf16[768]{0} %p325.12126, bf16[768]{0} %broadcast.12127), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.301 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.255, bf16[12,64]{1,0} %multiply.255), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1242 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.364 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.301, bf16[12,64]{1,0} %broadcast.1242), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3964 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.364), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12132 = bf16[768]{0} add(bf16[768]{0} %multiply.12128, bf16[768]{0} %reshape.3964), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12133 = bf16[768]{0} sqrt(bf16[768]{0} %add.12132), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12134 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12135 = bf16[768]{0} add(bf16[768]{0} %sqrt.12133, bf16[768]{0} %broadcast.12134), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12145 = bf16[768]{0} divide(bf16[768]{0} %add.12144, bf16[768]{0} %add.12135), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12146 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12147 = bf16[768]{0} multiply(bf16[768]{0} %divide.12145, bf16[768]{0} %broadcast.12146), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12148 = bf16[768]{0} add(bf16[768]{0} %p94.2019, bf16[768]{0} %multiply.12147), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p328.12170 = bf16[768,768]{1,0} parameter(328), frontend_attributes={neff_input_names="input328"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12171 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12172 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p328.12170, bf16[768,768]{1,0} %broadcast.12171), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12151 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12152 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.19, bf16[768,768]{1,0} %broadcast.12151), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12168 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12169 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12152, bf16[768,768]{1,0} %broadcast.12168), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12173 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12172, bf16[768,768]{0,1} %multiply.12169), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p327.12155 = bf16[768,768]{1,0} parameter(327), frontend_attributes={neff_input_names="input327"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12156 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12157 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p327.12155, bf16[768,768]{1,0} %broadcast.12156), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12158 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12152, bf16[768,768]{0,1} %multiply.12152), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12159 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12160 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12158, bf16[768,768]{1,0} %broadcast.12159), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12161 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12157, bf16[768,768]{0,1} %multiply.12160), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12162 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12161), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12163 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12164 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12162, bf16[768,768]{1,0} %broadcast.12163), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12174 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12173, bf16[768,768]{1,0} %add.12164), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12175 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12176 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12174, bf16[768,768]{1,0} %broadcast.12175), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12177 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p93.2000, bf16[768,768]{1,0} %multiply.12176), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p330.12199 = bf16[768]{0} parameter(330), frontend_attributes={neff_input_names="input330"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12200 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12201 = bf16[768]{0} multiply(bf16[768]{0} %p330.12199, bf16[768]{0} %broadcast.12200), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1046 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.256 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.16, bf16[12,64]{1,0} %broadcast.1046), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1157 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.304 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.256, bf16[12,64]{1,0} %broadcast.1157), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3582 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.304), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12202 = bf16[768]{0} add(bf16[768]{0} %multiply.12201, bf16[768]{0} %reshape.3582), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p329.12184 = bf16[768]{0} parameter(329), frontend_attributes={neff_input_names="input329"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12185 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12186 = bf16[768]{0} multiply(bf16[768]{0} %p329.12184, bf16[768]{0} %broadcast.12185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.303 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.256, bf16[12,64]{1,0} %multiply.256), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1245 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.365 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.303, bf16[12,64]{1,0} %broadcast.1245), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3969 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.365), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12190 = bf16[768]{0} add(bf16[768]{0} %multiply.12186, bf16[768]{0} %reshape.3969), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12191 = bf16[768]{0} sqrt(bf16[768]{0} %add.12190), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12192 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12193 = bf16[768]{0} add(bf16[768]{0} %sqrt.12191, bf16[768]{0} %broadcast.12192), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12203 = bf16[768]{0} divide(bf16[768]{0} %add.12202, bf16[768]{0} %add.12193), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12204 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12205 = bf16[768]{0} multiply(bf16[768]{0} %divide.12203, bf16[768]{0} %broadcast.12204), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12206 = bf16[768]{0} add(bf16[768]{0} %p92.1998, bf16[768]{0} %multiply.12205), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p332.12228 = bf16[768,768]{1,0} parameter(332), frontend_attributes={neff_input_names="input332"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12229 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12230 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p332.12228, bf16[768,768]{1,0} %broadcast.12229), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12209 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12210 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.20, bf16[768,768]{1,0} %broadcast.12209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12226 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12227 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12210, bf16[768,768]{1,0} %broadcast.12226), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12231 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12230, bf16[768,768]{0,1} %multiply.12227), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p331.12213 = bf16[768,768]{1,0} parameter(331), frontend_attributes={neff_input_names="input331"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12214 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12215 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p331.12213, bf16[768,768]{1,0} %broadcast.12214), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12216 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12210, bf16[768,768]{0,1} %multiply.12210), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12217 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12218 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12216, bf16[768,768]{1,0} %broadcast.12217), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12219 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12215, bf16[768,768]{0,1} %multiply.12218), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12220 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12219), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12221 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12222 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12220, bf16[768,768]{1,0} %broadcast.12221), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12232 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12231, bf16[768,768]{1,0} %add.12222), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12233 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12234 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12232, bf16[768,768]{1,0} %broadcast.12233), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12235 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p91.1941, bf16[768,768]{1,0} %multiply.12234), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p334.12257 = bf16[768]{0} parameter(334), frontend_attributes={neff_input_names="input334"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12258 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12259 = bf16[768]{0} multiply(bf16[768]{0} %p334.12257, bf16[768]{0} %broadcast.12258), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1049 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.257 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.17, bf16[12,64]{1,0} %broadcast.1049), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1159 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.306 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.257, bf16[12,64]{1,0} %broadcast.1159), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3588 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.306), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12260 = bf16[768]{0} add(bf16[768]{0} %multiply.12259, bf16[768]{0} %reshape.3588), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p333.12242 = bf16[768]{0} parameter(333), frontend_attributes={neff_input_names="input333"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12243 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12244 = bf16[768]{0} multiply(bf16[768]{0} %p333.12242, bf16[768]{0} %broadcast.12243), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.305 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.257, bf16[12,64]{1,0} %multiply.257), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1248 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.366 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.305, bf16[12,64]{1,0} %broadcast.1248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3972 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.366), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12248 = bf16[768]{0} add(bf16[768]{0} %multiply.12244, bf16[768]{0} %reshape.3972), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12249 = bf16[768]{0} sqrt(bf16[768]{0} %add.12248), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12250 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12251 = bf16[768]{0} add(bf16[768]{0} %sqrt.12249, bf16[768]{0} %broadcast.12250), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12261 = bf16[768]{0} divide(bf16[768]{0} %add.12260, bf16[768]{0} %add.12251), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12262 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12263 = bf16[768]{0} multiply(bf16[768]{0} %divide.12261, bf16[768]{0} %broadcast.12262), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12264 = bf16[768]{0} add(bf16[768]{0} %p90.1939, bf16[768]{0} %multiply.12263), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p336.12286 = bf16[768,768]{1,0} parameter(336), frontend_attributes={neff_input_names="input336"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12287 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12288 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p336.12286, bf16[768,768]{1,0} %broadcast.12287), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12267 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12268 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.21, bf16[768,768]{1,0} %broadcast.12267), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12284 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12285 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12268, bf16[768,768]{1,0} %broadcast.12284), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12289 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12288, bf16[768,768]{0,1} %multiply.12285), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p335.12271 = bf16[768,768]{1,0} parameter(335), frontend_attributes={neff_input_names="input335"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12272 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12273 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p335.12271, bf16[768,768]{1,0} %broadcast.12272), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12274 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12268, bf16[768,768]{0,1} %multiply.12268), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12275 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12276 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12274, bf16[768,768]{1,0} %broadcast.12275), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12277 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12273, bf16[768,768]{0,1} %multiply.12276), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12278 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12277), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12279 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12280 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12278, bf16[768,768]{1,0} %broadcast.12279), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12290 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12289, bf16[768,768]{1,0} %add.12280), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12291 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12292 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12290, bf16[768,768]{1,0} %broadcast.12291), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12293 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p89.1932, bf16[768,768]{1,0} %multiply.12292), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p338.12315 = bf16[768]{0} parameter(338), frontend_attributes={neff_input_names="input338"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12316 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12317 = bf16[768]{0} multiply(bf16[768]{0} %p338.12315, bf16[768]{0} %broadcast.12316), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12296 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12297 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8576, bf16[768]{0} %broadcast.12296), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12313 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12314 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12297, bf16[768]{0} %broadcast.12313), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12318 = bf16[768]{0} add(bf16[768]{0} %multiply.12317, bf16[768]{0} %multiply.12314), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p337.12300 = bf16[768]{0} parameter(337), frontend_attributes={neff_input_names="input337"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12301 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12302 = bf16[768]{0} multiply(bf16[768]{0} %p337.12300, bf16[768]{0} %broadcast.12301), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12303 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12297, bf16[768]{0} %multiply.12297), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12304 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12305 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12303, bf16[768]{0} %broadcast.12304), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12306 = bf16[768]{0} add(bf16[768]{0} %multiply.12302, bf16[768]{0} %multiply.12305), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12307 = bf16[768]{0} sqrt(bf16[768]{0} %add.12306), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12308 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12309 = bf16[768]{0} add(bf16[768]{0} %sqrt.12307, bf16[768]{0} %broadcast.12308), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12319 = bf16[768]{0} divide(bf16[768]{0} %add.12318, bf16[768]{0} %add.12309), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12320 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12321 = bf16[768]{0} multiply(bf16[768]{0} %divide.12319, bf16[768]{0} %broadcast.12320), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12322 = bf16[768]{0} add(bf16[768]{0} %p88.1930, bf16[768]{0} %multiply.12321), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p340.12344 = bf16[768]{0} parameter(340), frontend_attributes={neff_input_names="input340"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12345 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12346 = bf16[768]{0} multiply(bf16[768]{0} %p340.12344, bf16[768]{0} %broadcast.12345), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12325 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12326 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8531, bf16[768]{0} %broadcast.12325), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12342 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12343 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12326, bf16[768]{0} %broadcast.12342), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12347 = bf16[768]{0} add(bf16[768]{0} %multiply.12346, bf16[768]{0} %multiply.12343), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p339.12329 = bf16[768]{0} parameter(339), frontend_attributes={neff_input_names="input339"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12330 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12331 = bf16[768]{0} multiply(bf16[768]{0} %p339.12329, bf16[768]{0} %broadcast.12330), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12332 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12326, bf16[768]{0} %multiply.12326), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12333 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12334 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12332, bf16[768]{0} %broadcast.12333), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12335 = bf16[768]{0} add(bf16[768]{0} %multiply.12331, bf16[768]{0} %multiply.12334), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12336 = bf16[768]{0} sqrt(bf16[768]{0} %add.12335), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12337 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12338 = bf16[768]{0} add(bf16[768]{0} %sqrt.12336, bf16[768]{0} %broadcast.12337), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12348 = bf16[768]{0} divide(bf16[768]{0} %add.12347, bf16[768]{0} %add.12338), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12349 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12350 = bf16[768]{0} multiply(bf16[768]{0} %divide.12348, bf16[768]{0} %broadcast.12349), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12351 = bf16[768]{0} add(bf16[768]{0} %p28.664, bf16[768]{0} %multiply.12350), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p342.12373 = bf16[768]{0} parameter(342), frontend_attributes={neff_input_names="input342"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12374 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12375 = bf16[768]{0} multiply(bf16[768]{0} %p342.12373, bf16[768]{0} %broadcast.12374), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12354 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12355 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8506, bf16[768]{0} %broadcast.12354), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12371 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12372 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12355, bf16[768]{0} %broadcast.12371), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12376 = bf16[768]{0} add(bf16[768]{0} %multiply.12375, bf16[768]{0} %multiply.12372), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p341.12358 = bf16[768]{0} parameter(341), frontend_attributes={neff_input_names="input341"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12359 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12360 = bf16[768]{0} multiply(bf16[768]{0} %p341.12358, bf16[768]{0} %broadcast.12359), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12361 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12355, bf16[768]{0} %multiply.12355), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12362 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12363 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12361, bf16[768]{0} %broadcast.12362), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12364 = bf16[768]{0} add(bf16[768]{0} %multiply.12360, bf16[768]{0} %multiply.12363), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12365 = bf16[768]{0} sqrt(bf16[768]{0} %add.12364), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12366 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12367 = bf16[768]{0} add(bf16[768]{0} %sqrt.12365, bf16[768]{0} %broadcast.12366), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12377 = bf16[768]{0} divide(bf16[768]{0} %add.12376, bf16[768]{0} %add.12367), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12378 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12379 = bf16[768]{0} multiply(bf16[768]{0} %divide.12377, bf16[768]{0} %broadcast.12378), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12380 = bf16[768]{0} add(bf16[768]{0} %p96.2085, bf16[768]{0} %multiply.12379), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p344.12402 = bf16[3072,768]{1,0} parameter(344), frontend_attributes={neff_input_names="input344"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12403 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12404 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p344.12402, bf16[3072,768]{1,0} %broadcast.12403), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12383 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12384 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.22, bf16[3072,768]{1,0} %broadcast.12383), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12400 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12401 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12384, bf16[3072,768]{1,0} %broadcast.12400), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12405 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.12404, bf16[3072,768]{0,1} %multiply.12401), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p343.12387 = bf16[3072,768]{1,0} parameter(343), frontend_attributes={neff_input_names="input343"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12388 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12389 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p343.12387, bf16[3072,768]{1,0} %broadcast.12388), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12390 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12384, bf16[3072,768]{0,1} %multiply.12384), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12391 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12392 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12390, bf16[3072,768]{1,0} %broadcast.12391), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12393 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.12389, bf16[3072,768]{0,1} %multiply.12392), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12394 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.12393), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12395 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12396 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.12394, bf16[3072,768]{1,0} %broadcast.12395), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12406 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.12405, bf16[3072,768]{1,0} %add.12396), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12407 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12408 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.12406, bf16[3072,768]{1,0} %broadcast.12407), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12409 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p100.2143, bf16[3072,768]{1,0} %multiply.12408), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p346.12431 = bf16[3072]{0} parameter(346), frontend_attributes={neff_input_names="input346"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12432 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12433 = bf16[3072]{0} multiply(bf16[3072]{0} %p346.12431, bf16[3072]{0} %broadcast.12432), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12412 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12413 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8463, bf16[3072]{0} %broadcast.12412), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12429 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12430 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12413, bf16[3072]{0} %broadcast.12429), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12434 = bf16[3072]{0} add(bf16[3072]{0} %multiply.12433, bf16[3072]{0} %multiply.12430), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p345.12416 = bf16[3072]{0} parameter(345), frontend_attributes={neff_input_names="input345"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12417 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12418 = bf16[3072]{0} multiply(bf16[3072]{0} %p345.12416, bf16[3072]{0} %broadcast.12417), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12419 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12413, bf16[3072]{0} %multiply.12413), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12420 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12421 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12419, bf16[3072]{0} %broadcast.12420), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12422 = bf16[3072]{0} add(bf16[3072]{0} %multiply.12418, bf16[3072]{0} %multiply.12421), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12423 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.12422), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12424 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12425 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.12423, bf16[3072]{0} %broadcast.12424), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12435 = bf16[3072]{0} divide(bf16[3072]{0} %add.12434, bf16[3072]{0} %add.12425), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12436 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12437 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.12435, bf16[3072]{0} %broadcast.12436), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12438 = bf16[3072]{0} add(bf16[3072]{0} %p99.2141, bf16[3072]{0} %multiply.12437), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p348.12460 = bf16[768,3072]{1,0} parameter(348), frontend_attributes={neff_input_names="input348"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12461 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12462 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p348.12460, bf16[768,3072]{1,0} %broadcast.12461), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12441 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12442 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.23, bf16[768,3072]{1,0} %broadcast.12441), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12458 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12459 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12442, bf16[768,3072]{1,0} %broadcast.12458), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12463 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.12462, bf16[768,3072]{0,1} %multiply.12459), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p347.12445 = bf16[768,3072]{1,0} parameter(347), frontend_attributes={neff_input_names="input347"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12446 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12447 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p347.12445, bf16[768,3072]{1,0} %broadcast.12446), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12448 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12442, bf16[768,3072]{0,1} %multiply.12442), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12449 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12450 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12448, bf16[768,3072]{1,0} %broadcast.12449), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12451 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.12447, bf16[768,3072]{0,1} %multiply.12450), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12452 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.12451), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12453 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12454 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.12452, bf16[768,3072]{1,0} %broadcast.12453), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12464 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.12463, bf16[768,3072]{1,0} %add.12454), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12465 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12466 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.12464, bf16[768,3072]{1,0} %broadcast.12465), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12467 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p98.2134, bf16[768,3072]{1,0} %multiply.12466), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p350.12489 = bf16[768]{0} parameter(350), frontend_attributes={neff_input_names="input350"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12490 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12491 = bf16[768]{0} multiply(bf16[768]{0} %p350.12489, bf16[768]{0} %broadcast.12490), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12470 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12471 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8417, bf16[768]{0} %broadcast.12470), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12487 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12488 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12471, bf16[768]{0} %broadcast.12487), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12492 = bf16[768]{0} add(bf16[768]{0} %multiply.12491, bf16[768]{0} %multiply.12488), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p349.12474 = bf16[768]{0} parameter(349), frontend_attributes={neff_input_names="input349"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12475 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12476 = bf16[768]{0} multiply(bf16[768]{0} %p349.12474, bf16[768]{0} %broadcast.12475), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12477 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12471, bf16[768]{0} %multiply.12471), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12478 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12479 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12477, bf16[768]{0} %broadcast.12478), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12480 = bf16[768]{0} add(bf16[768]{0} %multiply.12476, bf16[768]{0} %multiply.12479), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12481 = bf16[768]{0} sqrt(bf16[768]{0} %add.12480), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12482 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12483 = bf16[768]{0} add(bf16[768]{0} %sqrt.12481, bf16[768]{0} %broadcast.12482), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12493 = bf16[768]{0} divide(bf16[768]{0} %add.12492, bf16[768]{0} %add.12483), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12494 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12495 = bf16[768]{0} multiply(bf16[768]{0} %divide.12493, bf16[768]{0} %broadcast.12494), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12496 = bf16[768]{0} add(bf16[768]{0} %p97.2132, bf16[768]{0} %multiply.12495), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p352.12518 = bf16[768]{0} parameter(352), frontend_attributes={neff_input_names="input352"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12519 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12520 = bf16[768]{0} multiply(bf16[768]{0} %p352.12518, bf16[768]{0} %broadcast.12519), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12499 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12500 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8372, bf16[768]{0} %broadcast.12499), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12516 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12517 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12500, bf16[768]{0} %broadcast.12516), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12521 = bf16[768]{0} add(bf16[768]{0} %multiply.12520, bf16[768]{0} %multiply.12517), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p351.12503 = bf16[768]{0} parameter(351), frontend_attributes={neff_input_names="input351"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12504 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12505 = bf16[768]{0} multiply(bf16[768]{0} %p351.12503, bf16[768]{0} %broadcast.12504), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12506 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12500, bf16[768]{0} %multiply.12500), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12507 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12508 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12506, bf16[768]{0} %broadcast.12507), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12509 = bf16[768]{0} add(bf16[768]{0} %multiply.12505, bf16[768]{0} %multiply.12508), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12510 = bf16[768]{0} sqrt(bf16[768]{0} %add.12509), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12511 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12512 = bf16[768]{0} add(bf16[768]{0} %sqrt.12510, bf16[768]{0} %broadcast.12511), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12522 = bf16[768]{0} divide(bf16[768]{0} %add.12521, bf16[768]{0} %add.12512), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12523 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12524 = bf16[768]{0} multiply(bf16[768]{0} %divide.12522, bf16[768]{0} %broadcast.12523), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12525 = bf16[768]{0} add(bf16[768]{0} %p27.637, bf16[768]{0} %multiply.12524), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p354.12547 = bf16[768]{0} parameter(354), frontend_attributes={neff_input_names="input354"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12548 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12549 = bf16[768]{0} multiply(bf16[768]{0} %p354.12547, bf16[768]{0} %broadcast.12548), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12528 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12529 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8347, bf16[768]{0} %broadcast.12528), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12545 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12546 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12529, bf16[768]{0} %broadcast.12545), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12550 = bf16[768]{0} add(bf16[768]{0} %multiply.12549, bf16[768]{0} %multiply.12546), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p353.12532 = bf16[768]{0} parameter(353), frontend_attributes={neff_input_names="input353"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12533 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12534 = bf16[768]{0} multiply(bf16[768]{0} %p353.12532, bf16[768]{0} %broadcast.12533), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12535 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12529, bf16[768]{0} %multiply.12529), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12536 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12537 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12535, bf16[768]{0} %broadcast.12536), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12538 = bf16[768]{0} add(bf16[768]{0} %multiply.12534, bf16[768]{0} %multiply.12537), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12539 = bf16[768]{0} sqrt(bf16[768]{0} %add.12538), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12540 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12541 = bf16[768]{0} add(bf16[768]{0} %sqrt.12539, bf16[768]{0} %broadcast.12540), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12551 = bf16[768]{0} divide(bf16[768]{0} %add.12550, bf16[768]{0} %add.12541), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12552 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12553 = bf16[768]{0} multiply(bf16[768]{0} %divide.12551, bf16[768]{0} %broadcast.12552), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12554 = bf16[768]{0} add(bf16[768]{0} %p101.2178, bf16[768]{0} %multiply.12553), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p356.12576 = bf16[768,768]{1,0} parameter(356), frontend_attributes={neff_input_names="input356"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12577 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12578 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p356.12576, bf16[768,768]{1,0} %broadcast.12577), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12557 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12558 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.24, bf16[768,768]{1,0} %broadcast.12557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12574 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12575 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12558, bf16[768,768]{1,0} %broadcast.12574), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12579 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12578, bf16[768,768]{0,1} %multiply.12575), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p355.12561 = bf16[768,768]{1,0} parameter(355), frontend_attributes={neff_input_names="input355"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12562 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12563 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p355.12561, bf16[768,768]{1,0} %broadcast.12562), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12564 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12558, bf16[768,768]{0,1} %multiply.12558), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12565 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12566 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12564, bf16[768,768]{1,0} %broadcast.12565), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12567 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12563, bf16[768,768]{0,1} %multiply.12566), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12568 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12567), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12569 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12570 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12568, bf16[768,768]{1,0} %broadcast.12569), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12580 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12579, bf16[768,768]{1,0} %add.12570), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12581 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12582 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12580, bf16[768,768]{1,0} %broadcast.12581), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12583 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p109.2316, bf16[768,768]{1,0} %multiply.12582), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p358.12605 = bf16[768]{0} parameter(358), frontend_attributes={neff_input_names="input358"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12606 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12607 = bf16[768]{0} multiply(bf16[768]{0} %p358.12605, bf16[768]{0} %broadcast.12606), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1052 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.258 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.18, bf16[12,64]{1,0} %broadcast.1052), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1161 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.308 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.258, bf16[12,64]{1,0} %broadcast.1161), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3594 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.308), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12608 = bf16[768]{0} add(bf16[768]{0} %multiply.12607, bf16[768]{0} %reshape.3594), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p357.12590 = bf16[768]{0} parameter(357), frontend_attributes={neff_input_names="input357"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12591 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12592 = bf16[768]{0} multiply(bf16[768]{0} %p357.12590, bf16[768]{0} %broadcast.12591), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.307 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.258, bf16[12,64]{1,0} %multiply.258), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1250 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.367 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.307, bf16[12,64]{1,0} %broadcast.1250), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3975 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.367), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12596 = bf16[768]{0} add(bf16[768]{0} %multiply.12592, bf16[768]{0} %reshape.3975), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12597 = bf16[768]{0} sqrt(bf16[768]{0} %add.12596), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12598 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12599 = bf16[768]{0} add(bf16[768]{0} %sqrt.12597, bf16[768]{0} %broadcast.12598), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12609 = bf16[768]{0} divide(bf16[768]{0} %add.12608, bf16[768]{0} %add.12599), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12610 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12611 = bf16[768]{0} multiply(bf16[768]{0} %divide.12609, bf16[768]{0} %broadcast.12610), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12612 = bf16[768]{0} add(bf16[768]{0} %p108.2314, bf16[768]{0} %multiply.12611), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p360.12634 = bf16[768,768]{1,0} parameter(360), frontend_attributes={neff_input_names="input360"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12635 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12636 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p360.12634, bf16[768,768]{1,0} %broadcast.12635), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12615 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12616 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.25, bf16[768,768]{1,0} %broadcast.12615), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12632 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12633 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12616, bf16[768,768]{1,0} %broadcast.12632), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12637 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12636, bf16[768,768]{0,1} %multiply.12633), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p359.12619 = bf16[768,768]{1,0} parameter(359), frontend_attributes={neff_input_names="input359"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12620 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12621 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p359.12619, bf16[768,768]{1,0} %broadcast.12620), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12622 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12616, bf16[768,768]{0,1} %multiply.12616), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12623 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12624 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12622, bf16[768,768]{1,0} %broadcast.12623), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12625 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12621, bf16[768,768]{0,1} %multiply.12624), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12626 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12625), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12627 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12628 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12626, bf16[768,768]{1,0} %broadcast.12627), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12638 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12637, bf16[768,768]{1,0} %add.12628), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12639 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12640 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12638, bf16[768,768]{1,0} %broadcast.12639), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12641 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p107.2295, bf16[768,768]{1,0} %multiply.12640), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p362.12663 = bf16[768]{0} parameter(362), frontend_attributes={neff_input_names="input362"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12664 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12665 = bf16[768]{0} multiply(bf16[768]{0} %p362.12663, bf16[768]{0} %broadcast.12664), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1054 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.259 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.19, bf16[12,64]{1,0} %broadcast.1054), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1164 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.310 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.259, bf16[12,64]{1,0} %broadcast.1164), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3600 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.310), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12666 = bf16[768]{0} add(bf16[768]{0} %multiply.12665, bf16[768]{0} %reshape.3600), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p361.12648 = bf16[768]{0} parameter(361), frontend_attributes={neff_input_names="input361"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12649 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12650 = bf16[768]{0} multiply(bf16[768]{0} %p361.12648, bf16[768]{0} %broadcast.12649), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.309 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.259, bf16[12,64]{1,0} %multiply.259), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1252 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.368 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.309, bf16[12,64]{1,0} %broadcast.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3978 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.368), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12654 = bf16[768]{0} add(bf16[768]{0} %multiply.12650, bf16[768]{0} %reshape.3978), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12655 = bf16[768]{0} sqrt(bf16[768]{0} %add.12654), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12656 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12657 = bf16[768]{0} add(bf16[768]{0} %sqrt.12655, bf16[768]{0} %broadcast.12656), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12667 = bf16[768]{0} divide(bf16[768]{0} %add.12666, bf16[768]{0} %add.12657), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12668 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12669 = bf16[768]{0} multiply(bf16[768]{0} %divide.12667, bf16[768]{0} %broadcast.12668), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12670 = bf16[768]{0} add(bf16[768]{0} %p106.2293, bf16[768]{0} %multiply.12669), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p364.12692 = bf16[768,768]{1,0} parameter(364), frontend_attributes={neff_input_names="input364"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12693 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12694 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p364.12692, bf16[768,768]{1,0} %broadcast.12693), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12673 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12674 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.26, bf16[768,768]{1,0} %broadcast.12673), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12690 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12691 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12674, bf16[768,768]{1,0} %broadcast.12690), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12695 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12694, bf16[768,768]{0,1} %multiply.12691), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p363.12677 = bf16[768,768]{1,0} parameter(363), frontend_attributes={neff_input_names="input363"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12678 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12679 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p363.12677, bf16[768,768]{1,0} %broadcast.12678), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12680 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12674, bf16[768,768]{0,1} %multiply.12674), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12681 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12682 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12680, bf16[768,768]{1,0} %broadcast.12681), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12683 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12679, bf16[768,768]{0,1} %multiply.12682), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12684 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12683), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12685 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12686 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12684, bf16[768,768]{1,0} %broadcast.12685), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12696 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12695, bf16[768,768]{1,0} %add.12686), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12697 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12698 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12696, bf16[768,768]{1,0} %broadcast.12697), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12699 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p105.2236, bf16[768,768]{1,0} %multiply.12698), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p366.12721 = bf16[768]{0} parameter(366), frontend_attributes={neff_input_names="input366"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12722 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12723 = bf16[768]{0} multiply(bf16[768]{0} %p366.12721, bf16[768]{0} %broadcast.12722), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1056 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.260 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.20, bf16[12,64]{1,0} %broadcast.1056), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1166 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.312 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.260, bf16[12,64]{1,0} %broadcast.1166), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3608 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.312), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12724 = bf16[768]{0} add(bf16[768]{0} %multiply.12723, bf16[768]{0} %reshape.3608), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p365.12706 = bf16[768]{0} parameter(365), frontend_attributes={neff_input_names="input365"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12707 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12708 = bf16[768]{0} multiply(bf16[768]{0} %p365.12706, bf16[768]{0} %broadcast.12707), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.311 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.260, bf16[12,64]{1,0} %multiply.260), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1256 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.369 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.311, bf16[12,64]{1,0} %broadcast.1256), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3981 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.369), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12712 = bf16[768]{0} add(bf16[768]{0} %multiply.12708, bf16[768]{0} %reshape.3981), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12713 = bf16[768]{0} sqrt(bf16[768]{0} %add.12712), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12714 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12715 = bf16[768]{0} add(bf16[768]{0} %sqrt.12713, bf16[768]{0} %broadcast.12714), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12725 = bf16[768]{0} divide(bf16[768]{0} %add.12724, bf16[768]{0} %add.12715), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12726 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12727 = bf16[768]{0} multiply(bf16[768]{0} %divide.12725, bf16[768]{0} %broadcast.12726), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12728 = bf16[768]{0} add(bf16[768]{0} %p104.2234, bf16[768]{0} %multiply.12727), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p368.12750 = bf16[768,768]{1,0} parameter(368), frontend_attributes={neff_input_names="input368"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12751 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12752 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p368.12750, bf16[768,768]{1,0} %broadcast.12751), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12731 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12732 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.27, bf16[768,768]{1,0} %broadcast.12731), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12748 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12749 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12732, bf16[768,768]{1,0} %broadcast.12748), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12753 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12752, bf16[768,768]{0,1} %multiply.12749), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p367.12735 = bf16[768,768]{1,0} parameter(367), frontend_attributes={neff_input_names="input367"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12736 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12737 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p367.12735, bf16[768,768]{1,0} %broadcast.12736), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12738 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12732, bf16[768,768]{0,1} %multiply.12732), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12739 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12740 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.12738, bf16[768,768]{1,0} %broadcast.12739), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12741 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.12737, bf16[768,768]{0,1} %multiply.12740), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12742 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.12741), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12743 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12744 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.12742, bf16[768,768]{1,0} %broadcast.12743), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12754 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.12753, bf16[768,768]{1,0} %add.12744), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12755 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12756 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.12754, bf16[768,768]{1,0} %broadcast.12755), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12757 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p103.2227, bf16[768,768]{1,0} %multiply.12756), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p370.12779 = bf16[768]{0} parameter(370), frontend_attributes={neff_input_names="input370"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12780 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12781 = bf16[768]{0} multiply(bf16[768]{0} %p370.12779, bf16[768]{0} %broadcast.12780), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12760 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12761 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8129, bf16[768]{0} %broadcast.12760), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12777 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12778 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12761, bf16[768]{0} %broadcast.12777), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12782 = bf16[768]{0} add(bf16[768]{0} %multiply.12781, bf16[768]{0} %multiply.12778), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p369.12764 = bf16[768]{0} parameter(369), frontend_attributes={neff_input_names="input369"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12765 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12766 = bf16[768]{0} multiply(bf16[768]{0} %p369.12764, bf16[768]{0} %broadcast.12765), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12767 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12761, bf16[768]{0} %multiply.12761), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12768 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12769 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12767, bf16[768]{0} %broadcast.12768), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12770 = bf16[768]{0} add(bf16[768]{0} %multiply.12766, bf16[768]{0} %multiply.12769), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12771 = bf16[768]{0} sqrt(bf16[768]{0} %add.12770), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12772 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12773 = bf16[768]{0} add(bf16[768]{0} %sqrt.12771, bf16[768]{0} %broadcast.12772), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12783 = bf16[768]{0} divide(bf16[768]{0} %add.12782, bf16[768]{0} %add.12773), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12784 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12785 = bf16[768]{0} multiply(bf16[768]{0} %divide.12783, bf16[768]{0} %broadcast.12784), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12786 = bf16[768]{0} add(bf16[768]{0} %p102.2225, bf16[768]{0} %multiply.12785), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p372.12808 = bf16[768]{0} parameter(372), frontend_attributes={neff_input_names="input372"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12809 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12810 = bf16[768]{0} multiply(bf16[768]{0} %p372.12808, bf16[768]{0} %broadcast.12809), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12789 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12790 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8084, bf16[768]{0} %broadcast.12789), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12806 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12807 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12790, bf16[768]{0} %broadcast.12806), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12811 = bf16[768]{0} add(bf16[768]{0} %multiply.12810, bf16[768]{0} %multiply.12807), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p371.12793 = bf16[768]{0} parameter(371), frontend_attributes={neff_input_names="input371"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12794 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12795 = bf16[768]{0} multiply(bf16[768]{0} %p371.12793, bf16[768]{0} %broadcast.12794), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12796 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12790, bf16[768]{0} %multiply.12790), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12797 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12798 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12796, bf16[768]{0} %broadcast.12797), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12799 = bf16[768]{0} add(bf16[768]{0} %multiply.12795, bf16[768]{0} %multiply.12798), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12800 = bf16[768]{0} sqrt(bf16[768]{0} %add.12799), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12801 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12802 = bf16[768]{0} add(bf16[768]{0} %sqrt.12800, bf16[768]{0} %broadcast.12801), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12812 = bf16[768]{0} divide(bf16[768]{0} %add.12811, bf16[768]{0} %add.12802), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12813 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12814 = bf16[768]{0} multiply(bf16[768]{0} %divide.12812, bf16[768]{0} %broadcast.12813), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12815 = bf16[768]{0} add(bf16[768]{0} %p26.610, bf16[768]{0} %multiply.12814), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p374.12837 = bf16[768]{0} parameter(374), frontend_attributes={neff_input_names="input374"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12838 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12839 = bf16[768]{0} multiply(bf16[768]{0} %p374.12837, bf16[768]{0} %broadcast.12838), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12818 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12819 = bf16[768]{0} multiply(bf16[768]{0} %reduce.8059, bf16[768]{0} %broadcast.12818), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12835 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12836 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12819, bf16[768]{0} %broadcast.12835), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12840 = bf16[768]{0} add(bf16[768]{0} %multiply.12839, bf16[768]{0} %multiply.12836), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p373.12822 = bf16[768]{0} parameter(373), frontend_attributes={neff_input_names="input373"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12823 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12824 = bf16[768]{0} multiply(bf16[768]{0} %p373.12822, bf16[768]{0} %broadcast.12823), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12825 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12819, bf16[768]{0} %multiply.12819), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12826 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12827 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12825, bf16[768]{0} %broadcast.12826), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12828 = bf16[768]{0} add(bf16[768]{0} %multiply.12824, bf16[768]{0} %multiply.12827), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12829 = bf16[768]{0} sqrt(bf16[768]{0} %add.12828), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12830 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12831 = bf16[768]{0} add(bf16[768]{0} %sqrt.12829, bf16[768]{0} %broadcast.12830), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12841 = bf16[768]{0} divide(bf16[768]{0} %add.12840, bf16[768]{0} %add.12831), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12842 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12843 = bf16[768]{0} multiply(bf16[768]{0} %divide.12841, bf16[768]{0} %broadcast.12842), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12844 = bf16[768]{0} add(bf16[768]{0} %p110.2380, bf16[768]{0} %multiply.12843), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p376.12866 = bf16[3072,768]{1,0} parameter(376), frontend_attributes={neff_input_names="input376"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12867 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12868 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p376.12866, bf16[3072,768]{1,0} %broadcast.12867), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12847 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12848 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.28, bf16[3072,768]{1,0} %broadcast.12847), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12864 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12865 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12848, bf16[3072,768]{1,0} %broadcast.12864), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12869 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.12868, bf16[3072,768]{0,1} %multiply.12865), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p375.12851 = bf16[3072,768]{1,0} parameter(375), frontend_attributes={neff_input_names="input375"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12852 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12853 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p375.12851, bf16[3072,768]{1,0} %broadcast.12852), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12854 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12848, bf16[3072,768]{0,1} %multiply.12848), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12855 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12856 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.12854, bf16[3072,768]{1,0} %broadcast.12855), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12857 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.12853, bf16[3072,768]{0,1} %multiply.12856), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12858 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.12857), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12859 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12860 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.12858, bf16[3072,768]{1,0} %broadcast.12859), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12870 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.12869, bf16[3072,768]{1,0} %add.12860), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12871 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12872 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.12870, bf16[3072,768]{1,0} %broadcast.12871), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12873 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p114.2438, bf16[3072,768]{1,0} %multiply.12872), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p378.12895 = bf16[3072]{0} parameter(378), frontend_attributes={neff_input_names="input378"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12896 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12897 = bf16[3072]{0} multiply(bf16[3072]{0} %p378.12895, bf16[3072]{0} %broadcast.12896), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12876 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12877 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.8016, bf16[3072]{0} %broadcast.12876), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12893 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12894 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12877, bf16[3072]{0} %broadcast.12893), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12898 = bf16[3072]{0} add(bf16[3072]{0} %multiply.12897, bf16[3072]{0} %multiply.12894), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p377.12880 = bf16[3072]{0} parameter(377), frontend_attributes={neff_input_names="input377"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12881 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12882 = bf16[3072]{0} multiply(bf16[3072]{0} %p377.12880, bf16[3072]{0} %broadcast.12881), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12883 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12877, bf16[3072]{0} %multiply.12877), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12884 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12885 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.12883, bf16[3072]{0} %broadcast.12884), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12886 = bf16[3072]{0} add(bf16[3072]{0} %multiply.12882, bf16[3072]{0} %multiply.12885), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12887 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.12886), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12888 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12889 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.12887, bf16[3072]{0} %broadcast.12888), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12899 = bf16[3072]{0} divide(bf16[3072]{0} %add.12898, bf16[3072]{0} %add.12889), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12900 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12901 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.12899, bf16[3072]{0} %broadcast.12900), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12902 = bf16[3072]{0} add(bf16[3072]{0} %p113.2436, bf16[3072]{0} %multiply.12901), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p380.12924 = bf16[768,3072]{1,0} parameter(380), frontend_attributes={neff_input_names="input380"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12925 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12926 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p380.12924, bf16[768,3072]{1,0} %broadcast.12925), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12905 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12906 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.29, bf16[768,3072]{1,0} %broadcast.12905), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12922 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12923 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12906, bf16[768,3072]{1,0} %broadcast.12922), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12927 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.12926, bf16[768,3072]{0,1} %multiply.12923), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p379.12909 = bf16[768,3072]{1,0} parameter(379), frontend_attributes={neff_input_names="input379"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12910 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12911 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p379.12909, bf16[768,3072]{1,0} %broadcast.12910), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12912 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12906, bf16[768,3072]{0,1} %multiply.12906), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12913 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12914 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.12912, bf16[768,3072]{1,0} %broadcast.12913), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12915 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.12911, bf16[768,3072]{0,1} %multiply.12914), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12916 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.12915), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12917 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12918 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.12916, bf16[768,3072]{1,0} %broadcast.12917), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12928 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.12927, bf16[768,3072]{1,0} %add.12918), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12929 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12930 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.12928, bf16[768,3072]{1,0} %broadcast.12929), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12931 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p112.2429, bf16[768,3072]{1,0} %multiply.12930), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p382.12953 = bf16[768]{0} parameter(382), frontend_attributes={neff_input_names="input382"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12954 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12955 = bf16[768]{0} multiply(bf16[768]{0} %p382.12953, bf16[768]{0} %broadcast.12954), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12934 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12935 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7970, bf16[768]{0} %broadcast.12934), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12951 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12952 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12935, bf16[768]{0} %broadcast.12951), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12956 = bf16[768]{0} add(bf16[768]{0} %multiply.12955, bf16[768]{0} %multiply.12952), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p381.12938 = bf16[768]{0} parameter(381), frontend_attributes={neff_input_names="input381"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12939 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12940 = bf16[768]{0} multiply(bf16[768]{0} %p381.12938, bf16[768]{0} %broadcast.12939), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12941 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12935, bf16[768]{0} %multiply.12935), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12942 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12943 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12941, bf16[768]{0} %broadcast.12942), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12944 = bf16[768]{0} add(bf16[768]{0} %multiply.12940, bf16[768]{0} %multiply.12943), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12945 = bf16[768]{0} sqrt(bf16[768]{0} %add.12944), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12946 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12947 = bf16[768]{0} add(bf16[768]{0} %sqrt.12945, bf16[768]{0} %broadcast.12946), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12957 = bf16[768]{0} divide(bf16[768]{0} %add.12956, bf16[768]{0} %add.12947), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12958 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12959 = bf16[768]{0} multiply(bf16[768]{0} %divide.12957, bf16[768]{0} %broadcast.12958), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12960 = bf16[768]{0} add(bf16[768]{0} %p111.2427, bf16[768]{0} %multiply.12959), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p384.12982 = bf16[768]{0} parameter(384), frontend_attributes={neff_input_names="input384"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12983 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12984 = bf16[768]{0} multiply(bf16[768]{0} %p384.12982, bf16[768]{0} %broadcast.12983), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12963 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12964 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7925, bf16[768]{0} %broadcast.12963), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12980 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.12981 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12964, bf16[768]{0} %broadcast.12980), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.12985 = bf16[768]{0} add(bf16[768]{0} %multiply.12984, bf16[768]{0} %multiply.12981), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p383.12967 = bf16[768]{0} parameter(383), frontend_attributes={neff_input_names="input383"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12968 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12969 = bf16[768]{0} multiply(bf16[768]{0} %p383.12967, bf16[768]{0} %broadcast.12968), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12970 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12964, bf16[768]{0} %multiply.12964), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12971 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12972 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12970, bf16[768]{0} %broadcast.12971), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.12973 = bf16[768]{0} add(bf16[768]{0} %multiply.12969, bf16[768]{0} %multiply.12972), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.12974 = bf16[768]{0} sqrt(bf16[768]{0} %add.12973), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.12975 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.12976 = bf16[768]{0} add(bf16[768]{0} %sqrt.12974, bf16[768]{0} %broadcast.12975), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.12986 = bf16[768]{0} divide(bf16[768]{0} %add.12985, bf16[768]{0} %add.12976), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.12987 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.12988 = bf16[768]{0} multiply(bf16[768]{0} %divide.12986, bf16[768]{0} %broadcast.12987), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.12989 = bf16[768]{0} add(bf16[768]{0} %p25.583, bf16[768]{0} %multiply.12988), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p386.13011 = bf16[768]{0} parameter(386), frontend_attributes={neff_input_names="input386"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13012 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13013 = bf16[768]{0} multiply(bf16[768]{0} %p386.13011, bf16[768]{0} %broadcast.13012), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.12992 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.12993 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7900, bf16[768]{0} %broadcast.12992), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13009 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13010 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12993, bf16[768]{0} %broadcast.13009), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13014 = bf16[768]{0} add(bf16[768]{0} %multiply.13013, bf16[768]{0} %multiply.13010), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p385.12996 = bf16[768]{0} parameter(385), frontend_attributes={neff_input_names="input385"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.12997 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12998 = bf16[768]{0} multiply(bf16[768]{0} %p385.12996, bf16[768]{0} %broadcast.12997), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.12999 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12993, bf16[768]{0} %multiply.12993), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13000 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13001 = bf16[768]{0} multiply(bf16[768]{0} %multiply.12999, bf16[768]{0} %broadcast.13000), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13002 = bf16[768]{0} add(bf16[768]{0} %multiply.12998, bf16[768]{0} %multiply.13001), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13003 = bf16[768]{0} sqrt(bf16[768]{0} %add.13002), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13004 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13005 = bf16[768]{0} add(bf16[768]{0} %sqrt.13003, bf16[768]{0} %broadcast.13004), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13015 = bf16[768]{0} divide(bf16[768]{0} %add.13014, bf16[768]{0} %add.13005), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13016 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13017 = bf16[768]{0} multiply(bf16[768]{0} %divide.13015, bf16[768]{0} %broadcast.13016), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13018 = bf16[768]{0} add(bf16[768]{0} %p115.2473, bf16[768]{0} %multiply.13017), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p388.13040 = bf16[768,768]{1,0} parameter(388), frontend_attributes={neff_input_names="input388"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13041 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13042 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p388.13040, bf16[768,768]{1,0} %broadcast.13041), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13021 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13022 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.30, bf16[768,768]{1,0} %broadcast.13021), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13038 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13039 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13022, bf16[768,768]{1,0} %broadcast.13038), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13043 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13042, bf16[768,768]{0,1} %multiply.13039), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p387.13025 = bf16[768,768]{1,0} parameter(387), frontend_attributes={neff_input_names="input387"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13026 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13027 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p387.13025, bf16[768,768]{1,0} %broadcast.13026), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13028 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13022, bf16[768,768]{0,1} %multiply.13022), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13029 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13030 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13028, bf16[768,768]{1,0} %broadcast.13029), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13031 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13027, bf16[768,768]{0,1} %multiply.13030), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13032 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13031), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13033 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13034 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13032, bf16[768,768]{1,0} %broadcast.13033), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13044 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13043, bf16[768,768]{1,0} %add.13034), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13045 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13046 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13044, bf16[768,768]{1,0} %broadcast.13045), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13047 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p123.2611, bf16[768,768]{1,0} %multiply.13046), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p390.13069 = bf16[768]{0} parameter(390), frontend_attributes={neff_input_names="input390"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13070 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13071 = bf16[768]{0} multiply(bf16[768]{0} %p390.13069, bf16[768]{0} %broadcast.13070), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1058 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.261 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.21, bf16[12,64]{1,0} %broadcast.1058), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1168 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.314 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.261, bf16[12,64]{1,0} %broadcast.1168), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3616 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.314), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13072 = bf16[768]{0} add(bf16[768]{0} %multiply.13071, bf16[768]{0} %reshape.3616), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p389.13054 = bf16[768]{0} parameter(389), frontend_attributes={neff_input_names="input389"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13055 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13056 = bf16[768]{0} multiply(bf16[768]{0} %p389.13054, bf16[768]{0} %broadcast.13055), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.313 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.261, bf16[12,64]{1,0} %multiply.261), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1258 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.370 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.313, bf16[12,64]{1,0} %broadcast.1258), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3984 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.370), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13060 = bf16[768]{0} add(bf16[768]{0} %multiply.13056, bf16[768]{0} %reshape.3984), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13061 = bf16[768]{0} sqrt(bf16[768]{0} %add.13060), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13062 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13063 = bf16[768]{0} add(bf16[768]{0} %sqrt.13061, bf16[768]{0} %broadcast.13062), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13073 = bf16[768]{0} divide(bf16[768]{0} %add.13072, bf16[768]{0} %add.13063), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13074 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13075 = bf16[768]{0} multiply(bf16[768]{0} %divide.13073, bf16[768]{0} %broadcast.13074), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13076 = bf16[768]{0} add(bf16[768]{0} %p122.2609, bf16[768]{0} %multiply.13075), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p392.13098 = bf16[768,768]{1,0} parameter(392), frontend_attributes={neff_input_names="input392"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13099 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13100 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p392.13098, bf16[768,768]{1,0} %broadcast.13099), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13079 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13080 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.31, bf16[768,768]{1,0} %broadcast.13079), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13096 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13097 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13080, bf16[768,768]{1,0} %broadcast.13096), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13101 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13100, bf16[768,768]{0,1} %multiply.13097), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p391.13083 = bf16[768,768]{1,0} parameter(391), frontend_attributes={neff_input_names="input391"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13084 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13085 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p391.13083, bf16[768,768]{1,0} %broadcast.13084), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13086 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13080, bf16[768,768]{0,1} %multiply.13080), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13087 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13088 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13086, bf16[768,768]{1,0} %broadcast.13087), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13089 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13085, bf16[768,768]{0,1} %multiply.13088), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13090 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13089), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13091 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13092 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13090, bf16[768,768]{1,0} %broadcast.13091), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13102 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13101, bf16[768,768]{1,0} %add.13092), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13103 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13104 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13102, bf16[768,768]{1,0} %broadcast.13103), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13105 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p121.2590, bf16[768,768]{1,0} %multiply.13104), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p394.13127 = bf16[768]{0} parameter(394), frontend_attributes={neff_input_names="input394"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13128 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13129 = bf16[768]{0} multiply(bf16[768]{0} %p394.13127, bf16[768]{0} %broadcast.13128), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1060 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.262 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.22, bf16[12,64]{1,0} %broadcast.1060), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1170 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.316 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.262, bf16[12,64]{1,0} %broadcast.1170), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3624 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.316), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13130 = bf16[768]{0} add(bf16[768]{0} %multiply.13129, bf16[768]{0} %reshape.3624), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p393.13112 = bf16[768]{0} parameter(393), frontend_attributes={neff_input_names="input393"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13113 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13114 = bf16[768]{0} multiply(bf16[768]{0} %p393.13112, bf16[768]{0} %broadcast.13113), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.315 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.262, bf16[12,64]{1,0} %multiply.262), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1260 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.371 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.315, bf16[12,64]{1,0} %broadcast.1260), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3987 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.371), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13118 = bf16[768]{0} add(bf16[768]{0} %multiply.13114, bf16[768]{0} %reshape.3987), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13119 = bf16[768]{0} sqrt(bf16[768]{0} %add.13118), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13120 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13121 = bf16[768]{0} add(bf16[768]{0} %sqrt.13119, bf16[768]{0} %broadcast.13120), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13131 = bf16[768]{0} divide(bf16[768]{0} %add.13130, bf16[768]{0} %add.13121), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13132 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13133 = bf16[768]{0} multiply(bf16[768]{0} %divide.13131, bf16[768]{0} %broadcast.13132), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13134 = bf16[768]{0} add(bf16[768]{0} %p120.2588, bf16[768]{0} %multiply.13133), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p396.13156 = bf16[768,768]{1,0} parameter(396), frontend_attributes={neff_input_names="input396"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13157 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13158 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p396.13156, bf16[768,768]{1,0} %broadcast.13157), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13137 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13138 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.32, bf16[768,768]{1,0} %broadcast.13137), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13154 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13155 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13138, bf16[768,768]{1,0} %broadcast.13154), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13159 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13158, bf16[768,768]{0,1} %multiply.13155), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p395.13141 = bf16[768,768]{1,0} parameter(395), frontend_attributes={neff_input_names="input395"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13142 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13143 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p395.13141, bf16[768,768]{1,0} %broadcast.13142), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13144 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13138, bf16[768,768]{0,1} %multiply.13138), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13145 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13146 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13144, bf16[768,768]{1,0} %broadcast.13145), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13147 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13143, bf16[768,768]{0,1} %multiply.13146), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13148 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13147), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13149 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13150 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13148, bf16[768,768]{1,0} %broadcast.13149), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13160 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13159, bf16[768,768]{1,0} %add.13150), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13161 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13162 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13160, bf16[768,768]{1,0} %broadcast.13161), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13163 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p119.2531, bf16[768,768]{1,0} %multiply.13162), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p398.13185 = bf16[768]{0} parameter(398), frontend_attributes={neff_input_names="input398"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13186 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13187 = bf16[768]{0} multiply(bf16[768]{0} %p398.13185, bf16[768]{0} %broadcast.13186), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1062 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.263 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.23, bf16[12,64]{1,0} %broadcast.1062), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1173 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.318 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.263, bf16[12,64]{1,0} %broadcast.1173), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3632 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.318), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13188 = bf16[768]{0} add(bf16[768]{0} %multiply.13187, bf16[768]{0} %reshape.3632), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p397.13170 = bf16[768]{0} parameter(397), frontend_attributes={neff_input_names="input397"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13171 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13172 = bf16[768]{0} multiply(bf16[768]{0} %p397.13170, bf16[768]{0} %broadcast.13171), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.317 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.263, bf16[12,64]{1,0} %multiply.263), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1262 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.372 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.317, bf16[12,64]{1,0} %broadcast.1262), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3990 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.372), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13176 = bf16[768]{0} add(bf16[768]{0} %multiply.13172, bf16[768]{0} %reshape.3990), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13177 = bf16[768]{0} sqrt(bf16[768]{0} %add.13176), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13178 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13179 = bf16[768]{0} add(bf16[768]{0} %sqrt.13177, bf16[768]{0} %broadcast.13178), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13189 = bf16[768]{0} divide(bf16[768]{0} %add.13188, bf16[768]{0} %add.13179), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13190 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13191 = bf16[768]{0} multiply(bf16[768]{0} %divide.13189, bf16[768]{0} %broadcast.13190), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13192 = bf16[768]{0} add(bf16[768]{0} %p118.2529, bf16[768]{0} %multiply.13191), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p400.13214 = bf16[768,768]{1,0} parameter(400), frontend_attributes={neff_input_names="input400"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13215 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13216 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p400.13214, bf16[768,768]{1,0} %broadcast.13215), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13195 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13196 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.33, bf16[768,768]{1,0} %broadcast.13195), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13212 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13213 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13196, bf16[768,768]{1,0} %broadcast.13212), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13217 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13216, bf16[768,768]{0,1} %multiply.13213), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p399.13199 = bf16[768,768]{1,0} parameter(399), frontend_attributes={neff_input_names="input399"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13200 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13201 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p399.13199, bf16[768,768]{1,0} %broadcast.13200), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13202 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13196, bf16[768,768]{0,1} %multiply.13196), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13203 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13204 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13202, bf16[768,768]{1,0} %broadcast.13203), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13205 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13201, bf16[768,768]{0,1} %multiply.13204), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13206 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13205), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13207 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13208 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13206, bf16[768,768]{1,0} %broadcast.13207), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13218 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13217, bf16[768,768]{1,0} %add.13208), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13219 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13220 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13218, bf16[768,768]{1,0} %broadcast.13219), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13221 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p117.2522, bf16[768,768]{1,0} %multiply.13220), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p402.13243 = bf16[768]{0} parameter(402), frontend_attributes={neff_input_names="input402"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13244 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13245 = bf16[768]{0} multiply(bf16[768]{0} %p402.13243, bf16[768]{0} %broadcast.13244), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13224 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13225 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7682, bf16[768]{0} %broadcast.13224), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13241 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13242 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13225, bf16[768]{0} %broadcast.13241), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13246 = bf16[768]{0} add(bf16[768]{0} %multiply.13245, bf16[768]{0} %multiply.13242), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p401.13228 = bf16[768]{0} parameter(401), frontend_attributes={neff_input_names="input401"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13229 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13230 = bf16[768]{0} multiply(bf16[768]{0} %p401.13228, bf16[768]{0} %broadcast.13229), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13231 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13225, bf16[768]{0} %multiply.13225), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13232 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13233 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13231, bf16[768]{0} %broadcast.13232), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13234 = bf16[768]{0} add(bf16[768]{0} %multiply.13230, bf16[768]{0} %multiply.13233), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13235 = bf16[768]{0} sqrt(bf16[768]{0} %add.13234), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13236 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13237 = bf16[768]{0} add(bf16[768]{0} %sqrt.13235, bf16[768]{0} %broadcast.13236), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13247 = bf16[768]{0} divide(bf16[768]{0} %add.13246, bf16[768]{0} %add.13237), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13248 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13249 = bf16[768]{0} multiply(bf16[768]{0} %divide.13247, bf16[768]{0} %broadcast.13248), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13250 = bf16[768]{0} add(bf16[768]{0} %p116.2520, bf16[768]{0} %multiply.13249), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p404.13272 = bf16[768]{0} parameter(404), frontend_attributes={neff_input_names="input404"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13273 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13274 = bf16[768]{0} multiply(bf16[768]{0} %p404.13272, bf16[768]{0} %broadcast.13273), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13253 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13254 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7637, bf16[768]{0} %broadcast.13253), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13270 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13271 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13254, bf16[768]{0} %broadcast.13270), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13275 = bf16[768]{0} add(bf16[768]{0} %multiply.13274, bf16[768]{0} %multiply.13271), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p403.13257 = bf16[768]{0} parameter(403), frontend_attributes={neff_input_names="input403"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13258 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13259 = bf16[768]{0} multiply(bf16[768]{0} %p403.13257, bf16[768]{0} %broadcast.13258), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13260 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13254, bf16[768]{0} %multiply.13254), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13261 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13262 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13260, bf16[768]{0} %broadcast.13261), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13263 = bf16[768]{0} add(bf16[768]{0} %multiply.13259, bf16[768]{0} %multiply.13262), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13264 = bf16[768]{0} sqrt(bf16[768]{0} %add.13263), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13265 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13266 = bf16[768]{0} add(bf16[768]{0} %sqrt.13264, bf16[768]{0} %broadcast.13265), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13276 = bf16[768]{0} divide(bf16[768]{0} %add.13275, bf16[768]{0} %add.13266), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13277 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13278 = bf16[768]{0} multiply(bf16[768]{0} %divide.13276, bf16[768]{0} %broadcast.13277), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13279 = bf16[768]{0} add(bf16[768]{0} %p24.556, bf16[768]{0} %multiply.13278), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p406.13301 = bf16[768]{0} parameter(406), frontend_attributes={neff_input_names="input406"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13302 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13303 = bf16[768]{0} multiply(bf16[768]{0} %p406.13301, bf16[768]{0} %broadcast.13302), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13282 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13283 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7612, bf16[768]{0} %broadcast.13282), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13299 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13300 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13283, bf16[768]{0} %broadcast.13299), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13304 = bf16[768]{0} add(bf16[768]{0} %multiply.13303, bf16[768]{0} %multiply.13300), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p405.13286 = bf16[768]{0} parameter(405), frontend_attributes={neff_input_names="input405"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13287 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13288 = bf16[768]{0} multiply(bf16[768]{0} %p405.13286, bf16[768]{0} %broadcast.13287), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13289 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13283, bf16[768]{0} %multiply.13283), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13290 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13291 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13289, bf16[768]{0} %broadcast.13290), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13292 = bf16[768]{0} add(bf16[768]{0} %multiply.13288, bf16[768]{0} %multiply.13291), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13293 = bf16[768]{0} sqrt(bf16[768]{0} %add.13292), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13294 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13295 = bf16[768]{0} add(bf16[768]{0} %sqrt.13293, bf16[768]{0} %broadcast.13294), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13305 = bf16[768]{0} divide(bf16[768]{0} %add.13304, bf16[768]{0} %add.13295), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13306 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13307 = bf16[768]{0} multiply(bf16[768]{0} %divide.13305, bf16[768]{0} %broadcast.13306), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13308 = bf16[768]{0} add(bf16[768]{0} %p124.2675, bf16[768]{0} %multiply.13307), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p408.13330 = bf16[3072,768]{1,0} parameter(408), frontend_attributes={neff_input_names="input408"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13331 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13332 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p408.13330, bf16[3072,768]{1,0} %broadcast.13331), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13311 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13312 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.34, bf16[3072,768]{1,0} %broadcast.13311), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13328 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13329 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13312, bf16[3072,768]{1,0} %broadcast.13328), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13333 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.13332, bf16[3072,768]{0,1} %multiply.13329), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p407.13315 = bf16[3072,768]{1,0} parameter(407), frontend_attributes={neff_input_names="input407"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13316 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13317 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p407.13315, bf16[3072,768]{1,0} %broadcast.13316), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13318 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13312, bf16[3072,768]{0,1} %multiply.13312), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13319 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13320 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13318, bf16[3072,768]{1,0} %broadcast.13319), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13321 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.13317, bf16[3072,768]{0,1} %multiply.13320), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13322 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.13321), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13323 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13324 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.13322, bf16[3072,768]{1,0} %broadcast.13323), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13334 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.13333, bf16[3072,768]{1,0} %add.13324), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13335 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13336 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.13334, bf16[3072,768]{1,0} %broadcast.13335), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13337 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p128.2733, bf16[3072,768]{1,0} %multiply.13336), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p410.13359 = bf16[3072]{0} parameter(410), frontend_attributes={neff_input_names="input410"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13360 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13361 = bf16[3072]{0} multiply(bf16[3072]{0} %p410.13359, bf16[3072]{0} %broadcast.13360), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13340 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13341 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.7569, bf16[3072]{0} %broadcast.13340), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13357 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13358 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13341, bf16[3072]{0} %broadcast.13357), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13362 = bf16[3072]{0} add(bf16[3072]{0} %multiply.13361, bf16[3072]{0} %multiply.13358), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p409.13344 = bf16[3072]{0} parameter(409), frontend_attributes={neff_input_names="input409"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13345 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13346 = bf16[3072]{0} multiply(bf16[3072]{0} %p409.13344, bf16[3072]{0} %broadcast.13345), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13347 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13341, bf16[3072]{0} %multiply.13341), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13348 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13349 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13347, bf16[3072]{0} %broadcast.13348), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13350 = bf16[3072]{0} add(bf16[3072]{0} %multiply.13346, bf16[3072]{0} %multiply.13349), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13351 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.13350), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13352 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13353 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.13351, bf16[3072]{0} %broadcast.13352), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13363 = bf16[3072]{0} divide(bf16[3072]{0} %add.13362, bf16[3072]{0} %add.13353), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13364 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13365 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.13363, bf16[3072]{0} %broadcast.13364), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13366 = bf16[3072]{0} add(bf16[3072]{0} %p127.2731, bf16[3072]{0} %multiply.13365), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p412.13388 = bf16[768,3072]{1,0} parameter(412), frontend_attributes={neff_input_names="input412"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13389 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13390 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p412.13388, bf16[768,3072]{1,0} %broadcast.13389), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13369 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13370 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.35, bf16[768,3072]{1,0} %broadcast.13369), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13386 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13387 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13370, bf16[768,3072]{1,0} %broadcast.13386), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13391 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.13390, bf16[768,3072]{0,1} %multiply.13387), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p411.13373 = bf16[768,3072]{1,0} parameter(411), frontend_attributes={neff_input_names="input411"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13374 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13375 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p411.13373, bf16[768,3072]{1,0} %broadcast.13374), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13376 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13370, bf16[768,3072]{0,1} %multiply.13370), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13377 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13378 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13376, bf16[768,3072]{1,0} %broadcast.13377), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13379 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.13375, bf16[768,3072]{0,1} %multiply.13378), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13380 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.13379), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13381 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13382 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.13380, bf16[768,3072]{1,0} %broadcast.13381), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13392 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.13391, bf16[768,3072]{1,0} %add.13382), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13393 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13394 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.13392, bf16[768,3072]{1,0} %broadcast.13393), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13395 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p126.2724, bf16[768,3072]{1,0} %multiply.13394), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p414.13417 = bf16[768]{0} parameter(414), frontend_attributes={neff_input_names="input414"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13418 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13419 = bf16[768]{0} multiply(bf16[768]{0} %p414.13417, bf16[768]{0} %broadcast.13418), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13398 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13399 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7523, bf16[768]{0} %broadcast.13398), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13415 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13416 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13399, bf16[768]{0} %broadcast.13415), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13420 = bf16[768]{0} add(bf16[768]{0} %multiply.13419, bf16[768]{0} %multiply.13416), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p413.13402 = bf16[768]{0} parameter(413), frontend_attributes={neff_input_names="input413"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13403 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13404 = bf16[768]{0} multiply(bf16[768]{0} %p413.13402, bf16[768]{0} %broadcast.13403), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13405 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13399, bf16[768]{0} %multiply.13399), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13406 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13407 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13405, bf16[768]{0} %broadcast.13406), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13408 = bf16[768]{0} add(bf16[768]{0} %multiply.13404, bf16[768]{0} %multiply.13407), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13409 = bf16[768]{0} sqrt(bf16[768]{0} %add.13408), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13410 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13411 = bf16[768]{0} add(bf16[768]{0} %sqrt.13409, bf16[768]{0} %broadcast.13410), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13421 = bf16[768]{0} divide(bf16[768]{0} %add.13420, bf16[768]{0} %add.13411), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13422 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13423 = bf16[768]{0} multiply(bf16[768]{0} %divide.13421, bf16[768]{0} %broadcast.13422), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13424 = bf16[768]{0} add(bf16[768]{0} %p125.2722, bf16[768]{0} %multiply.13423), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p416.13446 = bf16[768]{0} parameter(416), frontend_attributes={neff_input_names="input416"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13447 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13448 = bf16[768]{0} multiply(bf16[768]{0} %p416.13446, bf16[768]{0} %broadcast.13447), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13427 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13428 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7478, bf16[768]{0} %broadcast.13427), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13444 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13445 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13428, bf16[768]{0} %broadcast.13444), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13449 = bf16[768]{0} add(bf16[768]{0} %multiply.13448, bf16[768]{0} %multiply.13445), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p415.13431 = bf16[768]{0} parameter(415), frontend_attributes={neff_input_names="input415"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13432 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13433 = bf16[768]{0} multiply(bf16[768]{0} %p415.13431, bf16[768]{0} %broadcast.13432), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13434 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13428, bf16[768]{0} %multiply.13428), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13435 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13436 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13434, bf16[768]{0} %broadcast.13435), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13437 = bf16[768]{0} add(bf16[768]{0} %multiply.13433, bf16[768]{0} %multiply.13436), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13438 = bf16[768]{0} sqrt(bf16[768]{0} %add.13437), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13439 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13440 = bf16[768]{0} add(bf16[768]{0} %sqrt.13438, bf16[768]{0} %broadcast.13439), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13450 = bf16[768]{0} divide(bf16[768]{0} %add.13449, bf16[768]{0} %add.13440), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13451 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13452 = bf16[768]{0} multiply(bf16[768]{0} %divide.13450, bf16[768]{0} %broadcast.13451), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13453 = bf16[768]{0} add(bf16[768]{0} %p23.529, bf16[768]{0} %multiply.13452), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p418.13475 = bf16[768]{0} parameter(418), frontend_attributes={neff_input_names="input418"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13476 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13477 = bf16[768]{0} multiply(bf16[768]{0} %p418.13475, bf16[768]{0} %broadcast.13476), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13456 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13457 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7453, bf16[768]{0} %broadcast.13456), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13473 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13474 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13457, bf16[768]{0} %broadcast.13473), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13478 = bf16[768]{0} add(bf16[768]{0} %multiply.13477, bf16[768]{0} %multiply.13474), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p417.13460 = bf16[768]{0} parameter(417), frontend_attributes={neff_input_names="input417"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13461 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13462 = bf16[768]{0} multiply(bf16[768]{0} %p417.13460, bf16[768]{0} %broadcast.13461), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13463 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13457, bf16[768]{0} %multiply.13457), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13464 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13465 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13463, bf16[768]{0} %broadcast.13464), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13466 = bf16[768]{0} add(bf16[768]{0} %multiply.13462, bf16[768]{0} %multiply.13465), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13467 = bf16[768]{0} sqrt(bf16[768]{0} %add.13466), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13468 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13469 = bf16[768]{0} add(bf16[768]{0} %sqrt.13467, bf16[768]{0} %broadcast.13468), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13479 = bf16[768]{0} divide(bf16[768]{0} %add.13478, bf16[768]{0} %add.13469), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13480 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13481 = bf16[768]{0} multiply(bf16[768]{0} %divide.13479, bf16[768]{0} %broadcast.13480), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13482 = bf16[768]{0} add(bf16[768]{0} %p129.2768, bf16[768]{0} %multiply.13481), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p420.13504 = bf16[768,768]{1,0} parameter(420), frontend_attributes={neff_input_names="input420"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13505 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13506 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p420.13504, bf16[768,768]{1,0} %broadcast.13505), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13485 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13486 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.36, bf16[768,768]{1,0} %broadcast.13485), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13502 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13503 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13486, bf16[768,768]{1,0} %broadcast.13502), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13507 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13506, bf16[768,768]{0,1} %multiply.13503), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p419.13489 = bf16[768,768]{1,0} parameter(419), frontend_attributes={neff_input_names="input419"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13490 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13491 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p419.13489, bf16[768,768]{1,0} %broadcast.13490), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13492 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13486, bf16[768,768]{0,1} %multiply.13486), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13493 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13494 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13492, bf16[768,768]{1,0} %broadcast.13493), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13495 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13491, bf16[768,768]{0,1} %multiply.13494), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13496 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13495), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13497 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13498 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13496, bf16[768,768]{1,0} %broadcast.13497), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13508 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13507, bf16[768,768]{1,0} %add.13498), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13509 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13510 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13508, bf16[768,768]{1,0} %broadcast.13509), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13511 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p137.2906, bf16[768,768]{1,0} %multiply.13510), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p422.13533 = bf16[768]{0} parameter(422), frontend_attributes={neff_input_names="input422"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13534 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13535 = bf16[768]{0} multiply(bf16[768]{0} %p422.13533, bf16[768]{0} %broadcast.13534), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1065 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.264 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.24, bf16[12,64]{1,0} %broadcast.1065), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1176 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.320 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.264, bf16[12,64]{1,0} %broadcast.1176), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3642 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.320), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13536 = bf16[768]{0} add(bf16[768]{0} %multiply.13535, bf16[768]{0} %reshape.3642), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p421.13518 = bf16[768]{0} parameter(421), frontend_attributes={neff_input_names="input421"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13519 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13520 = bf16[768]{0} multiply(bf16[768]{0} %p421.13518, bf16[768]{0} %broadcast.13519), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.319 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.264, bf16[12,64]{1,0} %multiply.264), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1265 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.373 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.319, bf16[12,64]{1,0} %broadcast.1265), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3995 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.373), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13524 = bf16[768]{0} add(bf16[768]{0} %multiply.13520, bf16[768]{0} %reshape.3995), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13525 = bf16[768]{0} sqrt(bf16[768]{0} %add.13524), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13526 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13527 = bf16[768]{0} add(bf16[768]{0} %sqrt.13525, bf16[768]{0} %broadcast.13526), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13537 = bf16[768]{0} divide(bf16[768]{0} %add.13536, bf16[768]{0} %add.13527), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13538 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13539 = bf16[768]{0} multiply(bf16[768]{0} %divide.13537, bf16[768]{0} %broadcast.13538), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13540 = bf16[768]{0} add(bf16[768]{0} %p136.2904, bf16[768]{0} %multiply.13539), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p424.13562 = bf16[768,768]{1,0} parameter(424), frontend_attributes={neff_input_names="input424"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13563 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13564 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p424.13562, bf16[768,768]{1,0} %broadcast.13563), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13543 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13544 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.37, bf16[768,768]{1,0} %broadcast.13543), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13560 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13561 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13544, bf16[768,768]{1,0} %broadcast.13560), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13565 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13564, bf16[768,768]{0,1} %multiply.13561), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p423.13547 = bf16[768,768]{1,0} parameter(423), frontend_attributes={neff_input_names="input423"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13548 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13549 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p423.13547, bf16[768,768]{1,0} %broadcast.13548), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13550 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13544, bf16[768,768]{0,1} %multiply.13544), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13551 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13552 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13550, bf16[768,768]{1,0} %broadcast.13551), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13553 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13549, bf16[768,768]{0,1} %multiply.13552), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13554 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13553), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13555 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13556 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13554, bf16[768,768]{1,0} %broadcast.13555), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13566 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13565, bf16[768,768]{1,0} %add.13556), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13567 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13568 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13566, bf16[768,768]{1,0} %broadcast.13567), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13569 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p135.2885, bf16[768,768]{1,0} %multiply.13568), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p426.13591 = bf16[768]{0} parameter(426), frontend_attributes={neff_input_names="input426"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13592 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13593 = bf16[768]{0} multiply(bf16[768]{0} %p426.13591, bf16[768]{0} %broadcast.13592), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1067 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.265 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.25, bf16[12,64]{1,0} %broadcast.1067), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1178 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.322 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.265, bf16[12,64]{1,0} %broadcast.1178), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3649 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.322), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13594 = bf16[768]{0} add(bf16[768]{0} %multiply.13593, bf16[768]{0} %reshape.3649), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p425.13576 = bf16[768]{0} parameter(425), frontend_attributes={neff_input_names="input425"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13577 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13578 = bf16[768]{0} multiply(bf16[768]{0} %p425.13576, bf16[768]{0} %broadcast.13577), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.321 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.265, bf16[12,64]{1,0} %multiply.265), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1267 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.374 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.321, bf16[12,64]{1,0} %broadcast.1267), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.3998 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.374), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13582 = bf16[768]{0} add(bf16[768]{0} %multiply.13578, bf16[768]{0} %reshape.3998), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13583 = bf16[768]{0} sqrt(bf16[768]{0} %add.13582), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13584 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13585 = bf16[768]{0} add(bf16[768]{0} %sqrt.13583, bf16[768]{0} %broadcast.13584), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13595 = bf16[768]{0} divide(bf16[768]{0} %add.13594, bf16[768]{0} %add.13585), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13596 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13597 = bf16[768]{0} multiply(bf16[768]{0} %divide.13595, bf16[768]{0} %broadcast.13596), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13598 = bf16[768]{0} add(bf16[768]{0} %p134.2883, bf16[768]{0} %multiply.13597), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p428.13620 = bf16[768,768]{1,0} parameter(428), frontend_attributes={neff_input_names="input428"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13621 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13622 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p428.13620, bf16[768,768]{1,0} %broadcast.13621), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13601 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13602 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.38, bf16[768,768]{1,0} %broadcast.13601), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13618 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13619 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13602, bf16[768,768]{1,0} %broadcast.13618), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13623 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13622, bf16[768,768]{0,1} %multiply.13619), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p427.13605 = bf16[768,768]{1,0} parameter(427), frontend_attributes={neff_input_names="input427"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13606 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13607 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p427.13605, bf16[768,768]{1,0} %broadcast.13606), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13608 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13602, bf16[768,768]{0,1} %multiply.13602), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13609 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13610 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13608, bf16[768,768]{1,0} %broadcast.13609), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13611 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13607, bf16[768,768]{0,1} %multiply.13610), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13612 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13611), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13613 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13614 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13612, bf16[768,768]{1,0} %broadcast.13613), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13624 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13623, bf16[768,768]{1,0} %add.13614), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13625 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13626 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13624, bf16[768,768]{1,0} %broadcast.13625), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13627 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p133.2826, bf16[768,768]{1,0} %multiply.13626), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p430.13649 = bf16[768]{0} parameter(430), frontend_attributes={neff_input_names="input430"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13650 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13651 = bf16[768]{0} multiply(bf16[768]{0} %p430.13649, bf16[768]{0} %broadcast.13650), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1070 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.266 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.26, bf16[12,64]{1,0} %broadcast.1070), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1180 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.324 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.266, bf16[12,64]{1,0} %broadcast.1180), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3656 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.324), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13652 = bf16[768]{0} add(bf16[768]{0} %multiply.13651, bf16[768]{0} %reshape.3656), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p429.13634 = bf16[768]{0} parameter(429), frontend_attributes={neff_input_names="input429"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13635 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13636 = bf16[768]{0} multiply(bf16[768]{0} %p429.13634, bf16[768]{0} %broadcast.13635), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.323 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.266, bf16[12,64]{1,0} %multiply.266), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1269 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.375 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.323, bf16[12,64]{1,0} %broadcast.1269), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4003 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.375), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13640 = bf16[768]{0} add(bf16[768]{0} %multiply.13636, bf16[768]{0} %reshape.4003), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13641 = bf16[768]{0} sqrt(bf16[768]{0} %add.13640), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13642 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13643 = bf16[768]{0} add(bf16[768]{0} %sqrt.13641, bf16[768]{0} %broadcast.13642), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13653 = bf16[768]{0} divide(bf16[768]{0} %add.13652, bf16[768]{0} %add.13643), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13654 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13655 = bf16[768]{0} multiply(bf16[768]{0} %divide.13653, bf16[768]{0} %broadcast.13654), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13656 = bf16[768]{0} add(bf16[768]{0} %p132.2824, bf16[768]{0} %multiply.13655), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p432.13678 = bf16[768,768]{1,0} parameter(432), frontend_attributes={neff_input_names="input432"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13679 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13680 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p432.13678, bf16[768,768]{1,0} %broadcast.13679), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13659 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13660 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.39, bf16[768,768]{1,0} %broadcast.13659), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13676 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13677 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13660, bf16[768,768]{1,0} %broadcast.13676), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13681 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13680, bf16[768,768]{0,1} %multiply.13677), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p431.13663 = bf16[768,768]{1,0} parameter(431), frontend_attributes={neff_input_names="input431"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13664 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13665 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p431.13663, bf16[768,768]{1,0} %broadcast.13664), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13666 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13660, bf16[768,768]{0,1} %multiply.13660), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13667 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13668 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13666, bf16[768,768]{1,0} %broadcast.13667), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13669 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13665, bf16[768,768]{0,1} %multiply.13668), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13670 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13669), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13671 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13672 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13670, bf16[768,768]{1,0} %broadcast.13671), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13682 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13681, bf16[768,768]{1,0} %add.13672), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13683 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13684 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13682, bf16[768,768]{1,0} %broadcast.13683), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13685 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p131.2817, bf16[768,768]{1,0} %multiply.13684), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p434.13707 = bf16[768]{0} parameter(434), frontend_attributes={neff_input_names="input434"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13708 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13709 = bf16[768]{0} multiply(bf16[768]{0} %p434.13707, bf16[768]{0} %broadcast.13708), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13688 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13689 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7235, bf16[768]{0} %broadcast.13688), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13705 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13706 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13689, bf16[768]{0} %broadcast.13705), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13710 = bf16[768]{0} add(bf16[768]{0} %multiply.13709, bf16[768]{0} %multiply.13706), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p433.13692 = bf16[768]{0} parameter(433), frontend_attributes={neff_input_names="input433"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13693 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13694 = bf16[768]{0} multiply(bf16[768]{0} %p433.13692, bf16[768]{0} %broadcast.13693), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13695 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13689, bf16[768]{0} %multiply.13689), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13696 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13697 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13695, bf16[768]{0} %broadcast.13696), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13698 = bf16[768]{0} add(bf16[768]{0} %multiply.13694, bf16[768]{0} %multiply.13697), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13699 = bf16[768]{0} sqrt(bf16[768]{0} %add.13698), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13700 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13701 = bf16[768]{0} add(bf16[768]{0} %sqrt.13699, bf16[768]{0} %broadcast.13700), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13711 = bf16[768]{0} divide(bf16[768]{0} %add.13710, bf16[768]{0} %add.13701), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13712 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13713 = bf16[768]{0} multiply(bf16[768]{0} %divide.13711, bf16[768]{0} %broadcast.13712), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13714 = bf16[768]{0} add(bf16[768]{0} %p130.2815, bf16[768]{0} %multiply.13713), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p436.13736 = bf16[768]{0} parameter(436), frontend_attributes={neff_input_names="input436"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13737 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13738 = bf16[768]{0} multiply(bf16[768]{0} %p436.13736, bf16[768]{0} %broadcast.13737), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13717 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13718 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7190, bf16[768]{0} %broadcast.13717), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13734 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13735 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13718, bf16[768]{0} %broadcast.13734), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13739 = bf16[768]{0} add(bf16[768]{0} %multiply.13738, bf16[768]{0} %multiply.13735), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p435.13721 = bf16[768]{0} parameter(435), frontend_attributes={neff_input_names="input435"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13722 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13723 = bf16[768]{0} multiply(bf16[768]{0} %p435.13721, bf16[768]{0} %broadcast.13722), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13724 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13718, bf16[768]{0} %multiply.13718), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13725 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13726 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13724, bf16[768]{0} %broadcast.13725), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13727 = bf16[768]{0} add(bf16[768]{0} %multiply.13723, bf16[768]{0} %multiply.13726), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13728 = bf16[768]{0} sqrt(bf16[768]{0} %add.13727), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13729 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13730 = bf16[768]{0} add(bf16[768]{0} %sqrt.13728, bf16[768]{0} %broadcast.13729), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13740 = bf16[768]{0} divide(bf16[768]{0} %add.13739, bf16[768]{0} %add.13730), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13741 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13742 = bf16[768]{0} multiply(bf16[768]{0} %divide.13740, bf16[768]{0} %broadcast.13741), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13743 = bf16[768]{0} add(bf16[768]{0} %p22.502, bf16[768]{0} %multiply.13742), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p438.13765 = bf16[768]{0} parameter(438), frontend_attributes={neff_input_names="input438"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13766 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13767 = bf16[768]{0} multiply(bf16[768]{0} %p438.13765, bf16[768]{0} %broadcast.13766), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13746 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13747 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7165, bf16[768]{0} %broadcast.13746), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13763 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13764 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13747, bf16[768]{0} %broadcast.13763), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13768 = bf16[768]{0} add(bf16[768]{0} %multiply.13767, bf16[768]{0} %multiply.13764), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p437.13750 = bf16[768]{0} parameter(437), frontend_attributes={neff_input_names="input437"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13751 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13752 = bf16[768]{0} multiply(bf16[768]{0} %p437.13750, bf16[768]{0} %broadcast.13751), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13753 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13747, bf16[768]{0} %multiply.13747), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13754 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13755 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13753, bf16[768]{0} %broadcast.13754), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13756 = bf16[768]{0} add(bf16[768]{0} %multiply.13752, bf16[768]{0} %multiply.13755), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13757 = bf16[768]{0} sqrt(bf16[768]{0} %add.13756), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13758 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13759 = bf16[768]{0} add(bf16[768]{0} %sqrt.13757, bf16[768]{0} %broadcast.13758), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13769 = bf16[768]{0} divide(bf16[768]{0} %add.13768, bf16[768]{0} %add.13759), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13770 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13771 = bf16[768]{0} multiply(bf16[768]{0} %divide.13769, bf16[768]{0} %broadcast.13770), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13772 = bf16[768]{0} add(bf16[768]{0} %p138.2970, bf16[768]{0} %multiply.13771), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p440.13794 = bf16[3072,768]{1,0} parameter(440), frontend_attributes={neff_input_names="input440"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13795 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13796 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p440.13794, bf16[3072,768]{1,0} %broadcast.13795), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13775 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13776 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.40, bf16[3072,768]{1,0} %broadcast.13775), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13792 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13793 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13776, bf16[3072,768]{1,0} %broadcast.13792), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13797 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.13796, bf16[3072,768]{0,1} %multiply.13793), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p439.13779 = bf16[3072,768]{1,0} parameter(439), frontend_attributes={neff_input_names="input439"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13780 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13781 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p439.13779, bf16[3072,768]{1,0} %broadcast.13780), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13782 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13776, bf16[3072,768]{0,1} %multiply.13776), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13783 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13784 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.13782, bf16[3072,768]{1,0} %broadcast.13783), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13785 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.13781, bf16[3072,768]{0,1} %multiply.13784), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13786 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.13785), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13787 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13788 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.13786, bf16[3072,768]{1,0} %broadcast.13787), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13798 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.13797, bf16[3072,768]{1,0} %add.13788), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13799 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13800 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.13798, bf16[3072,768]{1,0} %broadcast.13799), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13801 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p142.3028, bf16[3072,768]{1,0} %multiply.13800), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p442.13823 = bf16[3072]{0} parameter(442), frontend_attributes={neff_input_names="input442"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13824 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13825 = bf16[3072]{0} multiply(bf16[3072]{0} %p442.13823, bf16[3072]{0} %broadcast.13824), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13804 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13805 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.7122, bf16[3072]{0} %broadcast.13804), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13821 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13822 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13805, bf16[3072]{0} %broadcast.13821), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13826 = bf16[3072]{0} add(bf16[3072]{0} %multiply.13825, bf16[3072]{0} %multiply.13822), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p441.13808 = bf16[3072]{0} parameter(441), frontend_attributes={neff_input_names="input441"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13809 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13810 = bf16[3072]{0} multiply(bf16[3072]{0} %p441.13808, bf16[3072]{0} %broadcast.13809), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13811 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13805, bf16[3072]{0} %multiply.13805), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13812 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13813 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.13811, bf16[3072]{0} %broadcast.13812), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13814 = bf16[3072]{0} add(bf16[3072]{0} %multiply.13810, bf16[3072]{0} %multiply.13813), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13815 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.13814), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13816 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13817 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.13815, bf16[3072]{0} %broadcast.13816), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13827 = bf16[3072]{0} divide(bf16[3072]{0} %add.13826, bf16[3072]{0} %add.13817), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13828 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13829 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.13827, bf16[3072]{0} %broadcast.13828), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13830 = bf16[3072]{0} add(bf16[3072]{0} %p141.3026, bf16[3072]{0} %multiply.13829), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p444.13852 = bf16[768,3072]{1,0} parameter(444), frontend_attributes={neff_input_names="input444"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13853 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13854 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p444.13852, bf16[768,3072]{1,0} %broadcast.13853), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13833 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13834 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.41, bf16[768,3072]{1,0} %broadcast.13833), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13850 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13851 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13834, bf16[768,3072]{1,0} %broadcast.13850), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13855 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.13854, bf16[768,3072]{0,1} %multiply.13851), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p443.13837 = bf16[768,3072]{1,0} parameter(443), frontend_attributes={neff_input_names="input443"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13838 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13839 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p443.13837, bf16[768,3072]{1,0} %broadcast.13838), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13840 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13834, bf16[768,3072]{0,1} %multiply.13834), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13841 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13842 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.13840, bf16[768,3072]{1,0} %broadcast.13841), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13843 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.13839, bf16[768,3072]{0,1} %multiply.13842), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13844 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.13843), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13845 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13846 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.13844, bf16[768,3072]{1,0} %broadcast.13845), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13856 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.13855, bf16[768,3072]{1,0} %add.13846), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13857 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13858 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.13856, bf16[768,3072]{1,0} %broadcast.13857), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13859 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p140.3019, bf16[768,3072]{1,0} %multiply.13858), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p446.13881 = bf16[768]{0} parameter(446), frontend_attributes={neff_input_names="input446"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13882 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13883 = bf16[768]{0} multiply(bf16[768]{0} %p446.13881, bf16[768]{0} %broadcast.13882), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13862 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13863 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7076, bf16[768]{0} %broadcast.13862), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13879 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13880 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13863, bf16[768]{0} %broadcast.13879), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13884 = bf16[768]{0} add(bf16[768]{0} %multiply.13883, bf16[768]{0} %multiply.13880), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p445.13866 = bf16[768]{0} parameter(445), frontend_attributes={neff_input_names="input445"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13867 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13868 = bf16[768]{0} multiply(bf16[768]{0} %p445.13866, bf16[768]{0} %broadcast.13867), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13869 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13863, bf16[768]{0} %multiply.13863), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13870 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13871 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13869, bf16[768]{0} %broadcast.13870), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13872 = bf16[768]{0} add(bf16[768]{0} %multiply.13868, bf16[768]{0} %multiply.13871), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13873 = bf16[768]{0} sqrt(bf16[768]{0} %add.13872), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13874 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13875 = bf16[768]{0} add(bf16[768]{0} %sqrt.13873, bf16[768]{0} %broadcast.13874), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13885 = bf16[768]{0} divide(bf16[768]{0} %add.13884, bf16[768]{0} %add.13875), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13886 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13887 = bf16[768]{0} multiply(bf16[768]{0} %divide.13885, bf16[768]{0} %broadcast.13886), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13888 = bf16[768]{0} add(bf16[768]{0} %p139.3017, bf16[768]{0} %multiply.13887), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p448.13910 = bf16[768]{0} parameter(448), frontend_attributes={neff_input_names="input448"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13911 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13912 = bf16[768]{0} multiply(bf16[768]{0} %p448.13910, bf16[768]{0} %broadcast.13911), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13891 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13892 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7031, bf16[768]{0} %broadcast.13891), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13908 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13909 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13892, bf16[768]{0} %broadcast.13908), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13913 = bf16[768]{0} add(bf16[768]{0} %multiply.13912, bf16[768]{0} %multiply.13909), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p447.13895 = bf16[768]{0} parameter(447), frontend_attributes={neff_input_names="input447"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13896 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13897 = bf16[768]{0} multiply(bf16[768]{0} %p447.13895, bf16[768]{0} %broadcast.13896), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13898 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13892, bf16[768]{0} %multiply.13892), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13899 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13900 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13898, bf16[768]{0} %broadcast.13899), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13901 = bf16[768]{0} add(bf16[768]{0} %multiply.13897, bf16[768]{0} %multiply.13900), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13902 = bf16[768]{0} sqrt(bf16[768]{0} %add.13901), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13903 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13904 = bf16[768]{0} add(bf16[768]{0} %sqrt.13902, bf16[768]{0} %broadcast.13903), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13914 = bf16[768]{0} divide(bf16[768]{0} %add.13913, bf16[768]{0} %add.13904), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13915 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13916 = bf16[768]{0} multiply(bf16[768]{0} %divide.13914, bf16[768]{0} %broadcast.13915), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13917 = bf16[768]{0} add(bf16[768]{0} %p21.475, bf16[768]{0} %multiply.13916), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p450.13939 = bf16[768]{0} parameter(450), frontend_attributes={neff_input_names="input450"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13940 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13941 = bf16[768]{0} multiply(bf16[768]{0} %p450.13939, bf16[768]{0} %broadcast.13940), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13920 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13921 = bf16[768]{0} multiply(bf16[768]{0} %reduce.7006, bf16[768]{0} %broadcast.13920), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13937 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13938 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13921, bf16[768]{0} %broadcast.13937), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13942 = bf16[768]{0} add(bf16[768]{0} %multiply.13941, bf16[768]{0} %multiply.13938), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p449.13924 = bf16[768]{0} parameter(449), frontend_attributes={neff_input_names="input449"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13925 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13926 = bf16[768]{0} multiply(bf16[768]{0} %p449.13924, bf16[768]{0} %broadcast.13925), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13927 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13921, bf16[768]{0} %multiply.13921), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13928 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13929 = bf16[768]{0} multiply(bf16[768]{0} %multiply.13927, bf16[768]{0} %broadcast.13928), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13930 = bf16[768]{0} add(bf16[768]{0} %multiply.13926, bf16[768]{0} %multiply.13929), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13931 = bf16[768]{0} sqrt(bf16[768]{0} %add.13930), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13932 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13933 = bf16[768]{0} add(bf16[768]{0} %sqrt.13931, bf16[768]{0} %broadcast.13932), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13943 = bf16[768]{0} divide(bf16[768]{0} %add.13942, bf16[768]{0} %add.13933), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13944 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13945 = bf16[768]{0} multiply(bf16[768]{0} %divide.13943, bf16[768]{0} %broadcast.13944), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13946 = bf16[768]{0} add(bf16[768]{0} %p143.3063, bf16[768]{0} %multiply.13945), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p452.13968 = bf16[768,768]{1,0} parameter(452), frontend_attributes={neff_input_names="input452"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13969 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13970 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p452.13968, bf16[768,768]{1,0} %broadcast.13969), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13949 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.13950 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.42, bf16[768,768]{1,0} %broadcast.13949), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13966 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13967 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13950, bf16[768,768]{1,0} %broadcast.13966), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.13971 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13970, bf16[768,768]{0,1} %multiply.13967), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p451.13953 = bf16[768,768]{1,0} parameter(451), frontend_attributes={neff_input_names="input451"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13954 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13955 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p451.13953, bf16[768,768]{1,0} %broadcast.13954), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13956 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13950, bf16[768,768]{0,1} %multiply.13950), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13957 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13958 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.13956, bf16[768,768]{1,0} %broadcast.13957), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13959 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.13955, bf16[768,768]{0,1} %multiply.13958), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13960 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.13959), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13961 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13962 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.13960, bf16[768,768]{1,0} %broadcast.13961), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.13972 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.13971, bf16[768,768]{1,0} %add.13962), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.13973 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.13974 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.13972, bf16[768,768]{1,0} %broadcast.13973), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.13975 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p151.3201, bf16[768,768]{1,0} %multiply.13974), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p454.13997 = bf16[768]{0} parameter(454), frontend_attributes={neff_input_names="input454"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.13998 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.13999 = bf16[768]{0} multiply(bf16[768]{0} %p454.13997, bf16[768]{0} %broadcast.13998), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1074 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.267 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.27, bf16[12,64]{1,0} %broadcast.1074), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1182 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.326 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.267, bf16[12,64]{1,0} %broadcast.1182), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3663 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.326), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14000 = bf16[768]{0} add(bf16[768]{0} %multiply.13999, bf16[768]{0} %reshape.3663), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p453.13982 = bf16[768]{0} parameter(453), frontend_attributes={neff_input_names="input453"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.13983 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.13984 = bf16[768]{0} multiply(bf16[768]{0} %p453.13982, bf16[768]{0} %broadcast.13983), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.325 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.267, bf16[12,64]{1,0} %multiply.267), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1271 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.376 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.325, bf16[12,64]{1,0} %broadcast.1271), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4006 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.376), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.13988 = bf16[768]{0} add(bf16[768]{0} %multiply.13984, bf16[768]{0} %reshape.4006), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.13989 = bf16[768]{0} sqrt(bf16[768]{0} %add.13988), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.13990 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.13991 = bf16[768]{0} add(bf16[768]{0} %sqrt.13989, bf16[768]{0} %broadcast.13990), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14001 = bf16[768]{0} divide(bf16[768]{0} %add.14000, bf16[768]{0} %add.13991), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14002 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14003 = bf16[768]{0} multiply(bf16[768]{0} %divide.14001, bf16[768]{0} %broadcast.14002), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14004 = bf16[768]{0} add(bf16[768]{0} %p150.3199, bf16[768]{0} %multiply.14003), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p456.14026 = bf16[768,768]{1,0} parameter(456), frontend_attributes={neff_input_names="input456"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14027 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14028 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p456.14026, bf16[768,768]{1,0} %broadcast.14027), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14007 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14008 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.43, bf16[768,768]{1,0} %broadcast.14007), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14024 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14025 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14008, bf16[768,768]{1,0} %broadcast.14024), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14029 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14028, bf16[768,768]{0,1} %multiply.14025), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p455.14011 = bf16[768,768]{1,0} parameter(455), frontend_attributes={neff_input_names="input455"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14012 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14013 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p455.14011, bf16[768,768]{1,0} %broadcast.14012), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14014 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14008, bf16[768,768]{0,1} %multiply.14008), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14015 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14016 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14014, bf16[768,768]{1,0} %broadcast.14015), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14017 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14013, bf16[768,768]{0,1} %multiply.14016), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14018 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14017), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14019 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14020 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14018, bf16[768,768]{1,0} %broadcast.14019), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14030 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14029, bf16[768,768]{1,0} %add.14020), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14031 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14032 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14030, bf16[768,768]{1,0} %broadcast.14031), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14033 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p149.3180, bf16[768,768]{1,0} %multiply.14032), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p458.14055 = bf16[768]{0} parameter(458), frontend_attributes={neff_input_names="input458"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14056 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14057 = bf16[768]{0} multiply(bf16[768]{0} %p458.14055, bf16[768]{0} %broadcast.14056), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1076 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.268 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.28, bf16[12,64]{1,0} %broadcast.1076), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1185 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.328 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.268, bf16[12,64]{1,0} %broadcast.1185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3670 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.328), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14058 = bf16[768]{0} add(bf16[768]{0} %multiply.14057, bf16[768]{0} %reshape.3670), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p457.14040 = bf16[768]{0} parameter(457), frontend_attributes={neff_input_names="input457"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14041 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14042 = bf16[768]{0} multiply(bf16[768]{0} %p457.14040, bf16[768]{0} %broadcast.14041), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.327 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.268, bf16[12,64]{1,0} %multiply.268), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1273 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.377 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.327, bf16[12,64]{1,0} %broadcast.1273), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4011 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.377), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14046 = bf16[768]{0} add(bf16[768]{0} %multiply.14042, bf16[768]{0} %reshape.4011), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14047 = bf16[768]{0} sqrt(bf16[768]{0} %add.14046), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14048 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14049 = bf16[768]{0} add(bf16[768]{0} %sqrt.14047, bf16[768]{0} %broadcast.14048), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14059 = bf16[768]{0} divide(bf16[768]{0} %add.14058, bf16[768]{0} %add.14049), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14060 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14061 = bf16[768]{0} multiply(bf16[768]{0} %divide.14059, bf16[768]{0} %broadcast.14060), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14062 = bf16[768]{0} add(bf16[768]{0} %p148.3178, bf16[768]{0} %multiply.14061), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p460.14084 = bf16[768,768]{1,0} parameter(460), frontend_attributes={neff_input_names="input460"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14085 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14086 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p460.14084, bf16[768,768]{1,0} %broadcast.14085), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14065 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14066 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.44, bf16[768,768]{1,0} %broadcast.14065), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14082 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14083 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14066, bf16[768,768]{1,0} %broadcast.14082), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14087 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14086, bf16[768,768]{0,1} %multiply.14083), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p459.14069 = bf16[768,768]{1,0} parameter(459), frontend_attributes={neff_input_names="input459"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14070 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14071 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p459.14069, bf16[768,768]{1,0} %broadcast.14070), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14072 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14066, bf16[768,768]{0,1} %multiply.14066), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14073 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14074 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14072, bf16[768,768]{1,0} %broadcast.14073), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14075 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14071, bf16[768,768]{0,1} %multiply.14074), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14076 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14075), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14077 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14078 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14076, bf16[768,768]{1,0} %broadcast.14077), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14088 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14087, bf16[768,768]{1,0} %add.14078), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14089 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14090 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14088, bf16[768,768]{1,0} %broadcast.14089), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14091 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p147.3121, bf16[768,768]{1,0} %multiply.14090), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p462.14113 = bf16[768]{0} parameter(462), frontend_attributes={neff_input_names="input462"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14114 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14115 = bf16[768]{0} multiply(bf16[768]{0} %p462.14113, bf16[768]{0} %broadcast.14114), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1079 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.269 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.29, bf16[12,64]{1,0} %broadcast.1079), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1187 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.330 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.269, bf16[12,64]{1,0} %broadcast.1187), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3678 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.330), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14116 = bf16[768]{0} add(bf16[768]{0} %multiply.14115, bf16[768]{0} %reshape.3678), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p461.14098 = bf16[768]{0} parameter(461), frontend_attributes={neff_input_names="input461"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14099 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14100 = bf16[768]{0} multiply(bf16[768]{0} %p461.14098, bf16[768]{0} %broadcast.14099), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.329 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.269, bf16[12,64]{1,0} %multiply.269), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1275 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.378 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.329, bf16[12,64]{1,0} %broadcast.1275), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4017 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.378), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14104 = bf16[768]{0} add(bf16[768]{0} %multiply.14100, bf16[768]{0} %reshape.4017), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14105 = bf16[768]{0} sqrt(bf16[768]{0} %add.14104), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14106 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14107 = bf16[768]{0} add(bf16[768]{0} %sqrt.14105, bf16[768]{0} %broadcast.14106), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14117 = bf16[768]{0} divide(bf16[768]{0} %add.14116, bf16[768]{0} %add.14107), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14118 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14119 = bf16[768]{0} multiply(bf16[768]{0} %divide.14117, bf16[768]{0} %broadcast.14118), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14120 = bf16[768]{0} add(bf16[768]{0} %p146.3119, bf16[768]{0} %multiply.14119), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p464.14142 = bf16[768,768]{1,0} parameter(464), frontend_attributes={neff_input_names="input464"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14143 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14144 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p464.14142, bf16[768,768]{1,0} %broadcast.14143), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14123 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14124 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.45, bf16[768,768]{1,0} %broadcast.14123), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14140 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14141 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14124, bf16[768,768]{1,0} %broadcast.14140), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14145 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14144, bf16[768,768]{0,1} %multiply.14141), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p463.14127 = bf16[768,768]{1,0} parameter(463), frontend_attributes={neff_input_names="input463"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14128 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14129 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p463.14127, bf16[768,768]{1,0} %broadcast.14128), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14130 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14124, bf16[768,768]{0,1} %multiply.14124), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14131 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14132 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14130, bf16[768,768]{1,0} %broadcast.14131), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14133 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14129, bf16[768,768]{0,1} %multiply.14132), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14134 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14133), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14135 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14136 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14134, bf16[768,768]{1,0} %broadcast.14135), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14146 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14145, bf16[768,768]{1,0} %add.14136), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14147 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14148 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14146, bf16[768,768]{1,0} %broadcast.14147), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14149 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p145.3112, bf16[768,768]{1,0} %multiply.14148), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p466.14171 = bf16[768]{0} parameter(466), frontend_attributes={neff_input_names="input466"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14172 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14173 = bf16[768]{0} multiply(bf16[768]{0} %p466.14171, bf16[768]{0} %broadcast.14172), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14152 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14153 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6788, bf16[768]{0} %broadcast.14152), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14169 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14170 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14153, bf16[768]{0} %broadcast.14169), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14174 = bf16[768]{0} add(bf16[768]{0} %multiply.14173, bf16[768]{0} %multiply.14170), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p465.14156 = bf16[768]{0} parameter(465), frontend_attributes={neff_input_names="input465"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14157 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14158 = bf16[768]{0} multiply(bf16[768]{0} %p465.14156, bf16[768]{0} %broadcast.14157), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14159 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14153, bf16[768]{0} %multiply.14153), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14160 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14161 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14159, bf16[768]{0} %broadcast.14160), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14162 = bf16[768]{0} add(bf16[768]{0} %multiply.14158, bf16[768]{0} %multiply.14161), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14163 = bf16[768]{0} sqrt(bf16[768]{0} %add.14162), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14164 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14165 = bf16[768]{0} add(bf16[768]{0} %sqrt.14163, bf16[768]{0} %broadcast.14164), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14175 = bf16[768]{0} divide(bf16[768]{0} %add.14174, bf16[768]{0} %add.14165), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14176 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14177 = bf16[768]{0} multiply(bf16[768]{0} %divide.14175, bf16[768]{0} %broadcast.14176), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14178 = bf16[768]{0} add(bf16[768]{0} %p144.3110, bf16[768]{0} %multiply.14177), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p468.14200 = bf16[768]{0} parameter(468), frontend_attributes={neff_input_names="input468"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14201 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14202 = bf16[768]{0} multiply(bf16[768]{0} %p468.14200, bf16[768]{0} %broadcast.14201), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14181 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14182 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6743, bf16[768]{0} %broadcast.14181), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14198 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14199 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14182, bf16[768]{0} %broadcast.14198), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14203 = bf16[768]{0} add(bf16[768]{0} %multiply.14202, bf16[768]{0} %multiply.14199), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p467.14185 = bf16[768]{0} parameter(467), frontend_attributes={neff_input_names="input467"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14186 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14187 = bf16[768]{0} multiply(bf16[768]{0} %p467.14185, bf16[768]{0} %broadcast.14186), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14188 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14182, bf16[768]{0} %multiply.14182), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14189 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14190 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14188, bf16[768]{0} %broadcast.14189), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14191 = bf16[768]{0} add(bf16[768]{0} %multiply.14187, bf16[768]{0} %multiply.14190), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14192 = bf16[768]{0} sqrt(bf16[768]{0} %add.14191), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14193 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14194 = bf16[768]{0} add(bf16[768]{0} %sqrt.14192, bf16[768]{0} %broadcast.14193), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14204 = bf16[768]{0} divide(bf16[768]{0} %add.14203, bf16[768]{0} %add.14194), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14205 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14206 = bf16[768]{0} multiply(bf16[768]{0} %divide.14204, bf16[768]{0} %broadcast.14205), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14207 = bf16[768]{0} add(bf16[768]{0} %p20.448, bf16[768]{0} %multiply.14206), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p470.14229 = bf16[768]{0} parameter(470), frontend_attributes={neff_input_names="input470"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14230 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14231 = bf16[768]{0} multiply(bf16[768]{0} %p470.14229, bf16[768]{0} %broadcast.14230), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14210 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14211 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6718, bf16[768]{0} %broadcast.14210), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14227 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14228 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14211, bf16[768]{0} %broadcast.14227), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14232 = bf16[768]{0} add(bf16[768]{0} %multiply.14231, bf16[768]{0} %multiply.14228), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p469.14214 = bf16[768]{0} parameter(469), frontend_attributes={neff_input_names="input469"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14215 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14216 = bf16[768]{0} multiply(bf16[768]{0} %p469.14214, bf16[768]{0} %broadcast.14215), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14217 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14211, bf16[768]{0} %multiply.14211), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14218 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14219 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14217, bf16[768]{0} %broadcast.14218), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14220 = bf16[768]{0} add(bf16[768]{0} %multiply.14216, bf16[768]{0} %multiply.14219), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14221 = bf16[768]{0} sqrt(bf16[768]{0} %add.14220), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14222 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14223 = bf16[768]{0} add(bf16[768]{0} %sqrt.14221, bf16[768]{0} %broadcast.14222), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14233 = bf16[768]{0} divide(bf16[768]{0} %add.14232, bf16[768]{0} %add.14223), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14234 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14235 = bf16[768]{0} multiply(bf16[768]{0} %divide.14233, bf16[768]{0} %broadcast.14234), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14236 = bf16[768]{0} add(bf16[768]{0} %p152.3265, bf16[768]{0} %multiply.14235), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p472.14258 = bf16[3072,768]{1,0} parameter(472), frontend_attributes={neff_input_names="input472"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14259 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14260 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p472.14258, bf16[3072,768]{1,0} %broadcast.14259), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14239 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14240 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.46, bf16[3072,768]{1,0} %broadcast.14239), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14256 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14257 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14240, bf16[3072,768]{1,0} %broadcast.14256), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14261 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.14260, bf16[3072,768]{0,1} %multiply.14257), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p471.14243 = bf16[3072,768]{1,0} parameter(471), frontend_attributes={neff_input_names="input471"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14244 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14245 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p471.14243, bf16[3072,768]{1,0} %broadcast.14244), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14246 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14240, bf16[3072,768]{0,1} %multiply.14240), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14247 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14248 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14246, bf16[3072,768]{1,0} %broadcast.14247), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14249 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.14245, bf16[3072,768]{0,1} %multiply.14248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14250 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.14249), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14251 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14252 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.14250, bf16[3072,768]{1,0} %broadcast.14251), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14262 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.14261, bf16[3072,768]{1,0} %add.14252), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14263 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14264 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.14262, bf16[3072,768]{1,0} %broadcast.14263), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14265 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p156.3323, bf16[3072,768]{1,0} %multiply.14264), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p474.14287 = bf16[3072]{0} parameter(474), frontend_attributes={neff_input_names="input474"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14288 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14289 = bf16[3072]{0} multiply(bf16[3072]{0} %p474.14287, bf16[3072]{0} %broadcast.14288), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14268 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14269 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.6675, bf16[3072]{0} %broadcast.14268), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14285 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14286 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14269, bf16[3072]{0} %broadcast.14285), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14290 = bf16[3072]{0} add(bf16[3072]{0} %multiply.14289, bf16[3072]{0} %multiply.14286), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p473.14272 = bf16[3072]{0} parameter(473), frontend_attributes={neff_input_names="input473"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14273 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14274 = bf16[3072]{0} multiply(bf16[3072]{0} %p473.14272, bf16[3072]{0} %broadcast.14273), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14275 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14269, bf16[3072]{0} %multiply.14269), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14276 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14277 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14275, bf16[3072]{0} %broadcast.14276), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14278 = bf16[3072]{0} add(bf16[3072]{0} %multiply.14274, bf16[3072]{0} %multiply.14277), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14279 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.14278), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14280 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14281 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.14279, bf16[3072]{0} %broadcast.14280), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14291 = bf16[3072]{0} divide(bf16[3072]{0} %add.14290, bf16[3072]{0} %add.14281), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14292 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14293 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.14291, bf16[3072]{0} %broadcast.14292), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14294 = bf16[3072]{0} add(bf16[3072]{0} %p155.3321, bf16[3072]{0} %multiply.14293), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p476.14316 = bf16[768,3072]{1,0} parameter(476), frontend_attributes={neff_input_names="input476"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14317 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14318 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p476.14316, bf16[768,3072]{1,0} %broadcast.14317), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14297 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14298 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.47, bf16[768,3072]{1,0} %broadcast.14297), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14314 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14315 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14298, bf16[768,3072]{1,0} %broadcast.14314), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14319 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.14318, bf16[768,3072]{0,1} %multiply.14315), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p475.14301 = bf16[768,3072]{1,0} parameter(475), frontend_attributes={neff_input_names="input475"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14302 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14303 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p475.14301, bf16[768,3072]{1,0} %broadcast.14302), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14304 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14298, bf16[768,3072]{0,1} %multiply.14298), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14305 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14306 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14304, bf16[768,3072]{1,0} %broadcast.14305), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14307 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.14303, bf16[768,3072]{0,1} %multiply.14306), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14308 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.14307), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14309 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14310 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.14308, bf16[768,3072]{1,0} %broadcast.14309), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14320 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.14319, bf16[768,3072]{1,0} %add.14310), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14321 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14322 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.14320, bf16[768,3072]{1,0} %broadcast.14321), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14323 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p154.3314, bf16[768,3072]{1,0} %multiply.14322), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p478.14345 = bf16[768]{0} parameter(478), frontend_attributes={neff_input_names="input478"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14346 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14347 = bf16[768]{0} multiply(bf16[768]{0} %p478.14345, bf16[768]{0} %broadcast.14346), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14326 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14327 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6629, bf16[768]{0} %broadcast.14326), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14343 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14344 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14327, bf16[768]{0} %broadcast.14343), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14348 = bf16[768]{0} add(bf16[768]{0} %multiply.14347, bf16[768]{0} %multiply.14344), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p477.14330 = bf16[768]{0} parameter(477), frontend_attributes={neff_input_names="input477"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14331 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14332 = bf16[768]{0} multiply(bf16[768]{0} %p477.14330, bf16[768]{0} %broadcast.14331), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14333 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14327, bf16[768]{0} %multiply.14327), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14334 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14335 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14333, bf16[768]{0} %broadcast.14334), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14336 = bf16[768]{0} add(bf16[768]{0} %multiply.14332, bf16[768]{0} %multiply.14335), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14337 = bf16[768]{0} sqrt(bf16[768]{0} %add.14336), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14338 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14339 = bf16[768]{0} add(bf16[768]{0} %sqrt.14337, bf16[768]{0} %broadcast.14338), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14349 = bf16[768]{0} divide(bf16[768]{0} %add.14348, bf16[768]{0} %add.14339), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14350 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14351 = bf16[768]{0} multiply(bf16[768]{0} %divide.14349, bf16[768]{0} %broadcast.14350), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14352 = bf16[768]{0} add(bf16[768]{0} %p153.3312, bf16[768]{0} %multiply.14351), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p480.14374 = bf16[768]{0} parameter(480), frontend_attributes={neff_input_names="input480"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14375 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14376 = bf16[768]{0} multiply(bf16[768]{0} %p480.14374, bf16[768]{0} %broadcast.14375), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14355 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14356 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6584, bf16[768]{0} %broadcast.14355), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14372 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14373 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14356, bf16[768]{0} %broadcast.14372), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14377 = bf16[768]{0} add(bf16[768]{0} %multiply.14376, bf16[768]{0} %multiply.14373), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p479.14359 = bf16[768]{0} parameter(479), frontend_attributes={neff_input_names="input479"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14360 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14361 = bf16[768]{0} multiply(bf16[768]{0} %p479.14359, bf16[768]{0} %broadcast.14360), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14362 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14356, bf16[768]{0} %multiply.14356), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14363 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14364 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14362, bf16[768]{0} %broadcast.14363), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14365 = bf16[768]{0} add(bf16[768]{0} %multiply.14361, bf16[768]{0} %multiply.14364), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14366 = bf16[768]{0} sqrt(bf16[768]{0} %add.14365), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14367 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14368 = bf16[768]{0} add(bf16[768]{0} %sqrt.14366, bf16[768]{0} %broadcast.14367), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14378 = bf16[768]{0} divide(bf16[768]{0} %add.14377, bf16[768]{0} %add.14368), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14379 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14380 = bf16[768]{0} multiply(bf16[768]{0} %divide.14378, bf16[768]{0} %broadcast.14379), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14381 = bf16[768]{0} add(bf16[768]{0} %p19.421, bf16[768]{0} %multiply.14380), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p482.14403 = bf16[768]{0} parameter(482), frontend_attributes={neff_input_names="input482"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14404 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14405 = bf16[768]{0} multiply(bf16[768]{0} %p482.14403, bf16[768]{0} %broadcast.14404), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14384 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14385 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6559, bf16[768]{0} %broadcast.14384), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14401 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14402 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14385, bf16[768]{0} %broadcast.14401), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14406 = bf16[768]{0} add(bf16[768]{0} %multiply.14405, bf16[768]{0} %multiply.14402), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p481.14388 = bf16[768]{0} parameter(481), frontend_attributes={neff_input_names="input481"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14389 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14390 = bf16[768]{0} multiply(bf16[768]{0} %p481.14388, bf16[768]{0} %broadcast.14389), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14391 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14385, bf16[768]{0} %multiply.14385), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14392 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14393 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14391, bf16[768]{0} %broadcast.14392), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14394 = bf16[768]{0} add(bf16[768]{0} %multiply.14390, bf16[768]{0} %multiply.14393), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14395 = bf16[768]{0} sqrt(bf16[768]{0} %add.14394), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14396 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14397 = bf16[768]{0} add(bf16[768]{0} %sqrt.14395, bf16[768]{0} %broadcast.14396), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14407 = bf16[768]{0} divide(bf16[768]{0} %add.14406, bf16[768]{0} %add.14397), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14408 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14409 = bf16[768]{0} multiply(bf16[768]{0} %divide.14407, bf16[768]{0} %broadcast.14408), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14410 = bf16[768]{0} add(bf16[768]{0} %p157.3358, bf16[768]{0} %multiply.14409), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p484.14432 = bf16[768,768]{1,0} parameter(484), frontend_attributes={neff_input_names="input484"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14433 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14434 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p484.14432, bf16[768,768]{1,0} %broadcast.14433), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14413 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14414 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.48, bf16[768,768]{1,0} %broadcast.14413), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14430 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14431 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14414, bf16[768,768]{1,0} %broadcast.14430), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14435 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14434, bf16[768,768]{0,1} %multiply.14431), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p483.14417 = bf16[768,768]{1,0} parameter(483), frontend_attributes={neff_input_names="input483"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14418 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14419 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p483.14417, bf16[768,768]{1,0} %broadcast.14418), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14420 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14414, bf16[768,768]{0,1} %multiply.14414), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14421 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14422 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14420, bf16[768,768]{1,0} %broadcast.14421), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14423 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14419, bf16[768,768]{0,1} %multiply.14422), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14424 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14423), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14425 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14426 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14424, bf16[768,768]{1,0} %broadcast.14425), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14436 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14435, bf16[768,768]{1,0} %add.14426), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14437 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14438 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14436, bf16[768,768]{1,0} %broadcast.14437), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14439 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p165.3496, bf16[768,768]{1,0} %multiply.14438), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p486.14461 = bf16[768]{0} parameter(486), frontend_attributes={neff_input_names="input486"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14462 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14463 = bf16[768]{0} multiply(bf16[768]{0} %p486.14461, bf16[768]{0} %broadcast.14462), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1081 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.270 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.30, bf16[12,64]{1,0} %broadcast.1081), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1189 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.332 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.270, bf16[12,64]{1,0} %broadcast.1189), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3684 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.332), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14464 = bf16[768]{0} add(bf16[768]{0} %multiply.14463, bf16[768]{0} %reshape.3684), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p485.14446 = bf16[768]{0} parameter(485), frontend_attributes={neff_input_names="input485"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14447 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14448 = bf16[768]{0} multiply(bf16[768]{0} %p485.14446, bf16[768]{0} %broadcast.14447), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.331 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.270, bf16[12,64]{1,0} %multiply.270), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1278 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.379 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.331, bf16[12,64]{1,0} %broadcast.1278), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4023 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.379), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14452 = bf16[768]{0} add(bf16[768]{0} %multiply.14448, bf16[768]{0} %reshape.4023), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14453 = bf16[768]{0} sqrt(bf16[768]{0} %add.14452), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14454 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14455 = bf16[768]{0} add(bf16[768]{0} %sqrt.14453, bf16[768]{0} %broadcast.14454), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14465 = bf16[768]{0} divide(bf16[768]{0} %add.14464, bf16[768]{0} %add.14455), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14466 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14467 = bf16[768]{0} multiply(bf16[768]{0} %divide.14465, bf16[768]{0} %broadcast.14466), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14468 = bf16[768]{0} add(bf16[768]{0} %p164.3494, bf16[768]{0} %multiply.14467), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p488.14490 = bf16[768,768]{1,0} parameter(488), frontend_attributes={neff_input_names="input488"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14491 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14492 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p488.14490, bf16[768,768]{1,0} %broadcast.14491), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14471 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14472 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.49, bf16[768,768]{1,0} %broadcast.14471), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14488 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14489 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14472, bf16[768,768]{1,0} %broadcast.14488), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14493 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14492, bf16[768,768]{0,1} %multiply.14489), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p487.14475 = bf16[768,768]{1,0} parameter(487), frontend_attributes={neff_input_names="input487"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14476 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14477 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p487.14475, bf16[768,768]{1,0} %broadcast.14476), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14478 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14472, bf16[768,768]{0,1} %multiply.14472), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14479 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14480 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14478, bf16[768,768]{1,0} %broadcast.14479), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14481 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14477, bf16[768,768]{0,1} %multiply.14480), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14482 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14481), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14483 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14484 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14482, bf16[768,768]{1,0} %broadcast.14483), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14494 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14493, bf16[768,768]{1,0} %add.14484), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14495 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14496 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14494, bf16[768,768]{1,0} %broadcast.14495), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14497 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p163.3475, bf16[768,768]{1,0} %multiply.14496), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p490.14519 = bf16[768]{0} parameter(490), frontend_attributes={neff_input_names="input490"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14520 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14521 = bf16[768]{0} multiply(bf16[768]{0} %p490.14519, bf16[768]{0} %broadcast.14520), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1085 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.271 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.31, bf16[12,64]{1,0} %broadcast.1085), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1191 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.334 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.271, bf16[12,64]{1,0} %broadcast.1191), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3690 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.334), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14522 = bf16[768]{0} add(bf16[768]{0} %multiply.14521, bf16[768]{0} %reshape.3690), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p489.14504 = bf16[768]{0} parameter(489), frontend_attributes={neff_input_names="input489"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14505 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14506 = bf16[768]{0} multiply(bf16[768]{0} %p489.14504, bf16[768]{0} %broadcast.14505), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.333 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.271, bf16[12,64]{1,0} %multiply.271), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1280 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.380 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.333, bf16[12,64]{1,0} %broadcast.1280), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4026 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.380), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14510 = bf16[768]{0} add(bf16[768]{0} %multiply.14506, bf16[768]{0} %reshape.4026), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14511 = bf16[768]{0} sqrt(bf16[768]{0} %add.14510), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14512 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14513 = bf16[768]{0} add(bf16[768]{0} %sqrt.14511, bf16[768]{0} %broadcast.14512), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14523 = bf16[768]{0} divide(bf16[768]{0} %add.14522, bf16[768]{0} %add.14513), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14524 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14525 = bf16[768]{0} multiply(bf16[768]{0} %divide.14523, bf16[768]{0} %broadcast.14524), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14526 = bf16[768]{0} add(bf16[768]{0} %p162.3473, bf16[768]{0} %multiply.14525), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p492.14548 = bf16[768,768]{1,0} parameter(492), frontend_attributes={neff_input_names="input492"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14549 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14550 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p492.14548, bf16[768,768]{1,0} %broadcast.14549), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14529 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14530 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.50, bf16[768,768]{1,0} %broadcast.14529), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14546 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14547 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14530, bf16[768,768]{1,0} %broadcast.14546), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14551 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14550, bf16[768,768]{0,1} %multiply.14547), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p491.14533 = bf16[768,768]{1,0} parameter(491), frontend_attributes={neff_input_names="input491"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14534 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14535 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p491.14533, bf16[768,768]{1,0} %broadcast.14534), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14536 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14530, bf16[768,768]{0,1} %multiply.14530), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14537 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14538 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14536, bf16[768,768]{1,0} %broadcast.14537), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14539 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14535, bf16[768,768]{0,1} %multiply.14538), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14540 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14539), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14541 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14542 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14540, bf16[768,768]{1,0} %broadcast.14541), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14552 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14551, bf16[768,768]{1,0} %add.14542), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14553 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14554 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14552, bf16[768,768]{1,0} %broadcast.14553), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14555 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p161.3416, bf16[768,768]{1,0} %multiply.14554), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p494.14577 = bf16[768]{0} parameter(494), frontend_attributes={neff_input_names="input494"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14578 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14579 = bf16[768]{0} multiply(bf16[768]{0} %p494.14577, bf16[768]{0} %broadcast.14578), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1087 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.272 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.32, bf16[12,64]{1,0} %broadcast.1087), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1193 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.336 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.272, bf16[12,64]{1,0} %broadcast.1193), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3697 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.336), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14580 = bf16[768]{0} add(bf16[768]{0} %multiply.14579, bf16[768]{0} %reshape.3697), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p493.14562 = bf16[768]{0} parameter(493), frontend_attributes={neff_input_names="input493"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14563 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14564 = bf16[768]{0} multiply(bf16[768]{0} %p493.14562, bf16[768]{0} %broadcast.14563), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.335 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.272, bf16[12,64]{1,0} %multiply.272), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1282 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.381 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.335, bf16[12,64]{1,0} %broadcast.1282), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4031 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.381), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14568 = bf16[768]{0} add(bf16[768]{0} %multiply.14564, bf16[768]{0} %reshape.4031), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14569 = bf16[768]{0} sqrt(bf16[768]{0} %add.14568), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14570 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14571 = bf16[768]{0} add(bf16[768]{0} %sqrt.14569, bf16[768]{0} %broadcast.14570), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14581 = bf16[768]{0} divide(bf16[768]{0} %add.14580, bf16[768]{0} %add.14571), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14582 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14583 = bf16[768]{0} multiply(bf16[768]{0} %divide.14581, bf16[768]{0} %broadcast.14582), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14584 = bf16[768]{0} add(bf16[768]{0} %p160.3414, bf16[768]{0} %multiply.14583), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p496.14606 = bf16[768,768]{1,0} parameter(496), frontend_attributes={neff_input_names="input496"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14607 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14608 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p496.14606, bf16[768,768]{1,0} %broadcast.14607), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14587 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14588 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.51, bf16[768,768]{1,0} %broadcast.14587), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14604 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14605 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14588, bf16[768,768]{1,0} %broadcast.14604), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14609 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14608, bf16[768,768]{0,1} %multiply.14605), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p495.14591 = bf16[768,768]{1,0} parameter(495), frontend_attributes={neff_input_names="input495"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14592 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14593 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p495.14591, bf16[768,768]{1,0} %broadcast.14592), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14594 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14588, bf16[768,768]{0,1} %multiply.14588), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14595 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14596 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14594, bf16[768,768]{1,0} %broadcast.14595), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14597 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14593, bf16[768,768]{0,1} %multiply.14596), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14598 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14597), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14599 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14600 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14598, bf16[768,768]{1,0} %broadcast.14599), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14610 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14609, bf16[768,768]{1,0} %add.14600), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14611 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14612 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14610, bf16[768,768]{1,0} %broadcast.14611), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14613 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p159.3407, bf16[768,768]{1,0} %multiply.14612), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p498.14635 = bf16[768]{0} parameter(498), frontend_attributes={neff_input_names="input498"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14636 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14637 = bf16[768]{0} multiply(bf16[768]{0} %p498.14635, bf16[768]{0} %broadcast.14636), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14616 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14617 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6341, bf16[768]{0} %broadcast.14616), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14633 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14634 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14617, bf16[768]{0} %broadcast.14633), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14638 = bf16[768]{0} add(bf16[768]{0} %multiply.14637, bf16[768]{0} %multiply.14634), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p497.14620 = bf16[768]{0} parameter(497), frontend_attributes={neff_input_names="input497"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14621 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14622 = bf16[768]{0} multiply(bf16[768]{0} %p497.14620, bf16[768]{0} %broadcast.14621), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14623 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14617, bf16[768]{0} %multiply.14617), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14624 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14625 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14623, bf16[768]{0} %broadcast.14624), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14626 = bf16[768]{0} add(bf16[768]{0} %multiply.14622, bf16[768]{0} %multiply.14625), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14627 = bf16[768]{0} sqrt(bf16[768]{0} %add.14626), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14628 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14629 = bf16[768]{0} add(bf16[768]{0} %sqrt.14627, bf16[768]{0} %broadcast.14628), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14639 = bf16[768]{0} divide(bf16[768]{0} %add.14638, bf16[768]{0} %add.14629), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14640 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14641 = bf16[768]{0} multiply(bf16[768]{0} %divide.14639, bf16[768]{0} %broadcast.14640), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14642 = bf16[768]{0} add(bf16[768]{0} %p158.3405, bf16[768]{0} %multiply.14641), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p500.14664 = bf16[768]{0} parameter(500), frontend_attributes={neff_input_names="input500"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14665 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14666 = bf16[768]{0} multiply(bf16[768]{0} %p500.14664, bf16[768]{0} %broadcast.14665), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14645 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14646 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6296, bf16[768]{0} %broadcast.14645), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14662 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14663 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14646, bf16[768]{0} %broadcast.14662), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14667 = bf16[768]{0} add(bf16[768]{0} %multiply.14666, bf16[768]{0} %multiply.14663), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p499.14649 = bf16[768]{0} parameter(499), frontend_attributes={neff_input_names="input499"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14650 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14651 = bf16[768]{0} multiply(bf16[768]{0} %p499.14649, bf16[768]{0} %broadcast.14650), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14652 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14646, bf16[768]{0} %multiply.14646), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14653 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14654 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14652, bf16[768]{0} %broadcast.14653), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14655 = bf16[768]{0} add(bf16[768]{0} %multiply.14651, bf16[768]{0} %multiply.14654), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14656 = bf16[768]{0} sqrt(bf16[768]{0} %add.14655), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14657 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14658 = bf16[768]{0} add(bf16[768]{0} %sqrt.14656, bf16[768]{0} %broadcast.14657), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14668 = bf16[768]{0} divide(bf16[768]{0} %add.14667, bf16[768]{0} %add.14658), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14669 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14670 = bf16[768]{0} multiply(bf16[768]{0} %divide.14668, bf16[768]{0} %broadcast.14669), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14671 = bf16[768]{0} add(bf16[768]{0} %p18.394, bf16[768]{0} %multiply.14670), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p502.14693 = bf16[768]{0} parameter(502), frontend_attributes={neff_input_names="input502"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14694 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14695 = bf16[768]{0} multiply(bf16[768]{0} %p502.14693, bf16[768]{0} %broadcast.14694), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14674 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14675 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6271, bf16[768]{0} %broadcast.14674), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14691 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14692 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14675, bf16[768]{0} %broadcast.14691), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14696 = bf16[768]{0} add(bf16[768]{0} %multiply.14695, bf16[768]{0} %multiply.14692), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p501.14678 = bf16[768]{0} parameter(501), frontend_attributes={neff_input_names="input501"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14679 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14680 = bf16[768]{0} multiply(bf16[768]{0} %p501.14678, bf16[768]{0} %broadcast.14679), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14681 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14675, bf16[768]{0} %multiply.14675), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14682 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14683 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14681, bf16[768]{0} %broadcast.14682), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14684 = bf16[768]{0} add(bf16[768]{0} %multiply.14680, bf16[768]{0} %multiply.14683), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14685 = bf16[768]{0} sqrt(bf16[768]{0} %add.14684), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14686 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14687 = bf16[768]{0} add(bf16[768]{0} %sqrt.14685, bf16[768]{0} %broadcast.14686), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14697 = bf16[768]{0} divide(bf16[768]{0} %add.14696, bf16[768]{0} %add.14687), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14698 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14699 = bf16[768]{0} multiply(bf16[768]{0} %divide.14697, bf16[768]{0} %broadcast.14698), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14700 = bf16[768]{0} add(bf16[768]{0} %p166.3560, bf16[768]{0} %multiply.14699), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p504.14722 = bf16[3072,768]{1,0} parameter(504), frontend_attributes={neff_input_names="input504"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14723 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14724 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p504.14722, bf16[3072,768]{1,0} %broadcast.14723), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14703 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14704 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.52, bf16[3072,768]{1,0} %broadcast.14703), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14720 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14721 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14704, bf16[3072,768]{1,0} %broadcast.14720), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14725 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.14724, bf16[3072,768]{0,1} %multiply.14721), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p503.14707 = bf16[3072,768]{1,0} parameter(503), frontend_attributes={neff_input_names="input503"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14708 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14709 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p503.14707, bf16[3072,768]{1,0} %broadcast.14708), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14710 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14704, bf16[3072,768]{0,1} %multiply.14704), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14711 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14712 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.14710, bf16[3072,768]{1,0} %broadcast.14711), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14713 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.14709, bf16[3072,768]{0,1} %multiply.14712), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14714 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.14713), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14715 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14716 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.14714, bf16[3072,768]{1,0} %broadcast.14715), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14726 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.14725, bf16[3072,768]{1,0} %add.14716), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14727 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14728 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.14726, bf16[3072,768]{1,0} %broadcast.14727), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14729 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p170.3618, bf16[3072,768]{1,0} %multiply.14728), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p506.14751 = bf16[3072]{0} parameter(506), frontend_attributes={neff_input_names="input506"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14752 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14753 = bf16[3072]{0} multiply(bf16[3072]{0} %p506.14751, bf16[3072]{0} %broadcast.14752), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14732 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14733 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.6228, bf16[3072]{0} %broadcast.14732), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14749 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14750 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14733, bf16[3072]{0} %broadcast.14749), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14754 = bf16[3072]{0} add(bf16[3072]{0} %multiply.14753, bf16[3072]{0} %multiply.14750), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p505.14736 = bf16[3072]{0} parameter(505), frontend_attributes={neff_input_names="input505"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14737 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14738 = bf16[3072]{0} multiply(bf16[3072]{0} %p505.14736, bf16[3072]{0} %broadcast.14737), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14739 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14733, bf16[3072]{0} %multiply.14733), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14740 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14741 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.14739, bf16[3072]{0} %broadcast.14740), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14742 = bf16[3072]{0} add(bf16[3072]{0} %multiply.14738, bf16[3072]{0} %multiply.14741), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14743 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.14742), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14744 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14745 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.14743, bf16[3072]{0} %broadcast.14744), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14755 = bf16[3072]{0} divide(bf16[3072]{0} %add.14754, bf16[3072]{0} %add.14745), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14756 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14757 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.14755, bf16[3072]{0} %broadcast.14756), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14758 = bf16[3072]{0} add(bf16[3072]{0} %p169.3616, bf16[3072]{0} %multiply.14757), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p508.14780 = bf16[768,3072]{1,0} parameter(508), frontend_attributes={neff_input_names="input508"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14781 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14782 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p508.14780, bf16[768,3072]{1,0} %broadcast.14781), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14761 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14762 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.53, bf16[768,3072]{1,0} %broadcast.14761), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14778 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14779 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14762, bf16[768,3072]{1,0} %broadcast.14778), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14783 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.14782, bf16[768,3072]{0,1} %multiply.14779), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p507.14765 = bf16[768,3072]{1,0} parameter(507), frontend_attributes={neff_input_names="input507"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14766 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14767 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p507.14765, bf16[768,3072]{1,0} %broadcast.14766), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14768 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14762, bf16[768,3072]{0,1} %multiply.14762), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14769 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14770 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.14768, bf16[768,3072]{1,0} %broadcast.14769), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14771 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.14767, bf16[768,3072]{0,1} %multiply.14770), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14772 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.14771), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14773 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14774 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.14772, bf16[768,3072]{1,0} %broadcast.14773), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14784 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.14783, bf16[768,3072]{1,0} %add.14774), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14785 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14786 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.14784, bf16[768,3072]{1,0} %broadcast.14785), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14787 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p168.3609, bf16[768,3072]{1,0} %multiply.14786), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p510.14809 = bf16[768]{0} parameter(510), frontend_attributes={neff_input_names="input510"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14810 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14811 = bf16[768]{0} multiply(bf16[768]{0} %p510.14809, bf16[768]{0} %broadcast.14810), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14790 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14791 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6182, bf16[768]{0} %broadcast.14790), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14807 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14808 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14791, bf16[768]{0} %broadcast.14807), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14812 = bf16[768]{0} add(bf16[768]{0} %multiply.14811, bf16[768]{0} %multiply.14808), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p509.14794 = bf16[768]{0} parameter(509), frontend_attributes={neff_input_names="input509"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14795 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14796 = bf16[768]{0} multiply(bf16[768]{0} %p509.14794, bf16[768]{0} %broadcast.14795), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14797 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14791, bf16[768]{0} %multiply.14791), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14798 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14799 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14797, bf16[768]{0} %broadcast.14798), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14800 = bf16[768]{0} add(bf16[768]{0} %multiply.14796, bf16[768]{0} %multiply.14799), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14801 = bf16[768]{0} sqrt(bf16[768]{0} %add.14800), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14802 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14803 = bf16[768]{0} add(bf16[768]{0} %sqrt.14801, bf16[768]{0} %broadcast.14802), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14813 = bf16[768]{0} divide(bf16[768]{0} %add.14812, bf16[768]{0} %add.14803), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14814 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14815 = bf16[768]{0} multiply(bf16[768]{0} %divide.14813, bf16[768]{0} %broadcast.14814), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14816 = bf16[768]{0} add(bf16[768]{0} %p167.3607, bf16[768]{0} %multiply.14815), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p512.14838 = bf16[768]{0} parameter(512), frontend_attributes={neff_input_names="input512"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14839 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14840 = bf16[768]{0} multiply(bf16[768]{0} %p512.14838, bf16[768]{0} %broadcast.14839), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14819 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14820 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6137, bf16[768]{0} %broadcast.14819), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14836 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14837 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14820, bf16[768]{0} %broadcast.14836), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14841 = bf16[768]{0} add(bf16[768]{0} %multiply.14840, bf16[768]{0} %multiply.14837), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p511.14823 = bf16[768]{0} parameter(511), frontend_attributes={neff_input_names="input511"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14824 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14825 = bf16[768]{0} multiply(bf16[768]{0} %p511.14823, bf16[768]{0} %broadcast.14824), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14826 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14820, bf16[768]{0} %multiply.14820), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14827 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14828 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14826, bf16[768]{0} %broadcast.14827), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14829 = bf16[768]{0} add(bf16[768]{0} %multiply.14825, bf16[768]{0} %multiply.14828), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14830 = bf16[768]{0} sqrt(bf16[768]{0} %add.14829), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14831 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14832 = bf16[768]{0} add(bf16[768]{0} %sqrt.14830, bf16[768]{0} %broadcast.14831), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14842 = bf16[768]{0} divide(bf16[768]{0} %add.14841, bf16[768]{0} %add.14832), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14843 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14844 = bf16[768]{0} multiply(bf16[768]{0} %divide.14842, bf16[768]{0} %broadcast.14843), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14845 = bf16[768]{0} add(bf16[768]{0} %p17.367, bf16[768]{0} %multiply.14844), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p514.14867 = bf16[768]{0} parameter(514), frontend_attributes={neff_input_names="input514"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14868 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14869 = bf16[768]{0} multiply(bf16[768]{0} %p514.14867, bf16[768]{0} %broadcast.14868), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14848 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14849 = bf16[768]{0} multiply(bf16[768]{0} %reduce.6112, bf16[768]{0} %broadcast.14848), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14865 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14866 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14849, bf16[768]{0} %broadcast.14865), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14870 = bf16[768]{0} add(bf16[768]{0} %multiply.14869, bf16[768]{0} %multiply.14866), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p513.14852 = bf16[768]{0} parameter(513), frontend_attributes={neff_input_names="input513"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14853 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14854 = bf16[768]{0} multiply(bf16[768]{0} %p513.14852, bf16[768]{0} %broadcast.14853), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14855 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14849, bf16[768]{0} %multiply.14849), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14856 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14857 = bf16[768]{0} multiply(bf16[768]{0} %multiply.14855, bf16[768]{0} %broadcast.14856), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14858 = bf16[768]{0} add(bf16[768]{0} %multiply.14854, bf16[768]{0} %multiply.14857), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14859 = bf16[768]{0} sqrt(bf16[768]{0} %add.14858), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14860 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14861 = bf16[768]{0} add(bf16[768]{0} %sqrt.14859, bf16[768]{0} %broadcast.14860), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14871 = bf16[768]{0} divide(bf16[768]{0} %add.14870, bf16[768]{0} %add.14861), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14872 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14873 = bf16[768]{0} multiply(bf16[768]{0} %divide.14871, bf16[768]{0} %broadcast.14872), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14874 = bf16[768]{0} add(bf16[768]{0} %p171.3653, bf16[768]{0} %multiply.14873), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p516.14896 = bf16[768,768]{1,0} parameter(516), frontend_attributes={neff_input_names="input516"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14897 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14898 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p516.14896, bf16[768,768]{1,0} %broadcast.14897), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14877 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14878 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.54, bf16[768,768]{1,0} %broadcast.14877), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14894 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14895 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14878, bf16[768,768]{1,0} %broadcast.14894), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14899 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14898, bf16[768,768]{0,1} %multiply.14895), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p515.14881 = bf16[768,768]{1,0} parameter(515), frontend_attributes={neff_input_names="input515"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14882 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14883 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p515.14881, bf16[768,768]{1,0} %broadcast.14882), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14884 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14878, bf16[768,768]{0,1} %multiply.14878), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14885 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14886 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14884, bf16[768,768]{1,0} %broadcast.14885), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14887 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14883, bf16[768,768]{0,1} %multiply.14886), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14888 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14887), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14889 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14890 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14888, bf16[768,768]{1,0} %broadcast.14889), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14900 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14899, bf16[768,768]{1,0} %add.14890), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14901 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14902 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14900, bf16[768,768]{1,0} %broadcast.14901), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14903 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p179.3791, bf16[768,768]{1,0} %multiply.14902), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p518.14925 = bf16[768]{0} parameter(518), frontend_attributes={neff_input_names="input518"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14926 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14927 = bf16[768]{0} multiply(bf16[768]{0} %p518.14925, bf16[768]{0} %broadcast.14926), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1089 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.273 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.33, bf16[12,64]{1,0} %broadcast.1089), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1195 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.338 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.273, bf16[12,64]{1,0} %broadcast.1195), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3704 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.338), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14928 = bf16[768]{0} add(bf16[768]{0} %multiply.14927, bf16[768]{0} %reshape.3704), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p517.14910 = bf16[768]{0} parameter(517), frontend_attributes={neff_input_names="input517"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14911 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14912 = bf16[768]{0} multiply(bf16[768]{0} %p517.14910, bf16[768]{0} %broadcast.14911), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.337 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.273, bf16[12,64]{1,0} %multiply.273), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1284 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.382 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.337, bf16[12,64]{1,0} %broadcast.1284), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4034 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.382), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14916 = bf16[768]{0} add(bf16[768]{0} %multiply.14912, bf16[768]{0} %reshape.4034), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14917 = bf16[768]{0} sqrt(bf16[768]{0} %add.14916), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14918 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14919 = bf16[768]{0} add(bf16[768]{0} %sqrt.14917, bf16[768]{0} %broadcast.14918), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14929 = bf16[768]{0} divide(bf16[768]{0} %add.14928, bf16[768]{0} %add.14919), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14930 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14931 = bf16[768]{0} multiply(bf16[768]{0} %divide.14929, bf16[768]{0} %broadcast.14930), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14932 = bf16[768]{0} add(bf16[768]{0} %p178.3789, bf16[768]{0} %multiply.14931), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p520.14954 = bf16[768,768]{1,0} parameter(520), frontend_attributes={neff_input_names="input520"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14955 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14956 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p520.14954, bf16[768,768]{1,0} %broadcast.14955), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14935 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14936 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.55, bf16[768,768]{1,0} %broadcast.14935), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14952 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14953 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14936, bf16[768,768]{1,0} %broadcast.14952), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14957 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14956, bf16[768,768]{0,1} %multiply.14953), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p519.14939 = bf16[768,768]{1,0} parameter(519), frontend_attributes={neff_input_names="input519"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14940 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14941 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p519.14939, bf16[768,768]{1,0} %broadcast.14940), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14942 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14936, bf16[768,768]{0,1} %multiply.14936), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14943 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14944 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14942, bf16[768,768]{1,0} %broadcast.14943), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14945 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14941, bf16[768,768]{0,1} %multiply.14944), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14946 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.14945), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14947 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14948 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.14946, bf16[768,768]{1,0} %broadcast.14947), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14958 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.14957, bf16[768,768]{1,0} %add.14948), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14959 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14960 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.14958, bf16[768,768]{1,0} %broadcast.14959), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14961 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p177.3770, bf16[768,768]{1,0} %multiply.14960), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p522.14983 = bf16[768]{0} parameter(522), frontend_attributes={neff_input_names="input522"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14984 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.14985 = bf16[768]{0} multiply(bf16[768]{0} %p522.14983, bf16[768]{0} %broadcast.14984), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1091 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.274 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.34, bf16[12,64]{1,0} %broadcast.1091), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1198 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.340 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.274, bf16[12,64]{1,0} %broadcast.1198), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3712 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.340), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.14986 = bf16[768]{0} add(bf16[768]{0} %multiply.14985, bf16[768]{0} %reshape.3712), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p521.14968 = bf16[768]{0} parameter(521), frontend_attributes={neff_input_names="input521"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14969 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14970 = bf16[768]{0} multiply(bf16[768]{0} %p521.14968, bf16[768]{0} %broadcast.14969), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.339 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.274, bf16[12,64]{1,0} %multiply.274), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1286 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.383 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.339, bf16[12,64]{1,0} %broadcast.1286), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4037 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.383), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.14974 = bf16[768]{0} add(bf16[768]{0} %multiply.14970, bf16[768]{0} %reshape.4037), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.14975 = bf16[768]{0} sqrt(bf16[768]{0} %add.14974), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.14976 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.14977 = bf16[768]{0} add(bf16[768]{0} %sqrt.14975, bf16[768]{0} %broadcast.14976), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.14987 = bf16[768]{0} divide(bf16[768]{0} %add.14986, bf16[768]{0} %add.14977), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.14988 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.14989 = bf16[768]{0} multiply(bf16[768]{0} %divide.14987, bf16[768]{0} %broadcast.14988), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.14990 = bf16[768]{0} add(bf16[768]{0} %p176.3768, bf16[768]{0} %multiply.14989), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p524.15012 = bf16[768,768]{1,0} parameter(524), frontend_attributes={neff_input_names="input524"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15013 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15014 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p524.15012, bf16[768,768]{1,0} %broadcast.15013), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.14993 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.14994 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.56, bf16[768,768]{1,0} %broadcast.14993), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15010 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15011 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14994, bf16[768,768]{1,0} %broadcast.15010), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15015 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15014, bf16[768,768]{0,1} %multiply.15011), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p523.14997 = bf16[768,768]{1,0} parameter(523), frontend_attributes={neff_input_names="input523"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.14998 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.14999 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p523.14997, bf16[768,768]{1,0} %broadcast.14998), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15000 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.14994, bf16[768,768]{0,1} %multiply.14994), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15001 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15002 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15000, bf16[768,768]{1,0} %broadcast.15001), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15003 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.14999, bf16[768,768]{0,1} %multiply.15002), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15004 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15003), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15005 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15006 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15004, bf16[768,768]{1,0} %broadcast.15005), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15016 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15015, bf16[768,768]{1,0} %add.15006), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15017 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15018 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15016, bf16[768,768]{1,0} %broadcast.15017), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15019 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p175.3711, bf16[768,768]{1,0} %multiply.15018), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p526.15041 = bf16[768]{0} parameter(526), frontend_attributes={neff_input_names="input526"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15042 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15043 = bf16[768]{0} multiply(bf16[768]{0} %p526.15041, bf16[768]{0} %broadcast.15042), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1093 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.275 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.35, bf16[12,64]{1,0} %broadcast.1093), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1200 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.342 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.275, bf16[12,64]{1,0} %broadcast.1200), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3725 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.342), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15044 = bf16[768]{0} add(bf16[768]{0} %multiply.15043, bf16[768]{0} %reshape.3725), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p525.15026 = bf16[768]{0} parameter(525), frontend_attributes={neff_input_names="input525"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15027 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15028 = bf16[768]{0} multiply(bf16[768]{0} %p525.15026, bf16[768]{0} %broadcast.15027), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.341 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.275, bf16[12,64]{1,0} %multiply.275), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1288 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.384 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.341, bf16[12,64]{1,0} %broadcast.1288), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4040 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.384), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15032 = bf16[768]{0} add(bf16[768]{0} %multiply.15028, bf16[768]{0} %reshape.4040), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15033 = bf16[768]{0} sqrt(bf16[768]{0} %add.15032), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15034 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15035 = bf16[768]{0} add(bf16[768]{0} %sqrt.15033, bf16[768]{0} %broadcast.15034), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15045 = bf16[768]{0} divide(bf16[768]{0} %add.15044, bf16[768]{0} %add.15035), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15046 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15047 = bf16[768]{0} multiply(bf16[768]{0} %divide.15045, bf16[768]{0} %broadcast.15046), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15048 = bf16[768]{0} add(bf16[768]{0} %p174.3709, bf16[768]{0} %multiply.15047), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p528.15070 = bf16[768,768]{1,0} parameter(528), frontend_attributes={neff_input_names="input528"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15071 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15072 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p528.15070, bf16[768,768]{1,0} %broadcast.15071), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15051 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15052 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.57, bf16[768,768]{1,0} %broadcast.15051), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15068 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15069 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15052, bf16[768,768]{1,0} %broadcast.15068), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15073 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15072, bf16[768,768]{0,1} %multiply.15069), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p527.15055 = bf16[768,768]{1,0} parameter(527), frontend_attributes={neff_input_names="input527"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15056 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15057 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p527.15055, bf16[768,768]{1,0} %broadcast.15056), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15058 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15052, bf16[768,768]{0,1} %multiply.15052), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15059 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15060 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15058, bf16[768,768]{1,0} %broadcast.15059), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15061 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15057, bf16[768,768]{0,1} %multiply.15060), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15062 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15061), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15063 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15064 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15062, bf16[768,768]{1,0} %broadcast.15063), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15074 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15073, bf16[768,768]{1,0} %add.15064), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15075 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15076 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15074, bf16[768,768]{1,0} %broadcast.15075), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15077 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p173.3702, bf16[768,768]{1,0} %multiply.15076), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p530.15099 = bf16[768]{0} parameter(530), frontend_attributes={neff_input_names="input530"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15100 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15101 = bf16[768]{0} multiply(bf16[768]{0} %p530.15099, bf16[768]{0} %broadcast.15100), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15080 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15081 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5894, bf16[768]{0} %broadcast.15080), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15097 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15098 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15081, bf16[768]{0} %broadcast.15097), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15102 = bf16[768]{0} add(bf16[768]{0} %multiply.15101, bf16[768]{0} %multiply.15098), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p529.15084 = bf16[768]{0} parameter(529), frontend_attributes={neff_input_names="input529"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15085 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15086 = bf16[768]{0} multiply(bf16[768]{0} %p529.15084, bf16[768]{0} %broadcast.15085), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15087 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15081, bf16[768]{0} %multiply.15081), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15088 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15089 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15087, bf16[768]{0} %broadcast.15088), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15090 = bf16[768]{0} add(bf16[768]{0} %multiply.15086, bf16[768]{0} %multiply.15089), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15091 = bf16[768]{0} sqrt(bf16[768]{0} %add.15090), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15092 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15093 = bf16[768]{0} add(bf16[768]{0} %sqrt.15091, bf16[768]{0} %broadcast.15092), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15103 = bf16[768]{0} divide(bf16[768]{0} %add.15102, bf16[768]{0} %add.15093), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15104 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15105 = bf16[768]{0} multiply(bf16[768]{0} %divide.15103, bf16[768]{0} %broadcast.15104), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15106 = bf16[768]{0} add(bf16[768]{0} %p172.3700, bf16[768]{0} %multiply.15105), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p532.15128 = bf16[768]{0} parameter(532), frontend_attributes={neff_input_names="input532"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15129 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15130 = bf16[768]{0} multiply(bf16[768]{0} %p532.15128, bf16[768]{0} %broadcast.15129), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15109 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15110 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5849, bf16[768]{0} %broadcast.15109), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15126 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15127 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15110, bf16[768]{0} %broadcast.15126), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15131 = bf16[768]{0} add(bf16[768]{0} %multiply.15130, bf16[768]{0} %multiply.15127), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p531.15113 = bf16[768]{0} parameter(531), frontend_attributes={neff_input_names="input531"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15114 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15115 = bf16[768]{0} multiply(bf16[768]{0} %p531.15113, bf16[768]{0} %broadcast.15114), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15116 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15110, bf16[768]{0} %multiply.15110), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15117 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15118 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15116, bf16[768]{0} %broadcast.15117), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15119 = bf16[768]{0} add(bf16[768]{0} %multiply.15115, bf16[768]{0} %multiply.15118), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15120 = bf16[768]{0} sqrt(bf16[768]{0} %add.15119), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15121 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15122 = bf16[768]{0} add(bf16[768]{0} %sqrt.15120, bf16[768]{0} %broadcast.15121), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15132 = bf16[768]{0} divide(bf16[768]{0} %add.15131, bf16[768]{0} %add.15122), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15133 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15134 = bf16[768]{0} multiply(bf16[768]{0} %divide.15132, bf16[768]{0} %broadcast.15133), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15135 = bf16[768]{0} add(bf16[768]{0} %p16.340, bf16[768]{0} %multiply.15134), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p534.15157 = bf16[768]{0} parameter(534), frontend_attributes={neff_input_names="input534"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15158 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15159 = bf16[768]{0} multiply(bf16[768]{0} %p534.15157, bf16[768]{0} %broadcast.15158), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15138 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15139 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5824, bf16[768]{0} %broadcast.15138), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15155 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15156 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15139, bf16[768]{0} %broadcast.15155), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15160 = bf16[768]{0} add(bf16[768]{0} %multiply.15159, bf16[768]{0} %multiply.15156), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p533.15142 = bf16[768]{0} parameter(533), frontend_attributes={neff_input_names="input533"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15143 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15144 = bf16[768]{0} multiply(bf16[768]{0} %p533.15142, bf16[768]{0} %broadcast.15143), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15145 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15139, bf16[768]{0} %multiply.15139), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15146 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15147 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15145, bf16[768]{0} %broadcast.15146), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15148 = bf16[768]{0} add(bf16[768]{0} %multiply.15144, bf16[768]{0} %multiply.15147), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15149 = bf16[768]{0} sqrt(bf16[768]{0} %add.15148), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15150 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15151 = bf16[768]{0} add(bf16[768]{0} %sqrt.15149, bf16[768]{0} %broadcast.15150), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15161 = bf16[768]{0} divide(bf16[768]{0} %add.15160, bf16[768]{0} %add.15151), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15162 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15163 = bf16[768]{0} multiply(bf16[768]{0} %divide.15161, bf16[768]{0} %broadcast.15162), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15164 = bf16[768]{0} add(bf16[768]{0} %p180.3855, bf16[768]{0} %multiply.15163), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p536.15186 = bf16[3072,768]{1,0} parameter(536), frontend_attributes={neff_input_names="input536"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15187 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15188 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p536.15186, bf16[3072,768]{1,0} %broadcast.15187), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15167 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15168 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.58, bf16[3072,768]{1,0} %broadcast.15167), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15184 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15185 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15168, bf16[3072,768]{1,0} %broadcast.15184), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15189 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.15188, bf16[3072,768]{0,1} %multiply.15185), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p535.15171 = bf16[3072,768]{1,0} parameter(535), frontend_attributes={neff_input_names="input535"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15172 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15173 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p535.15171, bf16[3072,768]{1,0} %broadcast.15172), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15174 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15168, bf16[3072,768]{0,1} %multiply.15168), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15175 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15176 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15174, bf16[3072,768]{1,0} %broadcast.15175), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15177 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.15173, bf16[3072,768]{0,1} %multiply.15176), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15178 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.15177), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15179 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15180 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.15178, bf16[3072,768]{1,0} %broadcast.15179), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15190 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.15189, bf16[3072,768]{1,0} %add.15180), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15191 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15192 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.15190, bf16[3072,768]{1,0} %broadcast.15191), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15193 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p184.3913, bf16[3072,768]{1,0} %multiply.15192), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p538.15215 = bf16[3072]{0} parameter(538), frontend_attributes={neff_input_names="input538"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15216 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15217 = bf16[3072]{0} multiply(bf16[3072]{0} %p538.15215, bf16[3072]{0} %broadcast.15216), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15196 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15197 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.5781, bf16[3072]{0} %broadcast.15196), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15213 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15214 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15197, bf16[3072]{0} %broadcast.15213), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15218 = bf16[3072]{0} add(bf16[3072]{0} %multiply.15217, bf16[3072]{0} %multiply.15214), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p537.15200 = bf16[3072]{0} parameter(537), frontend_attributes={neff_input_names="input537"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15201 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15202 = bf16[3072]{0} multiply(bf16[3072]{0} %p537.15200, bf16[3072]{0} %broadcast.15201), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15203 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15197, bf16[3072]{0} %multiply.15197), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15204 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15205 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15203, bf16[3072]{0} %broadcast.15204), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15206 = bf16[3072]{0} add(bf16[3072]{0} %multiply.15202, bf16[3072]{0} %multiply.15205), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15207 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.15206), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15208 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15209 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.15207, bf16[3072]{0} %broadcast.15208), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15219 = bf16[3072]{0} divide(bf16[3072]{0} %add.15218, bf16[3072]{0} %add.15209), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15220 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15221 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.15219, bf16[3072]{0} %broadcast.15220), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15222 = bf16[3072]{0} add(bf16[3072]{0} %p183.3911, bf16[3072]{0} %multiply.15221), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p540.15244 = bf16[768,3072]{1,0} parameter(540), frontend_attributes={neff_input_names="input540"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15245 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15246 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p540.15244, bf16[768,3072]{1,0} %broadcast.15245), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15225 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15226 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.59, bf16[768,3072]{1,0} %broadcast.15225), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15242 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15243 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15226, bf16[768,3072]{1,0} %broadcast.15242), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15247 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.15246, bf16[768,3072]{0,1} %multiply.15243), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p539.15229 = bf16[768,3072]{1,0} parameter(539), frontend_attributes={neff_input_names="input539"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15230 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15231 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p539.15229, bf16[768,3072]{1,0} %broadcast.15230), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15232 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15226, bf16[768,3072]{0,1} %multiply.15226), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15233 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15234 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15232, bf16[768,3072]{1,0} %broadcast.15233), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15235 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.15231, bf16[768,3072]{0,1} %multiply.15234), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15236 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.15235), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15237 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15238 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.15236, bf16[768,3072]{1,0} %broadcast.15237), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15248 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.15247, bf16[768,3072]{1,0} %add.15238), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15249 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15250 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.15248, bf16[768,3072]{1,0} %broadcast.15249), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15251 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p182.3904, bf16[768,3072]{1,0} %multiply.15250), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p542.15273 = bf16[768]{0} parameter(542), frontend_attributes={neff_input_names="input542"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15274 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15275 = bf16[768]{0} multiply(bf16[768]{0} %p542.15273, bf16[768]{0} %broadcast.15274), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15254 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15255 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5735, bf16[768]{0} %broadcast.15254), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15271 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15272 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15255, bf16[768]{0} %broadcast.15271), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15276 = bf16[768]{0} add(bf16[768]{0} %multiply.15275, bf16[768]{0} %multiply.15272), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p541.15258 = bf16[768]{0} parameter(541), frontend_attributes={neff_input_names="input541"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15259 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15260 = bf16[768]{0} multiply(bf16[768]{0} %p541.15258, bf16[768]{0} %broadcast.15259), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15261 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15255, bf16[768]{0} %multiply.15255), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15262 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15263 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15261, bf16[768]{0} %broadcast.15262), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15264 = bf16[768]{0} add(bf16[768]{0} %multiply.15260, bf16[768]{0} %multiply.15263), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15265 = bf16[768]{0} sqrt(bf16[768]{0} %add.15264), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15266 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15267 = bf16[768]{0} add(bf16[768]{0} %sqrt.15265, bf16[768]{0} %broadcast.15266), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15277 = bf16[768]{0} divide(bf16[768]{0} %add.15276, bf16[768]{0} %add.15267), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15278 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15279 = bf16[768]{0} multiply(bf16[768]{0} %divide.15277, bf16[768]{0} %broadcast.15278), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15280 = bf16[768]{0} add(bf16[768]{0} %p181.3902, bf16[768]{0} %multiply.15279), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p544.15302 = bf16[768]{0} parameter(544), frontend_attributes={neff_input_names="input544"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15303 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15304 = bf16[768]{0} multiply(bf16[768]{0} %p544.15302, bf16[768]{0} %broadcast.15303), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15283 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15284 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5690, bf16[768]{0} %broadcast.15283), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15300 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15301 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15284, bf16[768]{0} %broadcast.15300), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15305 = bf16[768]{0} add(bf16[768]{0} %multiply.15304, bf16[768]{0} %multiply.15301), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p543.15287 = bf16[768]{0} parameter(543), frontend_attributes={neff_input_names="input543"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15288 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15289 = bf16[768]{0} multiply(bf16[768]{0} %p543.15287, bf16[768]{0} %broadcast.15288), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15290 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15284, bf16[768]{0} %multiply.15284), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15291 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15292 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15290, bf16[768]{0} %broadcast.15291), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15293 = bf16[768]{0} add(bf16[768]{0} %multiply.15289, bf16[768]{0} %multiply.15292), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15294 = bf16[768]{0} sqrt(bf16[768]{0} %add.15293), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15295 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15296 = bf16[768]{0} add(bf16[768]{0} %sqrt.15294, bf16[768]{0} %broadcast.15295), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15306 = bf16[768]{0} divide(bf16[768]{0} %add.15305, bf16[768]{0} %add.15296), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15307 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15308 = bf16[768]{0} multiply(bf16[768]{0} %divide.15306, bf16[768]{0} %broadcast.15307), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15309 = bf16[768]{0} add(bf16[768]{0} %p15.313, bf16[768]{0} %multiply.15308), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p546.15331 = bf16[768]{0} parameter(546), frontend_attributes={neff_input_names="input546"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15332 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15333 = bf16[768]{0} multiply(bf16[768]{0} %p546.15331, bf16[768]{0} %broadcast.15332), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15312 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15313 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5665, bf16[768]{0} %broadcast.15312), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15329 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15330 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15313, bf16[768]{0} %broadcast.15329), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15334 = bf16[768]{0} add(bf16[768]{0} %multiply.15333, bf16[768]{0} %multiply.15330), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p545.15316 = bf16[768]{0} parameter(545), frontend_attributes={neff_input_names="input545"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15317 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15318 = bf16[768]{0} multiply(bf16[768]{0} %p545.15316, bf16[768]{0} %broadcast.15317), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15319 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15313, bf16[768]{0} %multiply.15313), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15320 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15321 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15319, bf16[768]{0} %broadcast.15320), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15322 = bf16[768]{0} add(bf16[768]{0} %multiply.15318, bf16[768]{0} %multiply.15321), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15323 = bf16[768]{0} sqrt(bf16[768]{0} %add.15322), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15324 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15325 = bf16[768]{0} add(bf16[768]{0} %sqrt.15323, bf16[768]{0} %broadcast.15324), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15335 = bf16[768]{0} divide(bf16[768]{0} %add.15334, bf16[768]{0} %add.15325), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15336 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15337 = bf16[768]{0} multiply(bf16[768]{0} %divide.15335, bf16[768]{0} %broadcast.15336), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15338 = bf16[768]{0} add(bf16[768]{0} %p185.3948, bf16[768]{0} %multiply.15337), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p548.15360 = bf16[768,768]{1,0} parameter(548), frontend_attributes={neff_input_names="input548"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15361 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15362 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p548.15360, bf16[768,768]{1,0} %broadcast.15361), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15341 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15342 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.60, bf16[768,768]{1,0} %broadcast.15341), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15358 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15359 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15342, bf16[768,768]{1,0} %broadcast.15358), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15363 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15362, bf16[768,768]{0,1} %multiply.15359), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p547.15345 = bf16[768,768]{1,0} parameter(547), frontend_attributes={neff_input_names="input547"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15346 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15347 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p547.15345, bf16[768,768]{1,0} %broadcast.15346), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15348 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15342, bf16[768,768]{0,1} %multiply.15342), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15349 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15350 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15348, bf16[768,768]{1,0} %broadcast.15349), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15351 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15347, bf16[768,768]{0,1} %multiply.15350), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15352 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15351), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15353 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15354 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15352, bf16[768,768]{1,0} %broadcast.15353), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15364 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15363, bf16[768,768]{1,0} %add.15354), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15365 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15366 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15364, bf16[768,768]{1,0} %broadcast.15365), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15367 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p193.4086, bf16[768,768]{1,0} %multiply.15366), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p550.15389 = bf16[768]{0} parameter(550), frontend_attributes={neff_input_names="input550"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15390 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15391 = bf16[768]{0} multiply(bf16[768]{0} %p550.15389, bf16[768]{0} %broadcast.15390), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1097 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.276 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.36, bf16[12,64]{1,0} %broadcast.1097), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1203 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.344 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.276, bf16[12,64]{1,0} %broadcast.1203), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3732 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.344), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15392 = bf16[768]{0} add(bf16[768]{0} %multiply.15391, bf16[768]{0} %reshape.3732), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p549.15374 = bf16[768]{0} parameter(549), frontend_attributes={neff_input_names="input549"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15375 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15376 = bf16[768]{0} multiply(bf16[768]{0} %p549.15374, bf16[768]{0} %broadcast.15375), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.343 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.276, bf16[12,64]{1,0} %multiply.276), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1291 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.385 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.343, bf16[12,64]{1,0} %broadcast.1291), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4043 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.385), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15380 = bf16[768]{0} add(bf16[768]{0} %multiply.15376, bf16[768]{0} %reshape.4043), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15381 = bf16[768]{0} sqrt(bf16[768]{0} %add.15380), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15382 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15383 = bf16[768]{0} add(bf16[768]{0} %sqrt.15381, bf16[768]{0} %broadcast.15382), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15393 = bf16[768]{0} divide(bf16[768]{0} %add.15392, bf16[768]{0} %add.15383), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15394 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15395 = bf16[768]{0} multiply(bf16[768]{0} %divide.15393, bf16[768]{0} %broadcast.15394), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15396 = bf16[768]{0} add(bf16[768]{0} %p192.4084, bf16[768]{0} %multiply.15395), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p552.15418 = bf16[768,768]{1,0} parameter(552), frontend_attributes={neff_input_names="input552"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15419 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15420 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p552.15418, bf16[768,768]{1,0} %broadcast.15419), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15399 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15400 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.61, bf16[768,768]{1,0} %broadcast.15399), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15416 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15417 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15400, bf16[768,768]{1,0} %broadcast.15416), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15421 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15420, bf16[768,768]{0,1} %multiply.15417), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p551.15403 = bf16[768,768]{1,0} parameter(551), frontend_attributes={neff_input_names="input551"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15404 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15405 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p551.15403, bf16[768,768]{1,0} %broadcast.15404), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15406 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15400, bf16[768,768]{0,1} %multiply.15400), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15407 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15408 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15406, bf16[768,768]{1,0} %broadcast.15407), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15409 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15405, bf16[768,768]{0,1} %multiply.15408), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15410 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15409), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15411 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15412 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15410, bf16[768,768]{1,0} %broadcast.15411), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15422 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15421, bf16[768,768]{1,0} %add.15412), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15423 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15424 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15422, bf16[768,768]{1,0} %broadcast.15423), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15425 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p191.4065, bf16[768,768]{1,0} %multiply.15424), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p554.15447 = bf16[768]{0} parameter(554), frontend_attributes={neff_input_names="input554"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15448 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15449 = bf16[768]{0} multiply(bf16[768]{0} %p554.15447, bf16[768]{0} %broadcast.15448), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1099 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.277 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.37, bf16[12,64]{1,0} %broadcast.1099), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1207 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.346 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.277, bf16[12,64]{1,0} %broadcast.1207), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3740 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.346), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15450 = bf16[768]{0} add(bf16[768]{0} %multiply.15449, bf16[768]{0} %reshape.3740), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p553.15432 = bf16[768]{0} parameter(553), frontend_attributes={neff_input_names="input553"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15433 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15434 = bf16[768]{0} multiply(bf16[768]{0} %p553.15432, bf16[768]{0} %broadcast.15433), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.345 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.277, bf16[12,64]{1,0} %multiply.277), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1293 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.386 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.345, bf16[12,64]{1,0} %broadcast.1293), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4046 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.386), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15438 = bf16[768]{0} add(bf16[768]{0} %multiply.15434, bf16[768]{0} %reshape.4046), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15439 = bf16[768]{0} sqrt(bf16[768]{0} %add.15438), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15440 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15441 = bf16[768]{0} add(bf16[768]{0} %sqrt.15439, bf16[768]{0} %broadcast.15440), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15451 = bf16[768]{0} divide(bf16[768]{0} %add.15450, bf16[768]{0} %add.15441), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15452 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15453 = bf16[768]{0} multiply(bf16[768]{0} %divide.15451, bf16[768]{0} %broadcast.15452), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15454 = bf16[768]{0} add(bf16[768]{0} %p190.4063, bf16[768]{0} %multiply.15453), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p556.15476 = bf16[768,768]{1,0} parameter(556), frontend_attributes={neff_input_names="input556"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15477 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15478 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p556.15476, bf16[768,768]{1,0} %broadcast.15477), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15457 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15458 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.62, bf16[768,768]{1,0} %broadcast.15457), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15474 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15475 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15458, bf16[768,768]{1,0} %broadcast.15474), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15479 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15478, bf16[768,768]{0,1} %multiply.15475), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p555.15461 = bf16[768,768]{1,0} parameter(555), frontend_attributes={neff_input_names="input555"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15462 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15463 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p555.15461, bf16[768,768]{1,0} %broadcast.15462), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15464 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15458, bf16[768,768]{0,1} %multiply.15458), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15465 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15466 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15464, bf16[768,768]{1,0} %broadcast.15465), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15467 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15463, bf16[768,768]{0,1} %multiply.15466), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15468 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15467), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15469 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15470 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15468, bf16[768,768]{1,0} %broadcast.15469), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15480 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15479, bf16[768,768]{1,0} %add.15470), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15481 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15482 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15480, bf16[768,768]{1,0} %broadcast.15481), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15483 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p189.4006, bf16[768,768]{1,0} %multiply.15482), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p558.15505 = bf16[768]{0} parameter(558), frontend_attributes={neff_input_names="input558"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15506 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15507 = bf16[768]{0} multiply(bf16[768]{0} %p558.15505, bf16[768]{0} %broadcast.15506), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1103 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.278 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.38, bf16[12,64]{1,0} %broadcast.1103), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1209 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.348 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.278, bf16[12,64]{1,0} %broadcast.1209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3746 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.348), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15508 = bf16[768]{0} add(bf16[768]{0} %multiply.15507, bf16[768]{0} %reshape.3746), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p557.15490 = bf16[768]{0} parameter(557), frontend_attributes={neff_input_names="input557"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15491 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15492 = bf16[768]{0} multiply(bf16[768]{0} %p557.15490, bf16[768]{0} %broadcast.15491), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.347 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.278, bf16[12,64]{1,0} %multiply.278), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1296 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.387 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.347, bf16[12,64]{1,0} %broadcast.1296), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4049 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.387), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15496 = bf16[768]{0} add(bf16[768]{0} %multiply.15492, bf16[768]{0} %reshape.4049), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15497 = bf16[768]{0} sqrt(bf16[768]{0} %add.15496), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15498 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15499 = bf16[768]{0} add(bf16[768]{0} %sqrt.15497, bf16[768]{0} %broadcast.15498), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15509 = bf16[768]{0} divide(bf16[768]{0} %add.15508, bf16[768]{0} %add.15499), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15510 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15511 = bf16[768]{0} multiply(bf16[768]{0} %divide.15509, bf16[768]{0} %broadcast.15510), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15512 = bf16[768]{0} add(bf16[768]{0} %p188.4004, bf16[768]{0} %multiply.15511), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p560.15534 = bf16[768,768]{1,0} parameter(560), frontend_attributes={neff_input_names="input560"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15535 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15536 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p560.15534, bf16[768,768]{1,0} %broadcast.15535), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15515 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15516 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.63, bf16[768,768]{1,0} %broadcast.15515), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15532 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15533 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15516, bf16[768,768]{1,0} %broadcast.15532), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15537 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15536, bf16[768,768]{0,1} %multiply.15533), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p559.15519 = bf16[768,768]{1,0} parameter(559), frontend_attributes={neff_input_names="input559"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15520 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15521 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p559.15519, bf16[768,768]{1,0} %broadcast.15520), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15522 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15516, bf16[768,768]{0,1} %multiply.15516), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15523 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15524 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15522, bf16[768,768]{1,0} %broadcast.15523), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15525 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15521, bf16[768,768]{0,1} %multiply.15524), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15526 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15525), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15527 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15528 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15526, bf16[768,768]{1,0} %broadcast.15527), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15538 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15537, bf16[768,768]{1,0} %add.15528), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15539 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15540 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15538, bf16[768,768]{1,0} %broadcast.15539), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15541 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p187.3997, bf16[768,768]{1,0} %multiply.15540), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p562.15563 = bf16[768]{0} parameter(562), frontend_attributes={neff_input_names="input562"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15564 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15565 = bf16[768]{0} multiply(bf16[768]{0} %p562.15563, bf16[768]{0} %broadcast.15564), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15544 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15545 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5447, bf16[768]{0} %broadcast.15544), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15561 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15562 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15545, bf16[768]{0} %broadcast.15561), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15566 = bf16[768]{0} add(bf16[768]{0} %multiply.15565, bf16[768]{0} %multiply.15562), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p561.15548 = bf16[768]{0} parameter(561), frontend_attributes={neff_input_names="input561"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15549 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15550 = bf16[768]{0} multiply(bf16[768]{0} %p561.15548, bf16[768]{0} %broadcast.15549), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15551 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15545, bf16[768]{0} %multiply.15545), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15552 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15553 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15551, bf16[768]{0} %broadcast.15552), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15554 = bf16[768]{0} add(bf16[768]{0} %multiply.15550, bf16[768]{0} %multiply.15553), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15555 = bf16[768]{0} sqrt(bf16[768]{0} %add.15554), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15556 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15557 = bf16[768]{0} add(bf16[768]{0} %sqrt.15555, bf16[768]{0} %broadcast.15556), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15567 = bf16[768]{0} divide(bf16[768]{0} %add.15566, bf16[768]{0} %add.15557), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15568 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15569 = bf16[768]{0} multiply(bf16[768]{0} %divide.15567, bf16[768]{0} %broadcast.15568), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15570 = bf16[768]{0} add(bf16[768]{0} %p186.3995, bf16[768]{0} %multiply.15569), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p564.15592 = bf16[768]{0} parameter(564), frontend_attributes={neff_input_names="input564"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15593 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15594 = bf16[768]{0} multiply(bf16[768]{0} %p564.15592, bf16[768]{0} %broadcast.15593), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15573 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15574 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5402, bf16[768]{0} %broadcast.15573), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15590 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15591 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15574, bf16[768]{0} %broadcast.15590), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15595 = bf16[768]{0} add(bf16[768]{0} %multiply.15594, bf16[768]{0} %multiply.15591), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p563.15577 = bf16[768]{0} parameter(563), frontend_attributes={neff_input_names="input563"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15578 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15579 = bf16[768]{0} multiply(bf16[768]{0} %p563.15577, bf16[768]{0} %broadcast.15578), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15580 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15574, bf16[768]{0} %multiply.15574), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15581 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15582 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15580, bf16[768]{0} %broadcast.15581), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15583 = bf16[768]{0} add(bf16[768]{0} %multiply.15579, bf16[768]{0} %multiply.15582), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15584 = bf16[768]{0} sqrt(bf16[768]{0} %add.15583), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15585 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15586 = bf16[768]{0} add(bf16[768]{0} %sqrt.15584, bf16[768]{0} %broadcast.15585), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15596 = bf16[768]{0} divide(bf16[768]{0} %add.15595, bf16[768]{0} %add.15586), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15597 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15598 = bf16[768]{0} multiply(bf16[768]{0} %divide.15596, bf16[768]{0} %broadcast.15597), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15599 = bf16[768]{0} add(bf16[768]{0} %p14.286, bf16[768]{0} %multiply.15598), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p566.15621 = bf16[768]{0} parameter(566), frontend_attributes={neff_input_names="input566"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15622 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15623 = bf16[768]{0} multiply(bf16[768]{0} %p566.15621, bf16[768]{0} %broadcast.15622), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15602 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15603 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5377, bf16[768]{0} %broadcast.15602), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15619 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15620 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15603, bf16[768]{0} %broadcast.15619), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15624 = bf16[768]{0} add(bf16[768]{0} %multiply.15623, bf16[768]{0} %multiply.15620), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p565.15606 = bf16[768]{0} parameter(565), frontend_attributes={neff_input_names="input565"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15607 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15608 = bf16[768]{0} multiply(bf16[768]{0} %p565.15606, bf16[768]{0} %broadcast.15607), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15609 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15603, bf16[768]{0} %multiply.15603), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15610 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15611 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15609, bf16[768]{0} %broadcast.15610), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15612 = bf16[768]{0} add(bf16[768]{0} %multiply.15608, bf16[768]{0} %multiply.15611), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15613 = bf16[768]{0} sqrt(bf16[768]{0} %add.15612), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15614 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15615 = bf16[768]{0} add(bf16[768]{0} %sqrt.15613, bf16[768]{0} %broadcast.15614), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15625 = bf16[768]{0} divide(bf16[768]{0} %add.15624, bf16[768]{0} %add.15615), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15626 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15627 = bf16[768]{0} multiply(bf16[768]{0} %divide.15625, bf16[768]{0} %broadcast.15626), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15628 = bf16[768]{0} add(bf16[768]{0} %p194.4150, bf16[768]{0} %multiply.15627), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p568.15650 = bf16[3072,768]{1,0} parameter(568), frontend_attributes={neff_input_names="input568"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15651 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15652 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p568.15650, bf16[3072,768]{1,0} %broadcast.15651), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15631 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15632 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.64, bf16[3072,768]{1,0} %broadcast.15631), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15648 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15649 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15632, bf16[3072,768]{1,0} %broadcast.15648), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15653 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.15652, bf16[3072,768]{0,1} %multiply.15649), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p567.15635 = bf16[3072,768]{1,0} parameter(567), frontend_attributes={neff_input_names="input567"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15636 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15637 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p567.15635, bf16[3072,768]{1,0} %broadcast.15636), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15638 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15632, bf16[3072,768]{0,1} %multiply.15632), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15639 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15640 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.15638, bf16[3072,768]{1,0} %broadcast.15639), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15641 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.15637, bf16[3072,768]{0,1} %multiply.15640), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15642 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.15641), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15643 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15644 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.15642, bf16[3072,768]{1,0} %broadcast.15643), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15654 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.15653, bf16[3072,768]{1,0} %add.15644), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15655 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15656 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.15654, bf16[3072,768]{1,0} %broadcast.15655), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15657 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p198.4208, bf16[3072,768]{1,0} %multiply.15656), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p570.15679 = bf16[3072]{0} parameter(570), frontend_attributes={neff_input_names="input570"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15680 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15681 = bf16[3072]{0} multiply(bf16[3072]{0} %p570.15679, bf16[3072]{0} %broadcast.15680), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15660 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15661 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.5334, bf16[3072]{0} %broadcast.15660), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15677 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15678 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15661, bf16[3072]{0} %broadcast.15677), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15682 = bf16[3072]{0} add(bf16[3072]{0} %multiply.15681, bf16[3072]{0} %multiply.15678), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p569.15664 = bf16[3072]{0} parameter(569), frontend_attributes={neff_input_names="input569"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15665 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15666 = bf16[3072]{0} multiply(bf16[3072]{0} %p569.15664, bf16[3072]{0} %broadcast.15665), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15667 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15661, bf16[3072]{0} %multiply.15661), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15668 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15669 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.15667, bf16[3072]{0} %broadcast.15668), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15670 = bf16[3072]{0} add(bf16[3072]{0} %multiply.15666, bf16[3072]{0} %multiply.15669), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15671 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.15670), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15672 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15673 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.15671, bf16[3072]{0} %broadcast.15672), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15683 = bf16[3072]{0} divide(bf16[3072]{0} %add.15682, bf16[3072]{0} %add.15673), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15684 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15685 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.15683, bf16[3072]{0} %broadcast.15684), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15686 = bf16[3072]{0} add(bf16[3072]{0} %p197.4206, bf16[3072]{0} %multiply.15685), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p572.15708 = bf16[768,3072]{1,0} parameter(572), frontend_attributes={neff_input_names="input572"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15709 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15710 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p572.15708, bf16[768,3072]{1,0} %broadcast.15709), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15689 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15690 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.65, bf16[768,3072]{1,0} %broadcast.15689), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15706 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15707 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15690, bf16[768,3072]{1,0} %broadcast.15706), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15711 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.15710, bf16[768,3072]{0,1} %multiply.15707), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p571.15693 = bf16[768,3072]{1,0} parameter(571), frontend_attributes={neff_input_names="input571"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15694 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15695 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p571.15693, bf16[768,3072]{1,0} %broadcast.15694), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15696 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15690, bf16[768,3072]{0,1} %multiply.15690), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15697 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15698 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.15696, bf16[768,3072]{1,0} %broadcast.15697), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15699 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.15695, bf16[768,3072]{0,1} %multiply.15698), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15700 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.15699), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15701 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15702 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.15700, bf16[768,3072]{1,0} %broadcast.15701), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15712 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.15711, bf16[768,3072]{1,0} %add.15702), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15713 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15714 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.15712, bf16[768,3072]{1,0} %broadcast.15713), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15715 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p196.4199, bf16[768,3072]{1,0} %multiply.15714), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p574.15737 = bf16[768]{0} parameter(574), frontend_attributes={neff_input_names="input574"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15738 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15739 = bf16[768]{0} multiply(bf16[768]{0} %p574.15737, bf16[768]{0} %broadcast.15738), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15718 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15719 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5288, bf16[768]{0} %broadcast.15718), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15735 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15736 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15719, bf16[768]{0} %broadcast.15735), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15740 = bf16[768]{0} add(bf16[768]{0} %multiply.15739, bf16[768]{0} %multiply.15736), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p573.15722 = bf16[768]{0} parameter(573), frontend_attributes={neff_input_names="input573"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15723 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15724 = bf16[768]{0} multiply(bf16[768]{0} %p573.15722, bf16[768]{0} %broadcast.15723), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15725 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15719, bf16[768]{0} %multiply.15719), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15726 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15727 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15725, bf16[768]{0} %broadcast.15726), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15728 = bf16[768]{0} add(bf16[768]{0} %multiply.15724, bf16[768]{0} %multiply.15727), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15729 = bf16[768]{0} sqrt(bf16[768]{0} %add.15728), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15730 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15731 = bf16[768]{0} add(bf16[768]{0} %sqrt.15729, bf16[768]{0} %broadcast.15730), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15741 = bf16[768]{0} divide(bf16[768]{0} %add.15740, bf16[768]{0} %add.15731), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15742 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15743 = bf16[768]{0} multiply(bf16[768]{0} %divide.15741, bf16[768]{0} %broadcast.15742), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15744 = bf16[768]{0} add(bf16[768]{0} %p195.4197, bf16[768]{0} %multiply.15743), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p576.15766 = bf16[768]{0} parameter(576), frontend_attributes={neff_input_names="input576"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15767 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15768 = bf16[768]{0} multiply(bf16[768]{0} %p576.15766, bf16[768]{0} %broadcast.15767), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15747 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15748 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5243, bf16[768]{0} %broadcast.15747), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15764 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15765 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15748, bf16[768]{0} %broadcast.15764), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15769 = bf16[768]{0} add(bf16[768]{0} %multiply.15768, bf16[768]{0} %multiply.15765), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p575.15751 = bf16[768]{0} parameter(575), frontend_attributes={neff_input_names="input575"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15752 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15753 = bf16[768]{0} multiply(bf16[768]{0} %p575.15751, bf16[768]{0} %broadcast.15752), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15754 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15748, bf16[768]{0} %multiply.15748), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15755 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15756 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15754, bf16[768]{0} %broadcast.15755), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15757 = bf16[768]{0} add(bf16[768]{0} %multiply.15753, bf16[768]{0} %multiply.15756), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15758 = bf16[768]{0} sqrt(bf16[768]{0} %add.15757), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15759 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15760 = bf16[768]{0} add(bf16[768]{0} %sqrt.15758, bf16[768]{0} %broadcast.15759), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15770 = bf16[768]{0} divide(bf16[768]{0} %add.15769, bf16[768]{0} %add.15760), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15771 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15772 = bf16[768]{0} multiply(bf16[768]{0} %divide.15770, bf16[768]{0} %broadcast.15771), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15773 = bf16[768]{0} add(bf16[768]{0} %p13.259, bf16[768]{0} %multiply.15772), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p578.15795 = bf16[768]{0} parameter(578), frontend_attributes={neff_input_names="input578"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15796 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15797 = bf16[768]{0} multiply(bf16[768]{0} %p578.15795, bf16[768]{0} %broadcast.15796), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15776 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15777 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5218, bf16[768]{0} %broadcast.15776), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15793 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15794 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15777, bf16[768]{0} %broadcast.15793), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15798 = bf16[768]{0} add(bf16[768]{0} %multiply.15797, bf16[768]{0} %multiply.15794), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p577.15780 = bf16[768]{0} parameter(577), frontend_attributes={neff_input_names="input577"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15781 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15782 = bf16[768]{0} multiply(bf16[768]{0} %p577.15780, bf16[768]{0} %broadcast.15781), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15783 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15777, bf16[768]{0} %multiply.15777), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15784 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15785 = bf16[768]{0} multiply(bf16[768]{0} %multiply.15783, bf16[768]{0} %broadcast.15784), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15786 = bf16[768]{0} add(bf16[768]{0} %multiply.15782, bf16[768]{0} %multiply.15785), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15787 = bf16[768]{0} sqrt(bf16[768]{0} %add.15786), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15788 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15789 = bf16[768]{0} add(bf16[768]{0} %sqrt.15787, bf16[768]{0} %broadcast.15788), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15799 = bf16[768]{0} divide(bf16[768]{0} %add.15798, bf16[768]{0} %add.15789), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15800 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15801 = bf16[768]{0} multiply(bf16[768]{0} %divide.15799, bf16[768]{0} %broadcast.15800), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15802 = bf16[768]{0} add(bf16[768]{0} %p199.4243, bf16[768]{0} %multiply.15801), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p580.15824 = bf16[768,768]{1,0} parameter(580), frontend_attributes={neff_input_names="input580"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15825 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15826 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p580.15824, bf16[768,768]{1,0} %broadcast.15825), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15805 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15806 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.66, bf16[768,768]{1,0} %broadcast.15805), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15822 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15823 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15806, bf16[768,768]{1,0} %broadcast.15822), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15827 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15826, bf16[768,768]{0,1} %multiply.15823), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p579.15809 = bf16[768,768]{1,0} parameter(579), frontend_attributes={neff_input_names="input579"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15810 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15811 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p579.15809, bf16[768,768]{1,0} %broadcast.15810), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15812 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15806, bf16[768,768]{0,1} %multiply.15806), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15813 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15814 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15812, bf16[768,768]{1,0} %broadcast.15813), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15815 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15811, bf16[768,768]{0,1} %multiply.15814), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15816 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15815), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15817 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15818 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15816, bf16[768,768]{1,0} %broadcast.15817), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15828 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15827, bf16[768,768]{1,0} %add.15818), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15829 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15830 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15828, bf16[768,768]{1,0} %broadcast.15829), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15831 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p207.4381, bf16[768,768]{1,0} %multiply.15830), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p582.15853 = bf16[768]{0} parameter(582), frontend_attributes={neff_input_names="input582"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15854 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15855 = bf16[768]{0} multiply(bf16[768]{0} %p582.15853, bf16[768]{0} %broadcast.15854), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1106 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.279 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.39, bf16[12,64]{1,0} %broadcast.1106), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1213 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.350 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.279, bf16[12,64]{1,0} %broadcast.1213), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3752 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.350), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15856 = bf16[768]{0} add(bf16[768]{0} %multiply.15855, bf16[768]{0} %reshape.3752), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p581.15838 = bf16[768]{0} parameter(581), frontend_attributes={neff_input_names="input581"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15839 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15840 = bf16[768]{0} multiply(bf16[768]{0} %p581.15838, bf16[768]{0} %broadcast.15839), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.349 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.279, bf16[12,64]{1,0} %multiply.279), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1300 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.388 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.349, bf16[12,64]{1,0} %broadcast.1300), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4052 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.388), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15844 = bf16[768]{0} add(bf16[768]{0} %multiply.15840, bf16[768]{0} %reshape.4052), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15845 = bf16[768]{0} sqrt(bf16[768]{0} %add.15844), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15846 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15847 = bf16[768]{0} add(bf16[768]{0} %sqrt.15845, bf16[768]{0} %broadcast.15846), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15857 = bf16[768]{0} divide(bf16[768]{0} %add.15856, bf16[768]{0} %add.15847), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15858 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15859 = bf16[768]{0} multiply(bf16[768]{0} %divide.15857, bf16[768]{0} %broadcast.15858), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15860 = bf16[768]{0} add(bf16[768]{0} %p206.4379, bf16[768]{0} %multiply.15859), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p584.15882 = bf16[768,768]{1,0} parameter(584), frontend_attributes={neff_input_names="input584"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15883 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15884 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p584.15882, bf16[768,768]{1,0} %broadcast.15883), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15863 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15864 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.67, bf16[768,768]{1,0} %broadcast.15863), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15880 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15881 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15864, bf16[768,768]{1,0} %broadcast.15880), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15885 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15884, bf16[768,768]{0,1} %multiply.15881), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p583.15867 = bf16[768,768]{1,0} parameter(583), frontend_attributes={neff_input_names="input583"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15868 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15869 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p583.15867, bf16[768,768]{1,0} %broadcast.15868), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15870 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15864, bf16[768,768]{0,1} %multiply.15864), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15871 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15872 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15870, bf16[768,768]{1,0} %broadcast.15871), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15873 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15869, bf16[768,768]{0,1} %multiply.15872), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15874 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15873), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15875 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15876 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15874, bf16[768,768]{1,0} %broadcast.15875), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15886 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15885, bf16[768,768]{1,0} %add.15876), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15887 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15888 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15886, bf16[768,768]{1,0} %broadcast.15887), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15889 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p205.4360, bf16[768,768]{1,0} %multiply.15888), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p586.15911 = bf16[768]{0} parameter(586), frontend_attributes={neff_input_names="input586"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15912 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15913 = bf16[768]{0} multiply(bf16[768]{0} %p586.15911, bf16[768]{0} %broadcast.15912), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1108 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.280 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.40, bf16[12,64]{1,0} %broadcast.1108), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1215 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.352 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.280, bf16[12,64]{1,0} %broadcast.1215), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3759 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.352), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15914 = bf16[768]{0} add(bf16[768]{0} %multiply.15913, bf16[768]{0} %reshape.3759), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p585.15896 = bf16[768]{0} parameter(585), frontend_attributes={neff_input_names="input585"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15897 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15898 = bf16[768]{0} multiply(bf16[768]{0} %p585.15896, bf16[768]{0} %broadcast.15897), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.351 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.280, bf16[12,64]{1,0} %multiply.280), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1302 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.389 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.351, bf16[12,64]{1,0} %broadcast.1302), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4057 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.389), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15902 = bf16[768]{0} add(bf16[768]{0} %multiply.15898, bf16[768]{0} %reshape.4057), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15903 = bf16[768]{0} sqrt(bf16[768]{0} %add.15902), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15904 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15905 = bf16[768]{0} add(bf16[768]{0} %sqrt.15903, bf16[768]{0} %broadcast.15904), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15915 = bf16[768]{0} divide(bf16[768]{0} %add.15914, bf16[768]{0} %add.15905), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15916 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15917 = bf16[768]{0} multiply(bf16[768]{0} %divide.15915, bf16[768]{0} %broadcast.15916), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15918 = bf16[768]{0} add(bf16[768]{0} %p204.4358, bf16[768]{0} %multiply.15917), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p588.15940 = bf16[768,768]{1,0} parameter(588), frontend_attributes={neff_input_names="input588"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15941 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15942 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p588.15940, bf16[768,768]{1,0} %broadcast.15941), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15921 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15922 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.68, bf16[768,768]{1,0} %broadcast.15921), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15938 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15939 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15922, bf16[768,768]{1,0} %broadcast.15938), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15943 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15942, bf16[768,768]{0,1} %multiply.15939), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p587.15925 = bf16[768,768]{1,0} parameter(587), frontend_attributes={neff_input_names="input587"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15926 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15927 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p587.15925, bf16[768,768]{1,0} %broadcast.15926), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15928 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15922, bf16[768,768]{0,1} %multiply.15922), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15929 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15930 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15928, bf16[768,768]{1,0} %broadcast.15929), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15931 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15927, bf16[768,768]{0,1} %multiply.15930), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15932 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15931), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15933 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15934 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15932, bf16[768,768]{1,0} %broadcast.15933), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15944 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.15943, bf16[768,768]{1,0} %add.15934), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15945 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15946 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.15944, bf16[768,768]{1,0} %broadcast.15945), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15947 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p203.4301, bf16[768,768]{1,0} %multiply.15946), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p590.15969 = bf16[768]{0} parameter(590), frontend_attributes={neff_input_names="input590"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15970 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15971 = bf16[768]{0} multiply(bf16[768]{0} %p590.15969, bf16[768]{0} %broadcast.15970), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.1111 = bf16[12,64]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.281 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %reduce.41, bf16[12,64]{1,0} %broadcast.1111), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1217 = bf16[12,64]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.354 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.281, bf16[12,64]{1,0} %broadcast.1217), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %reshape.3768 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.354), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.15972 = bf16[768]{0} add(bf16[768]{0} %multiply.15971, bf16[768]{0} %reshape.3768), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p589.15954 = bf16[768]{0} parameter(589), frontend_attributes={neff_input_names="input589"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15955 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15956 = bf16[768]{0} multiply(bf16[768]{0} %p589.15954, bf16[768]{0} %broadcast.15955), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.353 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.281, bf16[12,64]{1,0} %multiply.281), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.1306 = bf16[12,64]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.390 = bf16[12,64]{1,0} multiply(bf16[12,64]{1,0} %multiply.353, bf16[12,64]{1,0} %broadcast.1306), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %reshape.4062 = bf16[768]{0} reshape(bf16[12,64]{1,0} %multiply.390), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15960 = bf16[768]{0} add(bf16[768]{0} %multiply.15956, bf16[768]{0} %reshape.4062), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15961 = bf16[768]{0} sqrt(bf16[768]{0} %add.15960), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15962 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15963 = bf16[768]{0} add(bf16[768]{0} %sqrt.15961, bf16[768]{0} %broadcast.15962), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.15973 = bf16[768]{0} divide(bf16[768]{0} %add.15972, bf16[768]{0} %add.15963), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.15974 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.15975 = bf16[768]{0} multiply(bf16[768]{0} %divide.15973, bf16[768]{0} %broadcast.15974), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.15976 = bf16[768]{0} add(bf16[768]{0} %p202.4299, bf16[768]{0} %multiply.15975), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p592.15998 = bf16[768,768]{1,0} parameter(592), frontend_attributes={neff_input_names="input592"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15999 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16000 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p592.15998, bf16[768,768]{1,0} %broadcast.15999), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.15979 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.15980 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.69, bf16[768,768]{1,0} %broadcast.15979), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15996 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.15997 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15980, bf16[768,768]{1,0} %broadcast.15996), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16001 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.16000, bf16[768,768]{0,1} %multiply.15997), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p591.15983 = bf16[768,768]{1,0} parameter(591), frontend_attributes={neff_input_names="input591"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15984 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15985 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p591.15983, bf16[768,768]{1,0} %broadcast.15984), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15986 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15980, bf16[768,768]{0,1} %multiply.15980), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.15987 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.15988 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.15986, bf16[768,768]{1,0} %broadcast.15987), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.15989 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.15985, bf16[768,768]{0,1} %multiply.15988), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.15990 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.15989), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.15991 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.15992 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.15990, bf16[768,768]{1,0} %broadcast.15991), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16002 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.16001, bf16[768,768]{1,0} %add.15992), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16003 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16004 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.16002, bf16[768,768]{1,0} %broadcast.16003), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16005 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p201.4292, bf16[768,768]{1,0} %multiply.16004), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p594.16027 = bf16[768]{0} parameter(594), frontend_attributes={neff_input_names="input594"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16028 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16029 = bf16[768]{0} multiply(bf16[768]{0} %p594.16027, bf16[768]{0} %broadcast.16028), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16008 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16009 = bf16[768]{0} multiply(bf16[768]{0} %reduce.5000, bf16[768]{0} %broadcast.16008), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16025 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16026 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16009, bf16[768]{0} %broadcast.16025), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16030 = bf16[768]{0} add(bf16[768]{0} %multiply.16029, bf16[768]{0} %multiply.16026), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p593.16012 = bf16[768]{0} parameter(593), frontend_attributes={neff_input_names="input593"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16013 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16014 = bf16[768]{0} multiply(bf16[768]{0} %p593.16012, bf16[768]{0} %broadcast.16013), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16015 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16009, bf16[768]{0} %multiply.16009), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16016 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16017 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16015, bf16[768]{0} %broadcast.16016), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16018 = bf16[768]{0} add(bf16[768]{0} %multiply.16014, bf16[768]{0} %multiply.16017), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16019 = bf16[768]{0} sqrt(bf16[768]{0} %add.16018), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16020 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16021 = bf16[768]{0} add(bf16[768]{0} %sqrt.16019, bf16[768]{0} %broadcast.16020), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16031 = bf16[768]{0} divide(bf16[768]{0} %add.16030, bf16[768]{0} %add.16021), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16032 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16033 = bf16[768]{0} multiply(bf16[768]{0} %divide.16031, bf16[768]{0} %broadcast.16032), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16034 = bf16[768]{0} add(bf16[768]{0} %p200.4290, bf16[768]{0} %multiply.16033), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p596.16056 = bf16[768]{0} parameter(596), frontend_attributes={neff_input_names="input596"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16057 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16058 = bf16[768]{0} multiply(bf16[768]{0} %p596.16056, bf16[768]{0} %broadcast.16057), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16037 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16038 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4955, bf16[768]{0} %broadcast.16037), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16054 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16055 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16038, bf16[768]{0} %broadcast.16054), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16059 = bf16[768]{0} add(bf16[768]{0} %multiply.16058, bf16[768]{0} %multiply.16055), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p595.16041 = bf16[768]{0} parameter(595), frontend_attributes={neff_input_names="input595"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16042 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16043 = bf16[768]{0} multiply(bf16[768]{0} %p595.16041, bf16[768]{0} %broadcast.16042), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16044 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16038, bf16[768]{0} %multiply.16038), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16045 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16046 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16044, bf16[768]{0} %broadcast.16045), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16047 = bf16[768]{0} add(bf16[768]{0} %multiply.16043, bf16[768]{0} %multiply.16046), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16048 = bf16[768]{0} sqrt(bf16[768]{0} %add.16047), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16049 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16050 = bf16[768]{0} add(bf16[768]{0} %sqrt.16048, bf16[768]{0} %broadcast.16049), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16060 = bf16[768]{0} divide(bf16[768]{0} %add.16059, bf16[768]{0} %add.16050), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16061 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16062 = bf16[768]{0} multiply(bf16[768]{0} %divide.16060, bf16[768]{0} %broadcast.16061), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16063 = bf16[768]{0} add(bf16[768]{0} %p12.232, bf16[768]{0} %multiply.16062), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p598.16085 = bf16[768]{0} parameter(598), frontend_attributes={neff_input_names="input598"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16086 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16087 = bf16[768]{0} multiply(bf16[768]{0} %p598.16085, bf16[768]{0} %broadcast.16086), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16066 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16067 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4930, bf16[768]{0} %broadcast.16066), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16083 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16084 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16067, bf16[768]{0} %broadcast.16083), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16088 = bf16[768]{0} add(bf16[768]{0} %multiply.16087, bf16[768]{0} %multiply.16084), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p597.16070 = bf16[768]{0} parameter(597), frontend_attributes={neff_input_names="input597"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16071 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16072 = bf16[768]{0} multiply(bf16[768]{0} %p597.16070, bf16[768]{0} %broadcast.16071), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16073 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16067, bf16[768]{0} %multiply.16067), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16074 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16075 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16073, bf16[768]{0} %broadcast.16074), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16076 = bf16[768]{0} add(bf16[768]{0} %multiply.16072, bf16[768]{0} %multiply.16075), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16077 = bf16[768]{0} sqrt(bf16[768]{0} %add.16076), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16078 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16079 = bf16[768]{0} add(bf16[768]{0} %sqrt.16077, bf16[768]{0} %broadcast.16078), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16089 = bf16[768]{0} divide(bf16[768]{0} %add.16088, bf16[768]{0} %add.16079), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16090 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16091 = bf16[768]{0} multiply(bf16[768]{0} %divide.16089, bf16[768]{0} %broadcast.16090), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16092 = bf16[768]{0} add(bf16[768]{0} %p208.4445, bf16[768]{0} %multiply.16091), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p600.16114 = bf16[3072,768]{1,0} parameter(600), frontend_attributes={neff_input_names="input600"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16115 = bf16[3072,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16116 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p600.16114, bf16[3072,768]{1,0} %broadcast.16115), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16095 = bf16[3072,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16096 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %dot.70, bf16[3072,768]{1,0} %broadcast.16095), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16112 = bf16[3072,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16113 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.16096, bf16[3072,768]{1,0} %broadcast.16112), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16117 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.16116, bf16[3072,768]{0,1} %multiply.16113), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p599.16099 = bf16[3072,768]{1,0} parameter(599), frontend_attributes={neff_input_names="input599"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16100 = bf16[3072,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16101 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %p599.16099, bf16[3072,768]{1,0} %broadcast.16100), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16102 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.16096, bf16[3072,768]{0,1} %multiply.16096), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16103 = bf16[3072,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16104 = bf16[3072,768]{0,1} multiply(bf16[3072,768]{0,1} %multiply.16102, bf16[3072,768]{1,0} %broadcast.16103), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16105 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %multiply.16101, bf16[3072,768]{0,1} %multiply.16104), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16106 = bf16[3072,768]{1,0} sqrt(bf16[3072,768]{1,0} %add.16105), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16107 = bf16[3072,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16108 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %sqrt.16106, bf16[3072,768]{1,0} %broadcast.16107), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16118 = bf16[3072,768]{1,0} divide(bf16[3072,768]{1,0} %add.16117, bf16[3072,768]{1,0} %add.16108), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16119 = bf16[3072,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16120 = bf16[3072,768]{1,0} multiply(bf16[3072,768]{1,0} %divide.16118, bf16[3072,768]{1,0} %broadcast.16119), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16121 = bf16[3072,768]{1,0} add(bf16[3072,768]{1,0} %p212.4503, bf16[3072,768]{1,0} %multiply.16120), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p602.16143 = bf16[3072]{0} parameter(602), frontend_attributes={neff_input_names="input602"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16144 = bf16[3072]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16145 = bf16[3072]{0} multiply(bf16[3072]{0} %p602.16143, bf16[3072]{0} %broadcast.16144), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16124 = bf16[3072]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16125 = bf16[3072]{0} multiply(bf16[3072]{0} %reduce.4887, bf16[3072]{0} %broadcast.16124), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16141 = bf16[3072]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16142 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.16125, bf16[3072]{0} %broadcast.16141), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16146 = bf16[3072]{0} add(bf16[3072]{0} %multiply.16145, bf16[3072]{0} %multiply.16142), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p601.16128 = bf16[3072]{0} parameter(601), frontend_attributes={neff_input_names="input601"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16129 = bf16[3072]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16130 = bf16[3072]{0} multiply(bf16[3072]{0} %p601.16128, bf16[3072]{0} %broadcast.16129), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16131 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.16125, bf16[3072]{0} %multiply.16125), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16132 = bf16[3072]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16133 = bf16[3072]{0} multiply(bf16[3072]{0} %multiply.16131, bf16[3072]{0} %broadcast.16132), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16134 = bf16[3072]{0} add(bf16[3072]{0} %multiply.16130, bf16[3072]{0} %multiply.16133), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16135 = bf16[3072]{0} sqrt(bf16[3072]{0} %add.16134), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16136 = bf16[3072]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16137 = bf16[3072]{0} add(bf16[3072]{0} %sqrt.16135, bf16[3072]{0} %broadcast.16136), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16147 = bf16[3072]{0} divide(bf16[3072]{0} %add.16146, bf16[3072]{0} %add.16137), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16148 = bf16[3072]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16149 = bf16[3072]{0} multiply(bf16[3072]{0} %divide.16147, bf16[3072]{0} %broadcast.16148), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16150 = bf16[3072]{0} add(bf16[3072]{0} %p211.4501, bf16[3072]{0} %multiply.16149), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p604.16172 = bf16[768,3072]{1,0} parameter(604), frontend_attributes={neff_input_names="input604"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16173 = bf16[768,3072]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16174 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p604.16172, bf16[768,3072]{1,0} %broadcast.16173), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16153 = bf16[768,3072]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16154 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %dot.71, bf16[768,3072]{1,0} %broadcast.16153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16170 = bf16[768,3072]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16171 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.16154, bf16[768,3072]{1,0} %broadcast.16170), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16175 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.16174, bf16[768,3072]{0,1} %multiply.16171), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p603.16157 = bf16[768,3072]{1,0} parameter(603), frontend_attributes={neff_input_names="input603"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16158 = bf16[768,3072]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16159 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %p603.16157, bf16[768,3072]{1,0} %broadcast.16158), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16160 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.16154, bf16[768,3072]{0,1} %multiply.16154), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16161 = bf16[768,3072]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16162 = bf16[768,3072]{0,1} multiply(bf16[768,3072]{0,1} %multiply.16160, bf16[768,3072]{1,0} %broadcast.16161), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16163 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %multiply.16159, bf16[768,3072]{0,1} %multiply.16162), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16164 = bf16[768,3072]{1,0} sqrt(bf16[768,3072]{1,0} %add.16163), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16165 = bf16[768,3072]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16166 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %sqrt.16164, bf16[768,3072]{1,0} %broadcast.16165), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16176 = bf16[768,3072]{1,0} divide(bf16[768,3072]{1,0} %add.16175, bf16[768,3072]{1,0} %add.16166), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16177 = bf16[768,3072]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16178 = bf16[768,3072]{1,0} multiply(bf16[768,3072]{1,0} %divide.16176, bf16[768,3072]{1,0} %broadcast.16177), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16179 = bf16[768,3072]{1,0} add(bf16[768,3072]{1,0} %p210.4494, bf16[768,3072]{1,0} %multiply.16178), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p606.16201 = bf16[768]{0} parameter(606), frontend_attributes={neff_input_names="input606"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16202 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16203 = bf16[768]{0} multiply(bf16[768]{0} %p606.16201, bf16[768]{0} %broadcast.16202), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16182 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16183 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4841, bf16[768]{0} %broadcast.16182), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16199 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16200 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16183, bf16[768]{0} %broadcast.16199), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16204 = bf16[768]{0} add(bf16[768]{0} %multiply.16203, bf16[768]{0} %multiply.16200), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p605.16186 = bf16[768]{0} parameter(605), frontend_attributes={neff_input_names="input605"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16187 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16188 = bf16[768]{0} multiply(bf16[768]{0} %p605.16186, bf16[768]{0} %broadcast.16187), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16189 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16183, bf16[768]{0} %multiply.16183), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16190 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16191 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16189, bf16[768]{0} %broadcast.16190), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16192 = bf16[768]{0} add(bf16[768]{0} %multiply.16188, bf16[768]{0} %multiply.16191), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16193 = bf16[768]{0} sqrt(bf16[768]{0} %add.16192), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16194 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16195 = bf16[768]{0} add(bf16[768]{0} %sqrt.16193, bf16[768]{0} %broadcast.16194), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16205 = bf16[768]{0} divide(bf16[768]{0} %add.16204, bf16[768]{0} %add.16195), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16206 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16207 = bf16[768]{0} multiply(bf16[768]{0} %divide.16205, bf16[768]{0} %broadcast.16206), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16208 = bf16[768]{0} add(bf16[768]{0} %p209.4492, bf16[768]{0} %multiply.16207), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p608.16230 = bf16[768]{0} parameter(608), frontend_attributes={neff_input_names="input608"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16231 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16232 = bf16[768]{0} multiply(bf16[768]{0} %p608.16230, bf16[768]{0} %broadcast.16231), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16211 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16212 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4796, bf16[768]{0} %broadcast.16211), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16228 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16229 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16212, bf16[768]{0} %broadcast.16228), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16233 = bf16[768]{0} add(bf16[768]{0} %multiply.16232, bf16[768]{0} %multiply.16229), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p607.16215 = bf16[768]{0} parameter(607), frontend_attributes={neff_input_names="input607"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16216 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16217 = bf16[768]{0} multiply(bf16[768]{0} %p607.16215, bf16[768]{0} %broadcast.16216), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16218 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16212, bf16[768]{0} %multiply.16212), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16219 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16220 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16218, bf16[768]{0} %broadcast.16219), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16221 = bf16[768]{0} add(bf16[768]{0} %multiply.16217, bf16[768]{0} %multiply.16220), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16222 = bf16[768]{0} sqrt(bf16[768]{0} %add.16221), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16223 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16224 = bf16[768]{0} add(bf16[768]{0} %sqrt.16222, bf16[768]{0} %broadcast.16223), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16234 = bf16[768]{0} divide(bf16[768]{0} %add.16233, bf16[768]{0} %add.16224), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16235 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16236 = bf16[768]{0} multiply(bf16[768]{0} %divide.16234, bf16[768]{0} %broadcast.16235), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16237 = bf16[768]{0} add(bf16[768]{0} %p11.205, bf16[768]{0} %multiply.16236), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p610.16259 = bf16[768]{0} parameter(610), frontend_attributes={neff_input_names="input610"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16260 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16261 = bf16[768]{0} multiply(bf16[768]{0} %p610.16259, bf16[768]{0} %broadcast.16260), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16240 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16241 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4771, bf16[768]{0} %broadcast.16240), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16257 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16258 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16241, bf16[768]{0} %broadcast.16257), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16262 = bf16[768]{0} add(bf16[768]{0} %multiply.16261, bf16[768]{0} %multiply.16258), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p609.16244 = bf16[768]{0} parameter(609), frontend_attributes={neff_input_names="input609"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16245 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16246 = bf16[768]{0} multiply(bf16[768]{0} %p609.16244, bf16[768]{0} %broadcast.16245), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16247 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16241, bf16[768]{0} %multiply.16241), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16248 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16249 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16247, bf16[768]{0} %broadcast.16248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16250 = bf16[768]{0} add(bf16[768]{0} %multiply.16246, bf16[768]{0} %multiply.16249), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16251 = bf16[768]{0} sqrt(bf16[768]{0} %add.16250), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16252 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16253 = bf16[768]{0} add(bf16[768]{0} %sqrt.16251, bf16[768]{0} %broadcast.16252), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16263 = bf16[768]{0} divide(bf16[768]{0} %add.16262, bf16[768]{0} %add.16253), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16264 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16265 = bf16[768]{0} multiply(bf16[768]{0} %divide.16263, bf16[768]{0} %broadcast.16264), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16266 = bf16[768]{0} add(bf16[768]{0} %p213.4538, bf16[768]{0} %multiply.16265), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p612.16288 = bf16[768,768]{1,0} parameter(612), frontend_attributes={neff_input_names="input612"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16289 = bf16[768,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16290 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p612.16288, bf16[768,768]{1,0} %broadcast.16289), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16269 = bf16[768,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16270 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %dot.72, bf16[768,768]{1,0} %broadcast.16269), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16286 = bf16[768,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16287 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.16270, bf16[768,768]{1,0} %broadcast.16286), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16291 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.16290, bf16[768,768]{0,1} %multiply.16287), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p611.16273 = bf16[768,768]{1,0} parameter(611), frontend_attributes={neff_input_names="input611"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16274 = bf16[768,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16275 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %p611.16273, bf16[768,768]{1,0} %broadcast.16274), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16276 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.16270, bf16[768,768]{0,1} %multiply.16270), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16277 = bf16[768,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16278 = bf16[768,768]{0,1} multiply(bf16[768,768]{0,1} %multiply.16276, bf16[768,768]{1,0} %broadcast.16277), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16279 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %multiply.16275, bf16[768,768]{0,1} %multiply.16278), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16280 = bf16[768,768]{1,0} sqrt(bf16[768,768]{1,0} %add.16279), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16281 = bf16[768,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16282 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %sqrt.16280, bf16[768,768]{1,0} %broadcast.16281), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16292 = bf16[768,768]{1,0} divide(bf16[768,768]{1,0} %add.16291, bf16[768,768]{1,0} %add.16282), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16293 = bf16[768,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16294 = bf16[768,768]{1,0} multiply(bf16[768,768]{1,0} %divide.16292, bf16[768,768]{1,0} %broadcast.16293), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16295 = bf16[768,768]{1,0} add(bf16[768,768]{1,0} %p10.202, bf16[768,768]{1,0} %multiply.16294), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p614.16317 = bf16[768]{0} parameter(614), frontend_attributes={neff_input_names="input614"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16318 = bf16[768]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16319 = bf16[768]{0} multiply(bf16[768]{0} %p614.16317, bf16[768]{0} %broadcast.16318), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16298 = bf16[768]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16299 = bf16[768]{0} multiply(bf16[768]{0} %reduce.4720, bf16[768]{0} %broadcast.16298), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16315 = bf16[768]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16316 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16299, bf16[768]{0} %broadcast.16315), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16320 = bf16[768]{0} add(bf16[768]{0} %multiply.16319, bf16[768]{0} %multiply.16316), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p613.16302 = bf16[768]{0} parameter(613), frontend_attributes={neff_input_names="input613"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16303 = bf16[768]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16304 = bf16[768]{0} multiply(bf16[768]{0} %p613.16302, bf16[768]{0} %broadcast.16303), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16305 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16299, bf16[768]{0} %multiply.16299), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16306 = bf16[768]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16307 = bf16[768]{0} multiply(bf16[768]{0} %multiply.16305, bf16[768]{0} %broadcast.16306), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16308 = bf16[768]{0} add(bf16[768]{0} %multiply.16304, bf16[768]{0} %multiply.16307), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16309 = bf16[768]{0} sqrt(bf16[768]{0} %add.16308), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16310 = bf16[768]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16311 = bf16[768]{0} add(bf16[768]{0} %sqrt.16309, bf16[768]{0} %broadcast.16310), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16321 = bf16[768]{0} divide(bf16[768]{0} %add.16320, bf16[768]{0} %add.16311), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16322 = bf16[768]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16323 = bf16[768]{0} multiply(bf16[768]{0} %divide.16321, bf16[768]{0} %broadcast.16322), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16324 = bf16[768]{0} add(bf16[768]{0} %p9.201, bf16[768]{0} %multiply.16323), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p616.16346 = bf16[2,768]{1,0} parameter(616), frontend_attributes={neff_input_names="input616"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16347 = bf16[2,768]{1,0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16348 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %p616.16346, bf16[2,768]{1,0} %broadcast.16347), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16327 = bf16[2,768]{1,0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16328 = bf16[2,768]{0,1} multiply(bf16[2,768]{0,1} %dot.73, bf16[2,768]{1,0} %broadcast.16327), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16344 = bf16[2,768]{1,0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16345 = bf16[2,768]{0,1} multiply(bf16[2,768]{0,1} %multiply.16328, bf16[2,768]{1,0} %broadcast.16344), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16349 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %multiply.16348, bf16[2,768]{0,1} %multiply.16345), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p615.16331 = bf16[2,768]{1,0} parameter(615), frontend_attributes={neff_input_names="input615"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16332 = bf16[2,768]{1,0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16333 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %p615.16331, bf16[2,768]{1,0} %broadcast.16332), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16334 = bf16[2,768]{0,1} multiply(bf16[2,768]{0,1} %multiply.16328, bf16[2,768]{0,1} %multiply.16328), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16335 = bf16[2,768]{1,0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16336 = bf16[2,768]{0,1} multiply(bf16[2,768]{0,1} %multiply.16334, bf16[2,768]{1,0} %broadcast.16335), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16337 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %multiply.16333, bf16[2,768]{0,1} %multiply.16336), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16338 = bf16[2,768]{1,0} sqrt(bf16[2,768]{1,0} %add.16337), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16339 = bf16[2,768]{1,0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16340 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %sqrt.16338, bf16[2,768]{1,0} %broadcast.16339), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16350 = bf16[2,768]{1,0} divide(bf16[2,768]{1,0} %add.16349, bf16[2,768]{1,0} %add.16340), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16351 = bf16[2,768]{1,0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16352 = bf16[2,768]{1,0} multiply(bf16[2,768]{1,0} %divide.16350, bf16[2,768]{1,0} %broadcast.16351), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16353 = bf16[2,768]{1,0} add(bf16[2,768]{1,0} %p6.12, bf16[2,768]{1,0} %multiply.16352), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p618.16375 = bf16[2]{0} parameter(618), frontend_attributes={neff_input_names="input618"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16376 = bf16[2]{0} broadcast(bf16[] %p7.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16377 = bf16[2]{0} multiply(bf16[2]{0} %p618.16375, bf16[2]{0} %broadcast.16376), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %broadcast.16356 = bf16[2]{0} broadcast(bf16[] %select.10559), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %multiply.16357 = bf16[2]{0} multiply(bf16[2]{0} %reduce.4667, bf16[2]{0} %broadcast.16356), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16373 = bf16[2]{0} broadcast(bf16[] %p217.10573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %multiply.16374 = bf16[2]{0} multiply(bf16[2]{0} %multiply.16357, bf16[2]{0} %broadcast.16373), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %add.16378 = bf16[2]{0} add(bf16[2]{0} %multiply.16377, bf16[2]{0} %multiply.16374), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=445}
  %p617.16360 = bf16[2]{0} parameter(617), frontend_attributes={neff_input_names="input617"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16361 = bf16[2]{0} broadcast(bf16[] %p215.10562), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16362 = bf16[2]{0} multiply(bf16[2]{0} %p617.16360, bf16[2]{0} %broadcast.16361), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16363 = bf16[2]{0} multiply(bf16[2]{0} %multiply.16357, bf16[2]{0} %multiply.16357), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %broadcast.16364 = bf16[2]{0} broadcast(bf16[] %p2.5), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %multiply.16365 = bf16[2]{0} multiply(bf16[2]{0} %multiply.16363, bf16[2]{0} %broadcast.16364), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %add.16366 = bf16[2]{0} add(bf16[2]{0} %multiply.16362, bf16[2]{0} %multiply.16365), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=446}
  %sqrt.16367 = bf16[2]{0} sqrt(bf16[2]{0} %add.16366), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %broadcast.16368 = bf16[2]{0} broadcast(bf16[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %add.16369 = bf16[2]{0} add(bf16[2]{0} %sqrt.16367, bf16[2]{0} %broadcast.16368), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=447}
  %divide.16379 = bf16[2]{0} divide(bf16[2]{0} %add.16378, bf16[2]{0} %add.16369), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %broadcast.16380 = bf16[2]{0} broadcast(bf16[] %p0.1), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %multiply.16381 = bf16[2]{0} multiply(bf16[2]{0} %divide.16379, bf16[2]{0} %broadcast.16380), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %add.16382 = bf16[2]{0} add(bf16[2]{0} %p5.11, bf16[2]{0} %multiply.16381), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/optimization.py" source_line=455}
  %p619.16385 = bf16[] parameter(619), frontend_attributes={neff_input_names="input619"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/trainer.py" source_line=1912}
  %broadcast.3 = s64[4,2]{1,0} broadcast(s64[4]{0} %p4.10), dimensions={0}
  %iota.5 = s64[4,2]{1,0} iota(), iota_dimension=1
  %compare.0 = pred[4,2]{1,0} compare(s64[4,2]{1,0} %broadcast.3, s64[4,2]{1,0} %iota.5), direction=EQ
  %constant.3 = bf16[] constant(1)
  %broadcast.6 = bf16[4,2]{1,0} broadcast(bf16[] %constant.3), dimensions={}
  %constant.4 = bf16[] constant(0)
  %broadcast.7 = bf16[4,2]{1,0} broadcast(bf16[] %constant.4), dimensions={}
  %select.0 = bf16[4,2]{1,0} select(pred[4,2]{1,0} %compare.0, bf16[4,2]{1,0} %broadcast.6, bf16[4,2]{1,0} %broadcast.7)
  %multiply.14 = bf16[4,2]{1,0} multiply(bf16[4,2]{1,0} %subtract.1, bf16[4,2]{1,0} %select.0)
  %reduce.2 = bf16[4]{0} reduce(bf16[4,2]{1,0} %multiply.14, bf16[] %constant.1), dimensions={1}, to_apply=%SimpleCrossEntropyLossForwardAdd.4571
  %constant.93 = bf16[] constant(0)
  %broadcast.139 = bf16[4]{0} broadcast(bf16[] %constant.93), dimensions={}
  %select.4 = bf16[4]{0} select(pred[4]{0} %compare.1, bf16[4]{0} %reduce.2, bf16[4]{0} %broadcast.139)
  %reduce.3 = bf16[] reduce(bf16[4]{0} %select.4, bf16[] %constant.1), dimensions={0}, to_apply=%SimpleCrossEntropyLossForwardAdd.4575
  %reduce.4 = bf16[] reduce(bf16[4]{0} %convert.0, bf16[] %constant.1), dimensions={0}, to_apply=%SimpleCrossEntropyLossForwardAdd.4579
  %divide.0 = bf16[] divide(bf16[] %reduce.3, bf16[] %reduce.4)
  %negate.0 = bf16[] negate(bf16[] %divide.0), metadata={op_type="xla___op_SimpleCrossEntropyLossForwardImpl" op_name="xla___op_SimpleCrossEntropyLossForwardImpl" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/torch_xla/core/xla_op_registry.py" source_line=44}
  %add.16386 = bf16[] add(bf16[] %p619.16385, bf16[] %negate.0), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/SemanticEquivalenceFramework/training_experiments/neuronx_distributed_venv/lib/python3.8/site-packages/transformers/trainer.py" source_line=1912}
  ROOT %tuple.16387 = (bf16[28996,768]{1,0}, bf16[512,768]{1,0}, bf16[2,768]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=5*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=10*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=15*/bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=20*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=25*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=30*/bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, /*index=35*/bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=40*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=45*/bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, /*index=50*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=55*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=60*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, /*index=65*/bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=70*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=75*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, /*index=80*/bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=85*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=90*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=95*/bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=100*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=105*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=110*/bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, /*index=115*/bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=120*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=125*/bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, /*index=130*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=135*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=140*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, /*index=145*/bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=150*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=155*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[3072,768]{1,0}, /*index=160*/bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=165*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, /*index=170*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=175*/bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=180*/bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, /*index=185*/bf16[768,768]{1,0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[768]{0}, /*index=190*/bf16[768]{0}, bf16[3072,768]{1,0}, bf16[3072]{0}, bf16[768,3072]{1,0}, bf16[768]{0}, /*index=195*/bf16[768]{0}, bf16[768]{0}, bf16[768,768]{1,0}, bf16[768]{0}, bf16[2,768]{1,0}, /*index=200*/bf16[2]{0}, bf16[], bf16[28996,768]{1,0}, bf16[28996,768]{1,0}, bf16[512,768]{1,0}, /*index=205*/bf16[512,768]{1,0}, bf16[2,768]{1,0}, bf16[2,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=210*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=215*/bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, /*index=220*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=225*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, /*index=230*/bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=235*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=240*/bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, /*index=245*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=250*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, /*index=255*/bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=260*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, /*index=265*/bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=270*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=275*/bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, /*index=280*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=285*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, /*index=290*/bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=295*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=300*/bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, /*index=305*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=310*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, /*index=315*/bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=320*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, /*index=325*/bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=330*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=335*/bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, /*index=340*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, /*index=345*/bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[3072,768]{1,0}, bf16[3072,768]{1,0}, /*index=350*/bf16[768,3072]{1,0}, bf16[768,3072]{1,0}, bf16[768,768]{1,0}, bf16[768,768]{1,0}, bf16[2,768]{1,0}, /*index=355*/bf16[2,768]{1,0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=360*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=365*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=370*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=375*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=380*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=385*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=390*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=395*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=400*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=405*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=410*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=415*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=420*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=425*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=430*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=435*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=440*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=445*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=450*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=455*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=460*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=465*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=470*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=475*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=480*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=485*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=490*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=495*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=500*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=505*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=510*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=515*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=520*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=525*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=530*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=535*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=540*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=545*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=550*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=555*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=560*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=565*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=570*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=575*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=580*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=585*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=590*/bf16[768]{0}, bf16[768]{0}, bf16[3072]{0}, bf16[3072]{0}, bf16[768]{0}, /*index=595*/bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, bf16[768]{0}, /*index=600*/bf16[768]{0}, bf16[768]{0}, bf16[2]{0}, bf16[2]{0}, bf16[]) tuple(bf16[28996,768]{1,0} %add.10586, bf16[512,768]{1,0} %add.10613, bf16[2,768]{1,0} %add.10640, bf16[768]{0} %add.10669, bf16[768]{0} %add.10698, /*index=5*/bf16[768,768]{1,0} %add.10727, bf16[768]{0} %add.10756, bf16[768,768]{1,0} %add.10785, bf16[768]{0} %add.10814, bf16[768,768]{1,0} %add.10843, /*index=10*/bf16[768]{0} %add.10872, bf16[768,768]{1,0} %add.10901, bf16[768]{0} %add.10930, bf16[768]{0} %add.10959, bf16[768]{0} %add.10988, /*index=15*/bf16[3072,768]{1,0} %add.11017, bf16[3072]{0} %add.11046, bf16[768,3072]{1,0} %add.11075, bf16[768]{0} %add.11104, bf16[768]{0} %add.11133, /*index=20*/bf16[768]{0} %add.11162, bf16[768,768]{1,0} %add.11191, bf16[768]{0} %add.11220, bf16[768,768]{1,0} %add.11249, bf16[768]{0} %add.11278, /*index=25*/bf16[768,768]{1,0} %add.11307, bf16[768]{0} %add.11336, bf16[768,768]{1,0} %add.11365, bf16[768]{0} %add.11394, bf16[768]{0} %add.11423, /*index=30*/bf16[768]{0} %add.11452, bf16[3072,768]{1,0} %add.11481, bf16[3072]{0} %add.11510, bf16[768,3072]{1,0} %add.11539, bf16[768]{0} %add.11568, /*index=35*/bf16[768]{0} %add.11597, bf16[768]{0} %add.11626, bf16[768,768]{1,0} %add.11655, bf16[768]{0} %add.11684, bf16[768,768]{1,0} %add.11713, /*index=40*/bf16[768]{0} %add.11742, bf16[768,768]{1,0} %add.11771, bf16[768]{0} %add.11800, bf16[768,768]{1,0} %add.11829, bf16[768]{0} %add.11858, /*index=45*/bf16[768]{0} %add.11887, bf16[768]{0} %add.11916, bf16[3072,768]{1,0} %add.11945, bf16[3072]{0} %add.11974, bf16[768,3072]{1,0} %add.12003, /*index=50*/bf16[768]{0} %add.12032, bf16[768]{0} %add.12061, bf16[768]{0} %add.12090, bf16[768,768]{1,0} %add.12119, bf16[768]{0} %add.12148, /*index=55*/bf16[768,768]{1,0} %add.12177, bf16[768]{0} %add.12206, bf16[768,768]{1,0} %add.12235, bf16[768]{0} %add.12264, bf16[768,768]{1,0} %add.12293, /*index=60*/bf16[768]{0} %add.12322, bf16[768]{0} %add.12351, bf16[768]{0} %add.12380, bf16[3072,768]{1,0} %add.12409, bf16[3072]{0} %add.12438, /*index=65*/bf16[768,3072]{1,0} %add.12467, bf16[768]{0} %add.12496, bf16[768]{0} %add.12525, bf16[768]{0} %add.12554, bf16[768,768]{1,0} %add.12583, /*index=70*/bf16[768]{0} %add.12612, bf16[768,768]{1,0} %add.12641, bf16[768]{0} %add.12670, bf16[768,768]{1,0} %add.12699, bf16[768]{0} %add.12728, /*index=75*/bf16[768,768]{1,0} %add.12757, bf16[768]{0} %add.12786, bf16[768]{0} %add.12815, bf16[768]{0} %add.12844, bf16[3072,768]{1,0} %add.12873, /*index=80*/bf16[3072]{0} %add.12902, bf16[768,3072]{1,0} %add.12931, bf16[768]{0} %add.12960, bf16[768]{0} %add.12989, bf16[768]{0} %add.13018, /*index=85*/bf16[768,768]{1,0} %add.13047, bf16[768]{0} %add.13076, bf16[768,768]{1,0} %add.13105, bf16[768]{0} %add.13134, bf16[768,768]{1,0} %add.13163, /*index=90*/bf16[768]{0} %add.13192, bf16[768,768]{1,0} %add.13221, bf16[768]{0} %add.13250, bf16[768]{0} %add.13279, bf16[768]{0} %add.13308, /*index=95*/bf16[3072,768]{1,0} %add.13337, bf16[3072]{0} %add.13366, bf16[768,3072]{1,0} %add.13395, bf16[768]{0} %add.13424, bf16[768]{0} %add.13453, /*index=100*/bf16[768]{0} %add.13482, bf16[768,768]{1,0} %add.13511, bf16[768]{0} %add.13540, bf16[768,768]{1,0} %add.13569, bf16[768]{0} %add.13598, /*index=105*/bf16[768,768]{1,0} %add.13627, bf16[768]{0} %add.13656, bf16[768,768]{1,0} %add.13685, bf16[768]{0} %add.13714, bf16[768]{0} %add.13743, /*index=110*/bf16[768]{0} %add.13772, bf16[3072,768]{1,0} %add.13801, bf16[3072]{0} %add.13830, bf16[768,3072]{1,0} %add.13859, bf16[768]{0} %add.13888, /*index=115*/bf16[768]{0} %add.13917, bf16[768]{0} %add.13946, bf16[768,768]{1,0} %add.13975, bf16[768]{0} %add.14004, bf16[768,768]{1,0} %add.14033, /*index=120*/bf16[768]{0} %add.14062, bf16[768,768]{1,0} %add.14091, bf16[768]{0} %add.14120, bf16[768,768]{1,0} %add.14149, bf16[768]{0} %add.14178, /*index=125*/bf16[768]{0} %add.14207, bf16[768]{0} %add.14236, bf16[3072,768]{1,0} %add.14265, bf16[3072]{0} %add.14294, bf16[768,3072]{1,0} %add.14323, /*index=130*/bf16[768]{0} %add.14352, bf16[768]{0} %add.14381, bf16[768]{0} %add.14410, bf16[768,768]{1,0} %add.14439, bf16[768]{0} %add.14468, /*index=135*/bf16[768,768]{1,0} %add.14497, bf16[768]{0} %add.14526, bf16[768,768]{1,0} %add.14555, bf16[768]{0} %add.14584, bf16[768,768]{1,0} %add.14613, /*index=140*/bf16[768]{0} %add.14642, bf16[768]{0} %add.14671, bf16[768]{0} %add.14700, bf16[3072,768]{1,0} %add.14729, bf16[3072]{0} %add.14758, /*index=145*/bf16[768,3072]{1,0} %add.14787, bf16[768]{0} %add.14816, bf16[768]{0} %add.14845, bf16[768]{0} %add.14874, bf16[768,768]{1,0} %add.14903, /*index=150*/bf16[768]{0} %add.14932, bf16[768,768]{1,0} %add.14961, bf16[768]{0} %add.14990, bf16[768,768]{1,0} %add.15019, bf16[768]{0} %add.15048, /*index=155*/bf16[768,768]{1,0} %add.15077, bf16[768]{0} %add.15106, bf16[768]{0} %add.15135, bf16[768]{0} %add.15164, bf16[3072,768]{1,0} %add.15193, /*index=160*/bf16[3072]{0} %add.15222, bf16[768,3072]{1,0} %add.15251, bf16[768]{0} %add.15280, bf16[768]{0} %add.15309, bf16[768]{0} %add.15338, /*index=165*/bf16[768,768]{1,0} %add.15367, bf16[768]{0} %add.15396, bf16[768,768]{1,0} %add.15425, bf16[768]{0} %add.15454, bf16[768,768]{1,0} %add.15483, /*index=170*/bf16[768]{0} %add.15512, bf16[768,768]{1,0} %add.15541, bf16[768]{0} %add.15570, bf16[768]{0} %add.15599, bf16[768]{0} %add.15628, /*index=175*/bf16[3072,768]{1,0} %add.15657, bf16[3072]{0} %add.15686, bf16[768,3072]{1,0} %add.15715, bf16[768]{0} %add.15744, bf16[768]{0} %add.15773, /*index=180*/bf16[768]{0} %add.15802, bf16[768,768]{1,0} %add.15831, bf16[768]{0} %add.15860, bf16[768,768]{1,0} %add.15889, bf16[768]{0} %add.15918, /*index=185*/bf16[768,768]{1,0} %add.15947, bf16[768]{0} %add.15976, bf16[768,768]{1,0} %add.16005, bf16[768]{0} %add.16034, bf16[768]{0} %add.16063, /*index=190*/bf16[768]{0} %add.16092, bf16[3072,768]{1,0} %add.16121, bf16[3072]{0} %add.16150, bf16[768,3072]{1,0} %add.16179, bf16[768]{0} %add.16208, /*index=195*/bf16[768]{0} %add.16237, bf16[768]{0} %add.16266, bf16[768,768]{1,0} %add.16295, bf16[768]{0} %add.16324, bf16[2,768]{1,0} %add.16353, /*index=200*/bf16[2]{0} %add.16382, bf16[] %add.16386, bf16[28996,768]{1,0} %add.10582, bf16[28996,768]{1,0} %add.10569, bf16[512,768]{1,0} %add.10609, /*index=205*/bf16[512,768]{1,0} %add.10597, bf16[2,768]{1,0} %add.10636, bf16[2,768]{1,0} %add.10624, bf16[768,768]{1,0} %add.10723, bf16[768,768]{1,0} %add.10711, /*index=210*/bf16[768,768]{1,0} %add.10781, bf16[768,768]{1,0} %add.10769, bf16[768,768]{1,0} %add.10839, bf16[768,768]{1,0} %add.10827, bf16[768,768]{1,0} %add.10897, /*index=215*/bf16[768,768]{1,0} %add.10885, bf16[3072,768]{1,0} %add.11013, bf16[3072,768]{1,0} %add.11001, bf16[768,3072]{1,0} %add.11071, bf16[768,3072]{1,0} %add.11059, /*index=220*/bf16[768,768]{1,0} %add.11187, bf16[768,768]{1,0} %add.11175, bf16[768,768]{1,0} %add.11245, bf16[768,768]{1,0} %add.11233, bf16[768,768]{1,0} %add.11303, /*index=225*/bf16[768,768]{1,0} %add.11291, bf16[768,768]{1,0} %add.11361, bf16[768,768]{1,0} %add.11349, bf16[3072,768]{1,0} %add.11477, bf16[3072,768]{1,0} %add.11465, /*index=230*/bf16[768,3072]{1,0} %add.11535, bf16[768,3072]{1,0} %add.11523, bf16[768,768]{1,0} %add.11651, bf16[768,768]{1,0} %add.11639, bf16[768,768]{1,0} %add.11709, /*index=235*/bf16[768,768]{1,0} %add.11697, bf16[768,768]{1,0} %add.11767, bf16[768,768]{1,0} %add.11755, bf16[768,768]{1,0} %add.11825, bf16[768,768]{1,0} %add.11813, /*index=240*/bf16[3072,768]{1,0} %add.11941, bf16[3072,768]{1,0} %add.11929, bf16[768,3072]{1,0} %add.11999, bf16[768,3072]{1,0} %add.11987, bf16[768,768]{1,0} %add.12115, /*index=245*/bf16[768,768]{1,0} %add.12103, bf16[768,768]{1,0} %add.12173, bf16[768,768]{1,0} %add.12161, bf16[768,768]{1,0} %add.12231, bf16[768,768]{1,0} %add.12219, /*index=250*/bf16[768,768]{1,0} %add.12289, bf16[768,768]{1,0} %add.12277, bf16[3072,768]{1,0} %add.12405, bf16[3072,768]{1,0} %add.12393, bf16[768,3072]{1,0} %add.12463, /*index=255*/bf16[768,3072]{1,0} %add.12451, bf16[768,768]{1,0} %add.12579, bf16[768,768]{1,0} %add.12567, bf16[768,768]{1,0} %add.12637, bf16[768,768]{1,0} %add.12625, /*index=260*/bf16[768,768]{1,0} %add.12695, bf16[768,768]{1,0} %add.12683, bf16[768,768]{1,0} %add.12753, bf16[768,768]{1,0} %add.12741, bf16[3072,768]{1,0} %add.12869, /*index=265*/bf16[3072,768]{1,0} %add.12857, bf16[768,3072]{1,0} %add.12927, bf16[768,3072]{1,0} %add.12915, bf16[768,768]{1,0} %add.13043, bf16[768,768]{1,0} %add.13031, /*index=270*/bf16[768,768]{1,0} %add.13101, bf16[768,768]{1,0} %add.13089, bf16[768,768]{1,0} %add.13159, bf16[768,768]{1,0} %add.13147, bf16[768,768]{1,0} %add.13217, /*index=275*/bf16[768,768]{1,0} %add.13205, bf16[3072,768]{1,0} %add.13333, bf16[3072,768]{1,0} %add.13321, bf16[768,3072]{1,0} %add.13391, bf16[768,3072]{1,0} %add.13379, /*index=280*/bf16[768,768]{1,0} %add.13507, bf16[768,768]{1,0} %add.13495, bf16[768,768]{1,0} %add.13565, bf16[768,768]{1,0} %add.13553, bf16[768,768]{1,0} %add.13623, /*index=285*/bf16[768,768]{1,0} %add.13611, bf16[768,768]{1,0} %add.13681, bf16[768,768]{1,0} %add.13669, bf16[3072,768]{1,0} %add.13797, bf16[3072,768]{1,0} %add.13785, /*index=290*/bf16[768,3072]{1,0} %add.13855, bf16[768,3072]{1,0} %add.13843, bf16[768,768]{1,0} %add.13971, bf16[768,768]{1,0} %add.13959, bf16[768,768]{1,0} %add.14029, /*index=295*/bf16[768,768]{1,0} %add.14017, bf16[768,768]{1,0} %add.14087, bf16[768,768]{1,0} %add.14075, bf16[768,768]{1,0} %add.14145, bf16[768,768]{1,0} %add.14133, /*index=300*/bf16[3072,768]{1,0} %add.14261, bf16[3072,768]{1,0} %add.14249, bf16[768,3072]{1,0} %add.14319, bf16[768,3072]{1,0} %add.14307, bf16[768,768]{1,0} %add.14435, /*index=305*/bf16[768,768]{1,0} %add.14423, bf16[768,768]{1,0} %add.14493, bf16[768,768]{1,0} %add.14481, bf16[768,768]{1,0} %add.14551, bf16[768,768]{1,0} %add.14539, /*index=310*/bf16[768,768]{1,0} %add.14609, bf16[768,768]{1,0} %add.14597, bf16[3072,768]{1,0} %add.14725, bf16[3072,768]{1,0} %add.14713, bf16[768,3072]{1,0} %add.14783, /*index=315*/bf16[768,3072]{1,0} %add.14771, bf16[768,768]{1,0} %add.14899, bf16[768,768]{1,0} %add.14887, bf16[768,768]{1,0} %add.14957, bf16[768,768]{1,0} %add.14945, /*index=320*/bf16[768,768]{1,0} %add.15015, bf16[768,768]{1,0} %add.15003, bf16[768,768]{1,0} %add.15073, bf16[768,768]{1,0} %add.15061, bf16[3072,768]{1,0} %add.15189, /*index=325*/bf16[3072,768]{1,0} %add.15177, bf16[768,3072]{1,0} %add.15247, bf16[768,3072]{1,0} %add.15235, bf16[768,768]{1,0} %add.15363, bf16[768,768]{1,0} %add.15351, /*index=330*/bf16[768,768]{1,0} %add.15421, bf16[768,768]{1,0} %add.15409, bf16[768,768]{1,0} %add.15479, bf16[768,768]{1,0} %add.15467, bf16[768,768]{1,0} %add.15537, /*index=335*/bf16[768,768]{1,0} %add.15525, bf16[3072,768]{1,0} %add.15653, bf16[3072,768]{1,0} %add.15641, bf16[768,3072]{1,0} %add.15711, bf16[768,3072]{1,0} %add.15699, /*index=340*/bf16[768,768]{1,0} %add.15827, bf16[768,768]{1,0} %add.15815, bf16[768,768]{1,0} %add.15885, bf16[768,768]{1,0} %add.15873, bf16[768,768]{1,0} %add.15943, /*index=345*/bf16[768,768]{1,0} %add.15931, bf16[768,768]{1,0} %add.16001, bf16[768,768]{1,0} %add.15989, bf16[3072,768]{1,0} %add.16117, bf16[3072,768]{1,0} %add.16105, /*index=350*/bf16[768,3072]{1,0} %add.16175, bf16[768,3072]{1,0} %add.16163, bf16[768,768]{1,0} %add.16291, bf16[768,768]{1,0} %add.16279, bf16[2,768]{1,0} %add.16349, /*index=355*/bf16[2,768]{1,0} %add.16337, bf16[768]{0} %add.10665, bf16[768]{0} %add.10653, bf16[768]{0} %add.10694, bf16[768]{0} %add.10682, /*index=360*/bf16[768]{0} %add.10752, bf16[768]{0} %add.10740, bf16[768]{0} %add.10810, bf16[768]{0} %add.10798, bf16[768]{0} %add.10868, /*index=365*/bf16[768]{0} %add.10856, bf16[768]{0} %add.10926, bf16[768]{0} %add.10914, bf16[768]{0} %add.10955, bf16[768]{0} %add.10943, /*index=370*/bf16[768]{0} %add.10984, bf16[768]{0} %add.10972, bf16[3072]{0} %add.11042, bf16[3072]{0} %add.11030, bf16[768]{0} %add.11100, /*index=375*/bf16[768]{0} %add.11088, bf16[768]{0} %add.11129, bf16[768]{0} %add.11117, bf16[768]{0} %add.11158, bf16[768]{0} %add.11146, /*index=380*/bf16[768]{0} %add.11216, bf16[768]{0} %add.11204, bf16[768]{0} %add.11274, bf16[768]{0} %add.11262, bf16[768]{0} %add.11332, /*index=385*/bf16[768]{0} %add.11320, bf16[768]{0} %add.11390, bf16[768]{0} %add.11378, bf16[768]{0} %add.11419, bf16[768]{0} %add.11407, /*index=390*/bf16[768]{0} %add.11448, bf16[768]{0} %add.11436, bf16[3072]{0} %add.11506, bf16[3072]{0} %add.11494, bf16[768]{0} %add.11564, /*index=395*/bf16[768]{0} %add.11552, bf16[768]{0} %add.11593, bf16[768]{0} %add.11581, bf16[768]{0} %add.11622, bf16[768]{0} %add.11610, /*index=400*/bf16[768]{0} %add.11680, bf16[768]{0} %add.11668, bf16[768]{0} %add.11738, bf16[768]{0} %add.11726, bf16[768]{0} %add.11796, /*index=405*/bf16[768]{0} %add.11784, bf16[768]{0} %add.11854, bf16[768]{0} %add.11842, bf16[768]{0} %add.11883, bf16[768]{0} %add.11871, /*index=410*/bf16[768]{0} %add.11912, bf16[768]{0} %add.11900, bf16[3072]{0} %add.11970, bf16[3072]{0} %add.11958, bf16[768]{0} %add.12028, /*index=415*/bf16[768]{0} %add.12016, bf16[768]{0} %add.12057, bf16[768]{0} %add.12045, bf16[768]{0} %add.12086, bf16[768]{0} %add.12074, /*index=420*/bf16[768]{0} %add.12144, bf16[768]{0} %add.12132, bf16[768]{0} %add.12202, bf16[768]{0} %add.12190, bf16[768]{0} %add.12260, /*index=425*/bf16[768]{0} %add.12248, bf16[768]{0} %add.12318, bf16[768]{0} %add.12306, bf16[768]{0} %add.12347, bf16[768]{0} %add.12335, /*index=430*/bf16[768]{0} %add.12376, bf16[768]{0} %add.12364, bf16[3072]{0} %add.12434, bf16[3072]{0} %add.12422, bf16[768]{0} %add.12492, /*index=435*/bf16[768]{0} %add.12480, bf16[768]{0} %add.12521, bf16[768]{0} %add.12509, bf16[768]{0} %add.12550, bf16[768]{0} %add.12538, /*index=440*/bf16[768]{0} %add.12608, bf16[768]{0} %add.12596, bf16[768]{0} %add.12666, bf16[768]{0} %add.12654, bf16[768]{0} %add.12724, /*index=445*/bf16[768]{0} %add.12712, bf16[768]{0} %add.12782, bf16[768]{0} %add.12770, bf16[768]{0} %add.12811, bf16[768]{0} %add.12799, /*index=450*/bf16[768]{0} %add.12840, bf16[768]{0} %add.12828, bf16[3072]{0} %add.12898, bf16[3072]{0} %add.12886, bf16[768]{0} %add.12956, /*index=455*/bf16[768]{0} %add.12944, bf16[768]{0} %add.12985, bf16[768]{0} %add.12973, bf16[768]{0} %add.13014, bf16[768]{0} %add.13002, /*index=460*/bf16[768]{0} %add.13072, bf16[768]{0} %add.13060, bf16[768]{0} %add.13130, bf16[768]{0} %add.13118, bf16[768]{0} %add.13188, /*index=465*/bf16[768]{0} %add.13176, bf16[768]{0} %add.13246, bf16[768]{0} %add.13234, bf16[768]{0} %add.13275, bf16[768]{0} %add.13263, /*index=470*/bf16[768]{0} %add.13304, bf16[768]{0} %add.13292, bf16[3072]{0} %add.13362, bf16[3072]{0} %add.13350, bf16[768]{0} %add.13420, /*index=475*/bf16[768]{0} %add.13408, bf16[768]{0} %add.13449, bf16[768]{0} %add.13437, bf16[768]{0} %add.13478, bf16[768]{0} %add.13466, /*index=480*/bf16[768]{0} %add.13536, bf16[768]{0} %add.13524, bf16[768]{0} %add.13594, bf16[768]{0} %add.13582, bf16[768]{0} %add.13652, /*index=485*/bf16[768]{0} %add.13640, bf16[768]{0} %add.13710, bf16[768]{0} %add.13698, bf16[768]{0} %add.13739, bf16[768]{0} %add.13727, /*index=490*/bf16[768]{0} %add.13768, bf16[768]{0} %add.13756, bf16[3072]{0} %add.13826, bf16[3072]{0} %add.13814, bf16[768]{0} %add.13884, /*index=495*/bf16[768]{0} %add.13872, bf16[768]{0} %add.13913, bf16[768]{0} %add.13901, bf16[768]{0} %add.13942, bf16[768]{0} %add.13930, /*index=500*/bf16[768]{0} %add.14000, bf16[768]{0} %add.13988, bf16[768]{0} %add.14058, bf16[768]{0} %add.14046, bf16[768]{0} %add.14116, /*index=505*/bf16[768]{0} %add.14104, bf16[768]{0} %add.14174, bf16[768]{0} %add.14162, bf16[768]{0} %add.14203, bf16[768]{0} %add.14191, /*index=510*/bf16[768]{0} %add.14232, bf16[768]{0} %add.14220, bf16[3072]{0} %add.14290, bf16[3072]{0} %add.14278, bf16[768]{0} %add.14348, /*index=515*/bf16[768]{0} %add.14336, bf16[768]{0} %add.14377, bf16[768]{0} %add.14365, bf16[768]{0} %add.14406, bf16[768]{0} %add.14394, /*index=520*/bf16[768]{0} %add.14464, bf16[768]{0} %add.14452, bf16[768]{0} %add.14522, bf16[768]{0} %add.14510, bf16[768]{0} %add.14580, /*index=525*/bf16[768]{0} %add.14568, bf16[768]{0} %add.14638, bf16[768]{0} %add.14626, bf16[768]{0} %add.14667, bf16[768]{0} %add.14655, /*index=530*/bf16[768]{0} %add.14696, bf16[768]{0} %add.14684, bf16[3072]{0} %add.14754, bf16[3072]{0} %add.14742, bf16[768]{0} %add.14812, /*index=535*/bf16[768]{0} %add.14800, bf16[768]{0} %add.14841, bf16[768]{0} %add.14829, bf16[768]{0} %add.14870, bf16[768]{0} %add.14858, /*index=540*/bf16[768]{0} %add.14928, bf16[768]{0} %add.14916, bf16[768]{0} %add.14986, bf16[768]{0} %add.14974, bf16[768]{0} %add.15044, /*index=545*/bf16[768]{0} %add.15032, bf16[768]{0} %add.15102, bf16[768]{0} %add.15090, bf16[768]{0} %add.15131, bf16[768]{0} %add.15119, /*index=550*/bf16[768]{0} %add.15160, bf16[768]{0} %add.15148, bf16[3072]{0} %add.15218, bf16[3072]{0} %add.15206, bf16[768]{0} %add.15276, /*index=555*/bf16[768]{0} %add.15264, bf16[768]{0} %add.15305, bf16[768]{0} %add.15293, bf16[768]{0} %add.15334, bf16[768]{0} %add.15322, /*index=560*/bf16[768]{0} %add.15392, bf16[768]{0} %add.15380, bf16[768]{0} %add.15450, bf16[768]{0} %add.15438, bf16[768]{0} %add.15508, /*index=565*/bf16[768]{0} %add.15496, bf16[768]{0} %add.15566, bf16[768]{0} %add.15554, bf16[768]{0} %add.15595, bf16[768]{0} %add.15583, /*index=570*/bf16[768]{0} %add.15624, bf16[768]{0} %add.15612, bf16[3072]{0} %add.15682, bf16[3072]{0} %add.15670, bf16[768]{0} %add.15740, /*index=575*/bf16[768]{0} %add.15728, bf16[768]{0} %add.15769, bf16[768]{0} %add.15757, bf16[768]{0} %add.15798, bf16[768]{0} %add.15786, /*index=580*/bf16[768]{0} %add.15856, bf16[768]{0} %add.15844, bf16[768]{0} %add.15914, bf16[768]{0} %add.15902, bf16[768]{0} %add.15972, /*index=585*/bf16[768]{0} %add.15960, bf16[768]{0} %add.16030, bf16[768]{0} %add.16018, bf16[768]{0} %add.16059, bf16[768]{0} %add.16047, /*index=590*/bf16[768]{0} %add.16088, bf16[768]{0} %add.16076, bf16[3072]{0} %add.16146, bf16[3072]{0} %add.16134, bf16[768]{0} %add.16204, /*index=595*/bf16[768]{0} %add.16192, bf16[768]{0} %add.16233, bf16[768]{0} %add.16221, bf16[768]{0} %add.16262, bf16[768]{0} %add.16250, /*index=600*/bf16[768]{0} %add.16320, bf16[768]{0} %add.16308, bf16[2]{0} %add.16378, bf16[2]{0} %add.16366, bf16[] %negate.0), frontend_attributes={neff_output_names="output0,output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12,output13,output14,output15,output16,output17,output18,output19,output20,output21,output22,output23,output24,output25,output26,output27,output28,output29,output30,output31,output32,output33,output34,output35,output36,output37,output38,output39,output40,output41,output42,output43,output44,output45,output46,output47,output48,output49,output50,output51,output52,output53,output54,output55,output56,output57,output58,output59,output60,output61,output62,output63,output64,output65,output66,output67,output68,output69,output70,output71,output72,output73,output74,output75,output76,output77,output78,output79,output80,output81,output82,output83,output84,output85,output86,output87,output88,output89,output90,output91,output92,output93,output94,output95,output96,output97,output98,output99,output100,output101,output102,output103,output104,output105,output106,output107,output108,output109,output110,output111,output112,output113,output114,output115,output116,output117,output118,output119,output120,output121,output122,output123,output124,output125,output126,output127,output128,output129,output130,output131,output132,output133,output134,output135,output136,output137,output138,output139,output140,output141,output142,output143,output144,output145,output146,output147,output148,output149,output150,output151,output152,output153,output154,output155,output156,output157,output158,output159,output160,output161,output162,output163,output164,output165,output166,output167,output168,output169,output170,output171,output172,output173,output174,output175,output176,output177,output178,output179,output180,output181,output182,output183,output184,output185,output186,output187,output188,output189,output190,output191,output192,output193,output194,output195,output196,output197,output198,output199,output200,output201,output202,output203,output204,output205,output206,output207,output208,output209,output210,output211,output212,output213,output214,output215,output216,output217,output218,output219,output220,output221,output222,output223,output224,output225,output226,output227,output228,output229,output230,output231,output232,output233,output234,output235,output236,output237,output238,output239,output240,output241,output242,output243,output244,output245,output246,output247,output248,output249,output250,output251,output252,output253,output254,output255,output256,output257,output258,output259,output260,output261,output262,output263,output264,output265,output266,output267,output268,output269,output270,output271,output272,output273,output274,output275,output276,output277,output278,output279,output280,output281,output282,output283,output284,output285,output286,output287,output288,output289,output290,output291,output292,output293,output294,output295,output296,output297,output298,output299,output300,output301,output302,output303,output304,output305,output306,output307,output308,output309,output310,output311,output312,output313,output314,output315,output316,output317,output318,output319,output320,output321,output322,output323,output324,output325,output326,output327,output328,output329,output330,output331,output332,output333,output334,output335,output336,output337,output338,output339,output340,output341,output342,output343,output344,output345,output346,output347,output348,output349,output350,output351,output352,output353,output354,output355,output356,output357,output358,output359,output360,output361,output362,output363,output364,output365,output366,output367,output368,output369,output370,output371,output372,output373,output374,output375,output376,output377,output378,output379,output380,output381,output382,output383,output384,output385,output386,output387,output388,output389,output390,output391,output392,output393,output394,output395,output396,output397,output398,output399,output400,output401,output402,output403,output404,output405,output406,output407,output408,output409,output410,output411,output412,output413,output414,output415,output416,output417,output418,output419,output420,output421,output422,output423,output424,output425,output426,output427,output428,output429,output430,output431,output432,output433,output434,output435,output436,output437,output438,output439,output440,output441,output442,output443,output444,output445,output446,output447,output448,output449,output450,output451,output452,output453,output454,output455,output456,output457,output458,output459,output460,output461,output462,output463,output464,output465,output466,output467,output468,output469,output470,output471,output472,output473,output474,output475,output476,output477,output478,output479,output480,output481,output482,output483,output484,output485,output486,output487,output488,output489,output490,output491,output492,output493,output494,output495,output496,output497,output498,output499,output500,output501,output502,output503,output504,output505,output506,output507,output508,output509,output510,output511,output512,output513,output514,output515,output516,output517,output518,output519,output520,output521,output522,output523,output524,output525,output526,output527,output528,output529,output530,output531,output532,output533,output534,output535,output536,output537,output538,output539,output540,output541,output542,output543,output544,output545,output546,output547,output548,output549,output550,output551,output552,output553,output554,output555,output556,output557,output558,output559,output560,output561,output562,output563,output564,output565,output566,output567,output568,output569,output570,output571,output572,output573,output574,output575,output576,output577,output578,output579,output580,output581,output582,output583,output584,output585,output586,output587,output588,output589,output590,output591,output592,output593,output594,output595,output596,output597,output598,output599,output600,output601,output602,output603,output604"}
}

`;

export default text;
